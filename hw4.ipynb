{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10-714 Homework 4\n",
    "\n",
    "In this homework, you will leverage all of the components built in the last three homeworks to solve some modern problems with high performing network structures. We will start by adding a few new ops leveraging our new CPU/CUDA backends. Then, you will implement convolution, and a convolutional neural network to train a classifier on the CIFAR-10 image classification dataset. Then, you will implement recurrent and long-short term memory (LSTM) neural networks, and do word-level prediction language modeling on the Penn Treebank dataset. \n",
    "\n",
    "As always, we will start by copying this notebook and getting the starting code.\n",
    "Reminder: __you must save a copy in drive__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/dlsys10714/mugrade.git\n",
      "  Cloning https://github.com/dlsys10714/mugrade.git to /tmp/pip-req-build-_oxyg1_k\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/dlsys10714/mugrade.git /tmp/pip-req-build-_oxyg1_k\n",
      "  Resolved https://github.com/dlsys10714/mugrade.git to commit 656cdc2b7ad5a37e7a5347a7b0405df0acd72380\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: pybind11 in /opt/conda/envs/train/lib/python3.11/site-packages (2.13.6)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip3 install --upgrade --no-deps git+https://github.com/dlsys10714/mugrade.git\n",
    "!pip3 install pybind11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Found pybind11: /opt/conda/envs/train/lib/python3.11/site-packages/pybind11/include (found version \"2.13.6\")\n",
      "-- Found cuda, building cuda backend\n",
      "Mon Dec 23 02:33:46 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA H100 80GB HBM3          On  |   00000000:18:00.0 Off |                    0 |\n",
      "| N/A   30C    P0            100W /  700W |      11MiB /  81559MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA H100 80GB HBM3          On  |   00000000:2A:00.0 Off |                    0 |\n",
      "| N/A   30C    P0             70W /  700W |     129MiB /  81559MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   2  NVIDIA H100 80GB HBM3          On  |   00000000:3A:00.0 Off |                    0 |\n",
      "| N/A   30C    P0             69W /  700W |     123MiB /  81559MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |\n",
      "| N/A   28C    P0             69W /  700W |       3MiB /  81559MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9A:00.0 Off |                    0 |\n",
      "| N/A   28C    P0             69W /  700W |       3MiB /  81559MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   5  NVIDIA H100 80GB HBM3          On  |   00000000:AB:00.0 Off |                    0 |\n",
      "| N/A   31C    P0             69W /  700W |       3MiB /  81559MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   6  NVIDIA H100 80GB HBM3          On  |   00000000:BA:00.0 Off |                    0 |\n",
      "| N/A   29C    P0             68W /  700W |       3MiB /  81559MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |\n",
      "| N/A   29C    P0             70W /  700W |       3MiB /  81559MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "-- Autodetected CUDA architecture(s):  8.6+PTX 8.6+PTX 8.6+PTX 8.6+PTX 8.6+PTX 8.6+PTX 8.6+PTX 8.6+PTX\n",
      "-- Configuring done (2.0s)\n",
      "-- Generating done (0.0s)\n",
      "-- Build files have been written to: /mnt/zhumingzhu/work/00test/hw4/build\n",
      "make[1]: Entering directory '/mnt/zhumingzhu/work/00test/hw4/build'\n",
      "make[2]: Entering directory '/mnt/zhumingzhu/work/00test/hw4/build'\n",
      "make[3]: Entering directory '/mnt/zhumingzhu/work/00test/hw4/build'\n",
      "make[3]: Leaving directory '/mnt/zhumingzhu/work/00test/hw4/build'\n",
      "[ 50%] Built target ndarray_backend_cpu\n",
      "make[3]: Entering directory '/mnt/zhumingzhu/work/00test/hw4/build'\n",
      "make[3]: Leaving directory '/mnt/zhumingzhu/work/00test/hw4/build'\n",
      "[100%] Built target ndarray_backend_cuda\n",
      "make[2]: Leaving directory '/mnt/zhumingzhu/work/00test/hw4/build'\n",
      "make[1]: Leaving directory '/mnt/zhumingzhu/work/00test/hw4/build'\n"
     ]
    }
   ],
   "source": [
    "!make"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PYTHONPATH=./python\n",
      "env: NEEDLE_BACKEND=nd\n"
     ]
    }
   ],
   "source": [
    "%set_env PYTHONPATH ./python\n",
    "%set_env NEEDLE_BACKEND nd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the datasets you will be using for this assignment\n",
    "\n",
    "import urllib.request\n",
    "import os\n",
    "\n",
    "!mkdir -p './data/ptb'\n",
    "# Download Penn Treebank dataset\n",
    "ptb_data = \"https://raw.githubusercontent.com/wojzaremba/lstm/master/data/ptb.\"\n",
    "for f in ['train.txt', 'test.txt', 'valid.txt']:\n",
    "    if not os.path.exists(os.path.join('./data/ptb', f)):\n",
    "        urllib.request.urlretrieve(ptb_data + f, os.path.join('./data/ptb', f))\n",
    "\n",
    "# Download CIFAR-10 dataset\n",
    "if not os.path.isdir(\"./data/cifar-10-batches-py\"):\n",
    "    urllib.request.urlretrieve(\"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\", \"./data/cifar-10-python.tar.gz\")\n",
    "    !tar -xvzf './data/cifar-10-python.tar.gz' -C './data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To finish setting up the assignment, go ahead and fill in all the code in `python/needle/autograd.py` using your solution code from the previous homework. Also copy the solutions in `src/ndarray_backend_cpu.cc` and `src/ndarray_backend_cuda.cu` from homework 3.\n",
    "\n",
    "**Note**: Be careful not to accidentally delete or modify any new imports and function declarations when copying over code from previous assignments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: ND Backend [10 pts]\n",
    "\n",
    "Recall that in homework 2, the `array_api` was imported as `numpy`. In this part, the goal is to write the necessary operations with `array_api` imported from the needle backend `NDArray` in `python/needle/backend_ndarray/ndarray.py`. Make sure to copy the solutions for `reshape`, `permute`, `broadcast_to` and `__getitem__` from homework 3.\n",
    "\n",
    "Fill in the following classes in `python/needle/ops_logarithmic.py` and `python/needle/ops_mathematic.py`:\n",
    "\n",
    "- `PowerScalar`\n",
    "- `EWiseDiv`\n",
    "- `DivScalar`\n",
    "- `Transpose`\n",
    "- `Reshape`\n",
    "- `BroadcastTo`\n",
    "- `Summation`\n",
    "- `MatMul`\n",
    "- `Negate`\n",
    "- `Log`\n",
    "- `Exp`\n",
    "- `ReLU`\n",
    "- `LogSumExp`\n",
    "- `Tanh` (new)\n",
    "- `Stack` (new)\n",
    "- `Split` (new)\n",
    "\n",
    "Note that for most of these, you already wrote the solutions in the previous homework and you should not change most part of your previous solution, if issues arise, please check if the `array_api` function used is supported in the needle backend. \n",
    "\n",
    "The `Tanh`, `Stack`, and `Split` operators are newly added. `Stack` concatenates same-sized tensors along a new axis, and `Split` undoes this operation. The gradients of the two operations can be written in terms of each other. We do not directly test `Split`, and only test the backward pass of `Stack` (for which we assume you used `Split`).\n",
    "\n",
    "**Note:** You may want to make your Summation op support sums over multiple axes; you will likely need it for the backward pass of the BroadcastTo op if yours supports broadcasting over multiple axes at a time. However, this is more about ease of use than necessity, and we leave this decision up to you (there are no corresponding tests).\n",
    "\n",
    "**Note:** Depending on your implementations, you may want to ensure that you call `.compact()` before reshaping arrays. (If this is necessary, you will run into corresponding error messages later in the assignment.)\n",
    "\n",
    "**Note**: Be careful not to accidentally delete or modify any new imports and function declarations when copying over code from previous assignments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.11.10, pytest-8.3.4, pluggy-1.5.0 -- /opt/conda/envs/train/bin/python\n",
      "cachedir: .pytest_cache\n",
      "hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase(PosixPath('/mnt/zhumingzhu/work/00test/hw4/.hypothesis/examples'))\n",
      "rootdir: /mnt/zhumingzhu/work/00test/hw4\n",
      "plugins: hypothesis-6.115.5\n",
      "collected 1803 items / 1685 deselected / 118 selected                          \u001b[0m\u001b[1m\n",
      "\n",
      "tests/hw4/test_nd_backend.py::test_ewise_fn[cpu-shape0-divide] \u001b[32mPASSED\u001b[0m\u001b[32m    [  0%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_ewise_fn[cpu-shape0-subtract] \u001b[32mPASSED\u001b[0m\u001b[32m  [  1%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_ewise_fn[cpu-shape1-divide] \u001b[32mPASSED\u001b[0m\u001b[32m    [  2%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_ewise_fn[cpu-shape1-subtract] \u001b[32mPASSED\u001b[0m\u001b[32m  [  3%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_ewise_fn[cuda-shape0-divide] \u001b[32mPASSED\u001b[0m\u001b[32m   [  4%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_ewise_fn[cuda-shape0-subtract] \u001b[32mPASSED\u001b[0m\u001b[32m [  5%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_ewise_fn[cuda-shape1-divide] \u001b[32mPASSED\u001b[0m\u001b[32m   [  5%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_ewise_fn[cuda-shape1-subtract] \u001b[32mPASSED\u001b[0m\u001b[32m [  6%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_scalar_fn[cpu-shape0-divide] \u001b[32mPASSED\u001b[0m\u001b[32m   [  7%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_scalar_fn[cpu-shape0-subtract] \u001b[32mPASSED\u001b[0m\u001b[32m [  8%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_scalar_fn[cpu-shape1-divide] \u001b[32mPASSED\u001b[0m\u001b[32m   [  9%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_scalar_fn[cpu-shape1-subtract] \u001b[32mPASSED\u001b[0m\u001b[32m [ 10%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_scalar_fn[cuda-shape0-divide] \u001b[32mPASSED\u001b[0m\u001b[32m  [ 11%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_scalar_fn[cuda-shape0-subtract] \u001b[32mPASSED\u001b[0m\u001b[32m [ 11%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_scalar_fn[cuda-shape1-divide] \u001b[32mPASSED\u001b[0m\u001b[32m  [ 12%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_scalar_fn[cuda-shape1-subtract] \u001b[32mPASSED\u001b[0m\u001b[32m [ 13%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_matmul[cpu-16-16-16] \u001b[32mPASSED\u001b[0m\u001b[32m           [ 14%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_matmul[cpu-8-8-8] \u001b[32mPASSED\u001b[0m\u001b[32m              [ 15%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_matmul[cpu-1-2-3] \u001b[32mPASSED\u001b[0m\u001b[32m              [ 16%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_matmul[cpu-3-4-5] \u001b[32mPASSED\u001b[0m\u001b[32m              [ 16%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_matmul[cpu-5-4-3] \u001b[32mPASSED\u001b[0m\u001b[32m              [ 17%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_matmul[cpu-16-16-32] \u001b[32mPASSED\u001b[0m\u001b[32m           [ 18%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_matmul[cpu-64-64-64] \u001b[32mPASSED\u001b[0m\u001b[32m           [ 19%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_matmul[cpu-72-72-72] \u001b[32mPASSED\u001b[0m\u001b[32m           [ 20%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_matmul[cpu-72-73-74] \u001b[32mPASSED\u001b[0m\u001b[32m           [ 21%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_matmul[cpu-74-73-72] \u001b[32mPASSED\u001b[0m\u001b[32m           [ 22%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_matmul[cpu-128-128-128] \u001b[32mPASSED\u001b[0m\u001b[32m        [ 22%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_matmul[cuda-16-16-16] \u001b[32mPASSED\u001b[0m\u001b[32m          [ 23%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_matmul[cuda-8-8-8] \u001b[32mPASSED\u001b[0m\u001b[32m             [ 24%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_matmul[cuda-1-2-3] \u001b[32mPASSED\u001b[0m\u001b[32m             [ 25%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_matmul[cuda-3-4-5] \u001b[32mPASSED\u001b[0m\u001b[32m             [ 26%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_matmul[cuda-5-4-3] \u001b[32mPASSED\u001b[0m\u001b[32m             [ 27%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_matmul[cuda-16-16-32] \u001b[32mPASSED\u001b[0m\u001b[32m          [ 27%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_matmul[cuda-64-64-64] \u001b[32mPASSED\u001b[0m\u001b[32m          [ 28%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_matmul[cuda-72-72-72] \u001b[32mPASSED\u001b[0m\u001b[32m          [ 29%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_matmul[cuda-72-73-74] \u001b[32mPASSED\u001b[0m\u001b[32m          [ 30%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_matmul[cuda-74-73-72] \u001b[32mPASSED\u001b[0m\u001b[32m          [ 31%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_matmul[cuda-128-128-128] \u001b[32mPASSED\u001b[0m\u001b[32m       [ 32%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_power[cpu-shape0] \u001b[32mPASSED\u001b[0m\u001b[32m              [ 33%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_power[cpu-shape1] \u001b[32mPASSED\u001b[0m\u001b[32m              [ 33%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_power[cuda-shape0] \u001b[32mPASSED\u001b[0m\u001b[32m             [ 34%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_power[cuda-shape1] \u001b[32mPASSED\u001b[0m\u001b[32m             [ 35%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_log[cpu-shape0] \u001b[32mPASSED\u001b[0m\u001b[32m                [ 36%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_log[cpu-shape1] \u001b[32mPASSED\u001b[0m\u001b[32m                [ 37%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_log[cuda-shape0] \u001b[32mPASSED\u001b[0m\u001b[32m               [ 38%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_log[cuda-shape1] \u001b[32mPASSED\u001b[0m\u001b[32m               [ 38%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_exp[cpu-shape0] \u001b[32mPASSED\u001b[0m\u001b[32m                [ 39%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_exp[cpu-shape1] \u001b[32mPASSED\u001b[0m\u001b[32m                [ 40%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_exp[cuda-shape0] \u001b[32mPASSED\u001b[0m\u001b[32m               [ 41%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_exp[cuda-shape1] \u001b[32mPASSED\u001b[0m\u001b[32m               [ 42%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_relu[cpu-shape0] \u001b[32mPASSED\u001b[0m\u001b[32m               [ 43%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_relu[cpu-shape1] \u001b[32mPASSED\u001b[0m\u001b[32m               [ 44%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_relu[cuda-shape0] \u001b[32mPASSED\u001b[0m\u001b[32m              [ 44%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_relu[cuda-shape1] \u001b[32mPASSED\u001b[0m\u001b[32m              [ 45%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_tanh[cpu-shape0] \u001b[32mPASSED\u001b[0m\u001b[32m               [ 46%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_tanh[cpu-shape1] \u001b[32mPASSED\u001b[0m\u001b[32m               [ 47%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_tanh[cuda-shape0] \u001b[32mPASSED\u001b[0m\u001b[32m              [ 48%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_tanh[cuda-shape1] \u001b[32mPASSED\u001b[0m\u001b[32m              [ 49%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_tanh_backward[cpu-shape0] \u001b[32mPASSED\u001b[0m\u001b[32m      [ 50%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_tanh_backward[cpu-shape1] \u001b[32mPASSED\u001b[0m\u001b[32m      [ 50%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_tanh_backward[cuda-shape0] \u001b[32mPASSED\u001b[0m\u001b[32m     [ 51%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_tanh_backward[cuda-shape1] \u001b[32mPASSED\u001b[0m\u001b[32m     [ 52%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_stack[cpu-shape0-0-1] \u001b[32mPASSED\u001b[0m\u001b[32m          [ 53%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_stack[cpu-shape1-0-2] \u001b[32mPASSED\u001b[0m\u001b[32m          [ 54%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_stack[cpu-shape2-2-5] \u001b[32mPASSED\u001b[0m\u001b[32m          [ 55%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_stack[cuda-shape0-0-1] \u001b[32mPASSED\u001b[0m\u001b[32m         [ 55%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_stack[cuda-shape1-0-2] \u001b[32mPASSED\u001b[0m\u001b[32m         [ 56%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_stack[cuda-shape2-2-5] \u001b[32mPASSED\u001b[0m\u001b[32m         [ 57%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_stack_backward[cpu-shape0-0-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 58%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_stack_backward[cpu-shape1-0-2] \u001b[32mPASSED\u001b[0m\u001b[32m [ 59%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_stack_backward[cpu-shape2-2-5] \u001b[32mPASSED\u001b[0m\u001b[32m [ 60%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_stack_backward[cuda-shape0-0-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 61%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_stack_backward[cuda-shape1-0-2] \u001b[32mPASSED\u001b[0m\u001b[32m [ 61%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_stack_backward[cuda-shape2-2-5] \u001b[32mPASSED\u001b[0m\u001b[32m [ 62%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_summation[cpu-shape0-None] \u001b[32mPASSED\u001b[0m\u001b[32m     [ 63%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_summation[cpu-shape1-0] \u001b[32mPASSED\u001b[0m\u001b[32m        [ 64%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_summation[cpu-shape2-1] \u001b[32mPASSED\u001b[0m\u001b[32m        [ 65%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_summation[cpu-shape3-2] \u001b[32mPASSED\u001b[0m\u001b[32m        [ 66%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_summation[cuda-shape0-None] \u001b[32mPASSED\u001b[0m\u001b[32m    [ 66%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_summation[cuda-shape1-0] \u001b[32mPASSED\u001b[0m\u001b[32m       [ 67%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_summation[cuda-shape2-1] \u001b[32mPASSED\u001b[0m\u001b[32m       [ 68%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_summation[cuda-shape3-2] \u001b[32mPASSED\u001b[0m\u001b[32m       [ 69%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_summation_backward[cpu-shape0-None] \u001b[32mPASSED\u001b[0m\u001b[32m [ 70%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_summation_backward[cpu-shape1-0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 71%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_summation_backward[cpu-shape2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 72%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_summation_backward[cpu-shape3-2] \u001b[32mPASSED\u001b[0m\u001b[32m [ 72%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_summation_backward[cuda-shape0-None] \u001b[32mPASSED\u001b[0m\u001b[32m [ 73%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_summation_backward[cuda-shape1-0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 74%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_summation_backward[cuda-shape2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 75%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_summation_backward[cuda-shape3-2] \u001b[32mPASSED\u001b[0m\u001b[32m [ 76%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_broadcast_to[cpu-shape0-shape_to0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 77%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_broadcast_to[cpu-shape1-shape_to1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 77%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_broadcast_to[cuda-shape0-shape_to0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 78%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_broadcast_to[cuda-shape1-shape_to1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 79%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_reshape[cpu-shape0-shape_to0] \u001b[32mPASSED\u001b[0m\u001b[32m  [ 80%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_reshape[cpu-shape1-shape_to1] \u001b[32mPASSED\u001b[0m\u001b[32m  [ 81%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_reshape[cuda-shape0-shape_to0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 82%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_reshape[cuda-shape1-shape_to1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 83%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_transpose[cpu-axes0-shape0] \u001b[32mPASSED\u001b[0m\u001b[32m    [ 83%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_transpose[cpu-axes0-shape1] \u001b[32mPASSED\u001b[0m\u001b[32m    [ 84%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_transpose[cpu-axes1-shape0] \u001b[32mPASSED\u001b[0m\u001b[32m    [ 85%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_transpose[cpu-axes1-shape1] \u001b[32mPASSED\u001b[0m\u001b[32m    [ 86%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_transpose[cpu-None-shape0] \u001b[32mPASSED\u001b[0m\u001b[32m     [ 87%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_transpose[cpu-None-shape1] \u001b[32mPASSED\u001b[0m\u001b[32m     [ 88%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_transpose[cuda-axes0-shape0] \u001b[32mPASSED\u001b[0m\u001b[32m   [ 88%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_transpose[cuda-axes0-shape1] \u001b[32mPASSED\u001b[0m\u001b[32m   [ 89%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_transpose[cuda-axes1-shape0] \u001b[32mPASSED\u001b[0m\u001b[32m   [ 90%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_transpose[cuda-axes1-shape1] \u001b[32mPASSED\u001b[0m\u001b[32m   [ 91%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_transpose[cuda-None-shape0] \u001b[32mPASSED\u001b[0m\u001b[32m    [ 92%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_transpose[cuda-None-shape1] \u001b[32mPASSED\u001b[0m\u001b[32m    [ 93%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_logsumexp[cpu-shape0-None] \u001b[32mPASSED\u001b[0m\u001b[32m     [ 94%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_logsumexp[cpu-shape1-0] \u001b[32mPASSED\u001b[0m\u001b[32m        [ 94%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_logsumexp[cpu-shape2-1] \u001b[32mPASSED\u001b[0m\u001b[32m        [ 95%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_logsumexp[cpu-shape3-2] \u001b[32mPASSED\u001b[0m\u001b[32m        [ 96%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_logsumexp[cuda-shape0-None] \u001b[32mPASSED\u001b[0m\u001b[32m    [ 97%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_logsumexp[cuda-shape1-0] \u001b[32mPASSED\u001b[0m\u001b[32m       [ 98%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_logsumexp[cuda-shape2-1] \u001b[32mPASSED\u001b[0m\u001b[32m       [ 99%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_logsumexp[cuda-shape3-2] \u001b[32mPASSED\u001b[0m\u001b[32m       [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m===================== \u001b[32m\u001b[1m118 passed\u001b[0m, \u001b[33m1685 deselected\u001b[0m\u001b[32m in 3.76s\u001b[0m\u001b[32m =====================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m pytest -l -v -k \"nd_backend\"\n",
    "# %pdb on\n",
    "# !python -m pytest -l -v -k \"test_stack and cpu\" -s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: CIFAR-10 dataset [10 points]\n",
    "\n",
    "Next, you will write support for the [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) image classification dataset, which consists of 60,000 32x32 color images in 10 classes, with 6,000 images per class. There are 50k training images and 10k test images. \n",
    "\n",
    "Start by implementing the `__init__` function in the `CIFAR10Dataset` class in `python/needle/data/datasets/cifar10_dataset.py`. You can read in the link above how to properly read the CIFAR-10 dataset files you downloaded at the beginning of the homework. Also fill in `__getitem__` and `__len__`. Note that the return shape of the data from `__getitem__` should be in order (3, 32, 32).\n",
    "\n",
    "Copy `python/needle/data/data_transforms.py` and `python/needle/data/data_basic.py` from previous homeworks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.11.10, pytest-8.3.4, pluggy-1.5.0 -- /opt/conda/envs/train/bin/python\n",
      "cachedir: .pytest_cache\n",
      "hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase(PosixPath('/mnt/zhumingzhu/work/00test/hw4/.hypothesis/examples'))\n",
      "rootdir: /mnt/zhumingzhu/work/00test/hw4\n",
      "plugins: hypothesis-6.115.5\n",
      "collected 1803 items / 1793 deselected / 10 selected                           \u001b[0m\u001b[1m\n",
      "\n",
      "tests/hw4/test_cifar_ptb_data.py::test_cifar10_dataset[True] \u001b[32mPASSED\u001b[0m\u001b[32m      [ 10%]\u001b[0m\n",
      "tests/hw4/test_cifar_ptb_data.py::test_cifar10_dataset[False] \u001b[32mPASSED\u001b[0m\u001b[32m     [ 20%]\u001b[0m\n",
      "tests/hw4/test_cifar_ptb_data.py::test_cifar10_loader[cpu-True-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 30%]\u001b[0m\n",
      "tests/hw4/test_cifar_ptb_data.py::test_cifar10_loader[cpu-True-15] \u001b[31mFAILED\u001b[0m\u001b[31m [ 40%]\u001b[0m\n",
      "tests/hw4/test_cifar_ptb_data.py::test_cifar10_loader[cpu-False-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 50%]\u001b[0m\n",
      "tests/hw4/test_cifar_ptb_data.py::test_cifar10_loader[cpu-False-15] \u001b[31mFAILED\u001b[0m\u001b[31m [ 60%]\u001b[0m\n",
      "tests/hw4/test_cifar_ptb_data.py::test_cifar10_loader[cuda-True-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 70%]\u001b[0m\n",
      "tests/hw4/test_cifar_ptb_data.py::test_cifar10_loader[cuda-True-15] \u001b[31mFAILED\u001b[0m\u001b[31m [ 80%]\u001b[0m\n",
      "tests/hw4/test_cifar_ptb_data.py::test_cifar10_loader[cuda-False-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 90%]\u001b[0m\n",
      "tests/hw4/test_cifar_ptb_data.py::test_cifar10_loader[cuda-False-15] \u001b[31mFAILED\u001b[0m\u001b[31m [100%]\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m_______________________ test_cifar10_loader[cpu-True-1] ________________________\u001b[0m\n",
      "\n",
      "batch_size = 1, train = True, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mtrain\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRAIN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_cifar10_loader\u001b[39;49;00m(batch_size, train, device):\u001b[90m\u001b[39;49;00m\n",
      "        cifar10_train_dataset = ndl.data.CIFAR10Dataset(\u001b[33m\"\u001b[39;49;00m\u001b[33mdata/cifar-10-batches-py\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, train=\u001b[94mTrue\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        train_loader = ndl.data.DataLoader(cifar10_train_dataset, batch_size)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mfor\u001b[39;49;00m (X, y) \u001b[95min\u001b[39;49;00m train_loader:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mbreak\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94massert\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(X.cached_data, nd.NDArray), \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mExpected X.cached_data to be nd.NDArray, but got \u001b[39;49;00m\u001b[33m{\u001b[39;49;00m\u001b[96mtype\u001b[39;49;00m(X.cached_data)\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AssertionError: Expected X.cached_data to be nd.NDArray, but got <class 'numpy.ndarray'>\u001b[0m\n",
      "\u001b[1m\u001b[31mE       assert False\u001b[0m\n",
      "\u001b[1m\u001b[31mE        +  where False = isinstance(array([[[[0.23137255, 0.16862745, 0.19607843, ..., 0.61960784,\\n          0.59607843, 0.58039216],\\n         [0.0627451 , 0.        , 0.07058824, ..., 0.48235294,\\n          0.46666667, 0.47843137],\\n         [0.09803922, 0.0627451 , 0.19215686, ..., 0.4627451 ,\\n          0.47058824, 0.42745098],\\n         ...,\\n         [0.81568627, 0.78823529, 0.77647059, ..., 0.62745098,\\n          0.21960784, 0.20784314],\\n         [0.70588235, 0.67843137, 0.72941176, ..., 0.72156863,\\n          0.38039216, 0.3254902 ],\\n         [0.69411765, 0.65882353, 0.70196078, ..., 0.84705882,\\n          0.59215686, 0.48235294]],\\n\\n        [[0.24313725, 0.18039216, 0.18823529, ..., 0.51764706,\\n          0.49019608, 0.48627451],\\n         [0.07843137, 0.        , 0.03137255, ..., 0.34509804,\\n          0.3254902 , 0.34117647],\\n         [0.09411765, 0.02745098, 0.10588235, ..., 0.32941176,\\n          0.32941176, 0.28627451],\\n         ...,\\n         [0.66666667, 0.6       , 0.63137255, ..., 0.52156863,\\n          0.12156863, 0.13333333],\\n         [0.54509804, 0.48235294, 0.56470588, ..., 0.58039216,\\n          0.24313725, 0.20784314],\\n         [0.56470588, 0.50588235, 0.55686275, ..., 0.72156863,\\n          0.4627451 , 0.36078431]],\\n\\n        [[0.24705882, 0.17647059, 0.16862745, ..., 0.42352941,\\n          0.4       , 0.40392157],\\n         [0.07843137, 0.        , 0.        , ..., 0.21568627,\\n          0.19607843, 0.22352941],\\n         [0.08235294, 0.        , 0.03137255, ..., 0.19607843,\\n          0.19607843, 0.16470588],\\n         ...,\\n         [0.37647059, 0.13333333, 0.10196078, ..., 0.2745098 ,\\n          0.02745098, 0.07843137],\\n         [0.37647059, 0.16470588, 0.11764706, ..., 0.36862745,\\n          0.13333333, 0.13333333],\\n         [0.45490196, 0.36862745, 0.34117647, ..., 0.54901961,\\n          0.32941176, 0.28235294]]]]), <class 'needle.backend_ndarray.ndarray.NDArray'>)\u001b[0m\n",
      "\u001b[1m\u001b[31mE        +    where array([[[[0.23137255, 0.16862745, 0.19607843, ..., 0.61960784,\\n          0.59607843, 0.58039216],\\n         [0.0627451 , 0.        , 0.07058824, ..., 0.48235294,\\n          0.46666667, 0.47843137],\\n         [0.09803922, 0.0627451 , 0.19215686, ..., 0.4627451 ,\\n          0.47058824, 0.42745098],\\n         ...,\\n         [0.81568627, 0.78823529, 0.77647059, ..., 0.62745098,\\n          0.21960784, 0.20784314],\\n         [0.70588235, 0.67843137, 0.72941176, ..., 0.72156863,\\n          0.38039216, 0.3254902 ],\\n         [0.69411765, 0.65882353, 0.70196078, ..., 0.84705882,\\n          0.59215686, 0.48235294]],\\n\\n        [[0.24313725, 0.18039216, 0.18823529, ..., 0.51764706,\\n          0.49019608, 0.48627451],\\n         [0.07843137, 0.        , 0.03137255, ..., 0.34509804,\\n          0.3254902 , 0.34117647],\\n         [0.09411765, 0.02745098, 0.10588235, ..., 0.32941176,\\n          0.32941176, 0.28627451],\\n         ...,\\n         [0.66666667, 0.6       , 0.63137255, ..., 0.52156863,\\n          0.12156863, 0.13333333],\\n         [0.54509804, 0.48235294, 0.56470588, ..., 0.58039216,\\n          0.24313725, 0.20784314],\\n         [0.56470588, 0.50588235, 0.55686275, ..., 0.72156863,\\n          0.4627451 , 0.36078431]],\\n\\n        [[0.24705882, 0.17647059, 0.16862745, ..., 0.42352941,\\n          0.4       , 0.40392157],\\n         [0.07843137, 0.        , 0.        , ..., 0.21568627,\\n          0.19607843, 0.22352941],\\n         [0.08235294, 0.        , 0.03137255, ..., 0.19607843,\\n          0.19607843, 0.16470588],\\n         ...,\\n         [0.37647059, 0.13333333, 0.10196078, ..., 0.2745098 ,\\n          0.02745098, 0.07843137],\\n         [0.37647059, 0.16470588, 0.11764706, ..., 0.36862745,\\n          0.13333333, 0.13333333],\\n         [0.45490196, 0.36862745, 0.34117647, ..., 0.54901961,\\n          0.32941176, 0.28235294]]]]) = needle.Tensor([[[[0.23137255 0.16862745 0.19607843 ... 0.61960784 0.59607843\\n    0.58039216]\\n   [0.0627451  0.         0.07058824 ... 0.48235294 0.46666667\\n    0.47843137]\\n   [0.09803922 0.0627451  0.19215686 ... 0.4627451  0.47058824\\n    0.42745098]\\n   ...\\n   [0.81568627 0.78823529 0.77647059 ... 0.62745098 0.21960784\\n    0.20784314]\\n   [0.70588235 0.67843137 0.72941176 ... 0.72156863 0.38039216\\n    0.3254902 ]\\n   [0.69411765 0.65882353 0.70196078 ... 0.84705882 0.59215686\\n    0.48235294]]\\n\\n  [[0.24313725 0.18039216 0.18823529 ... 0.51764706 0.49019608\\n    0.48627451]\\n   [0.07843137 0.         0.03137255 ... 0.34509804 0.3254902\\n    0.34117647]\\n   [0.09411765 0.02745098 0.10588235 ... 0.32941176 0.32941176\\n    0.28627451]\\n   ...\\n   [0.66666667 0.6        0.63137255 ... 0.52156863 0.12156863\\n    0.13333333]\\n   [0.54509804 0.48235294 0.56470588 ... 0.58039216 0.24313725\\n    0.20784314]\\n   [0.56470588 0.50588235 0.55686275 ... 0.72156863 0.4627451\\n    0.36078431]]\\n\\n  [[0.24705882 0.17647059 0.16862745 ... 0.42352941 0.4\\n    0.40392157]\\n   [0.07843137 0.         0.         ... 0.21568627 0.19607843\\n    0.22352941]\\n   [0.08235294 0.         0.03137255 ... 0.19607843 0.19607843\\n    0.16470588]\\n   ...\\n   [0.37647059 0.13333333 0.10196078 ... 0.2745098  0.02745098\\n    0.07843137]\\n   [0.37647059 0.16470588 0.11764706 ... 0.36862745 0.13333333\\n    0.13333333]\\n   [0.45490196 0.36862745 0.34117647 ... 0.54901961 0.32941176\\n    0.28235294]]]]).cached_data\u001b[0m\n",
      "\u001b[1m\u001b[31mE        +    and   <class 'needle.backend_ndarray.ndarray.NDArray'> = nd.NDArray\u001b[0m\n",
      "\n",
      "X          = needle.Tensor([[[[0.23137255 0.16862745 0.19607843 ... 0.61960784 0.59607843\n",
      "    0.58039216]\n",
      "   [0.0627451  0.        ....36862745 0.13333333\n",
      "    0.13333333]\n",
      "   [0.45490196 0.36862745 0.34117647 ... 0.54901961 0.32941176\n",
      "    0.28235294]]]])\n",
      "batch_size = 1\n",
      "cifar10_train_dataset = <needle.data.datasets.cifar10_dataset.CIFAR10Dataset object at 0x7f95567e8910>\n",
      "device     = cpu()\n",
      "train      = True\n",
      "train_loader = <needle.data.data_basic.DataLoader object at 0x7f9613c2f7d0>\n",
      "y          = needle.Tensor([6])\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_cifar_ptb_data.py\u001b[0m:43: AssertionError\n",
      "\u001b[31m\u001b[1m_______________________ test_cifar10_loader[cpu-True-15] _______________________\u001b[0m\n",
      "\n",
      "batch_size = 15, train = True, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mtrain\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRAIN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_cifar10_loader\u001b[39;49;00m(batch_size, train, device):\u001b[90m\u001b[39;49;00m\n",
      "        cifar10_train_dataset = ndl.data.CIFAR10Dataset(\u001b[33m\"\u001b[39;49;00m\u001b[33mdata/cifar-10-batches-py\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, train=\u001b[94mTrue\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        train_loader = ndl.data.DataLoader(cifar10_train_dataset, batch_size)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mfor\u001b[39;49;00m (X, y) \u001b[95min\u001b[39;49;00m train_loader:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mbreak\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94massert\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(X.cached_data, nd.NDArray), \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mExpected X.cached_data to be nd.NDArray, but got \u001b[39;49;00m\u001b[33m{\u001b[39;49;00m\u001b[96mtype\u001b[39;49;00m(X.cached_data)\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AssertionError: Expected X.cached_data to be nd.NDArray, but got <class 'numpy.ndarray'>\u001b[0m\n",
      "\u001b[1m\u001b[31mE       assert False\u001b[0m\n",
      "\u001b[1m\u001b[31mE        +  where False = isinstance(array([[[[0.23137255, 0.16862745, 0.19607843, ..., 0.61960784,\\n          0.59607843, 0.58039216],\\n         [0.0627451 , 0.        , 0.07058824, ..., 0.48235294,\\n          0.46666667, 0.47843137],\\n         [0.09803922, 0.0627451 , 0.19215686, ..., 0.4627451 ,\\n          0.47058824, 0.42745098],\\n         ...,\\n         [0.81568627, 0.78823529, 0.77647059, ..., 0.62745098,\\n          0.21960784, 0.20784314],\\n         [0.70588235, 0.67843137, 0.72941176, ..., 0.72156863,\\n          0.38039216, 0.3254902 ],\\n         [0.69411765, 0.65882353, 0.70196078, ..., 0.84705882,\\n          0.59215686, 0.48235294]],\\n\\n        [[0.24313725, 0.18039216, 0.18823529, ..., 0.51764706,\\n          0.49019608, 0.48627451],\\n         [0.07843137, 0.        , 0.03137255, ..., 0.34509804,\\n          0.3254902 , 0.34117647],\\n         [0.09411765, 0.02745098, 0.10588235, ..., 0.32941176,\\n          0.32941176, 0.28627451],\\n         ...,\\n         [0.66666667, 0.6       , 0.63137255, ..., 0.52156863,\\n          0.12156863, 0.13333333],\\n         [0.54509804, 0.48235294, 0.56470588, ..., 0.58039216,\\n          0.24313725, 0.20784314],\\n         [0.56470588, 0.50588235, 0.55686275, ..., 0.72156863,\\n          0.4627451 , 0.360...[0.57254902, 0.56470588, 0.56470588, ..., 0.64705882,\\n          0.63921569, 0.64705882],\\n         [0.56862745, 0.56078431, 0.56078431, ..., 0.63137255,\\n          0.63529412, 0.63137255],\\n         [0.57647059, 0.56862745, 0.56862745, ..., 0.58823529,\\n          0.52156863, 0.45882353],\\n         ...,\\n         [0.61176471, 0.61176471, 0.61568627, ..., 0.49803922,\\n          0.49411765, 0.53333333],\\n         [0.59607843, 0.6       , 0.61960784, ..., 0.49019608,\\n          0.52156863, 0.51764706],\\n         [0.61960784, 0.61176471, 0.61960784, ..., 0.57254902,\\n          0.59215686, 0.58823529]],\\n\\n        [[0.75294118, 0.74117647, 0.74117647, ..., 0.64705882,\\n          0.63921569, 0.64705882],\\n         [0.7372549 , 0.72941176, 0.72941176, ..., 0.62352941,\\n          0.62352941, 0.61568627],\\n         [0.74117647, 0.72941176, 0.73333333, ..., 0.57647059,\\n          0.50588235, 0.43529412],\\n         ...,\\n         [0.58823529, 0.59215686, 0.59607843, ..., 0.43921569,\\n          0.44705882, 0.49803922],\\n         [0.58431373, 0.58823529, 0.60784314, ..., 0.45882353,\\n          0.48627451, 0.47058824],\\n         [0.60784314, 0.6       , 0.60784314, ..., 0.54509804,\\n          0.55294118, 0.5372549 ]]]]), <class 'needle.backend_ndarray.ndarray.NDArray'>)\u001b[0m\n",
      "\u001b[1m\u001b[31mE        +    where array([[[[0.23137255, 0.16862745, 0.19607843, ..., 0.61960784,\\n          0.59607843, 0.58039216],\\n         [0.0627451 , 0.        , 0.07058824, ..., 0.48235294,\\n          0.46666667, 0.47843137],\\n         [0.09803922, 0.0627451 , 0.19215686, ..., 0.4627451 ,\\n          0.47058824, 0.42745098],\\n         ...,\\n         [0.81568627, 0.78823529, 0.77647059, ..., 0.62745098,\\n          0.21960784, 0.20784314],\\n         [0.70588235, 0.67843137, 0.72941176, ..., 0.72156863,\\n          0.38039216, 0.3254902 ],\\n         [0.69411765, 0.65882353, 0.70196078, ..., 0.84705882,\\n          0.59215686, 0.48235294]],\\n\\n        [[0.24313725, 0.18039216, 0.18823529, ..., 0.51764706,\\n          0.49019608, 0.48627451],\\n         [0.07843137, 0.        , 0.03137255, ..., 0.34509804,\\n          0.3254902 , 0.34117647],\\n         [0.09411765, 0.02745098, 0.10588235, ..., 0.32941176,\\n          0.32941176, 0.28627451],\\n         ...,\\n         [0.66666667, 0.6       , 0.63137255, ..., 0.52156863,\\n          0.12156863, 0.13333333],\\n         [0.54509804, 0.48235294, 0.56470588, ..., 0.58039216,\\n          0.24313725, 0.20784314],\\n         [0.56470588, 0.50588235, 0.55686275, ..., 0.72156863,\\n          0.4627451 , 0.360...[0.57254902, 0.56470588, 0.56470588, ..., 0.64705882,\\n          0.63921569, 0.64705882],\\n         [0.56862745, 0.56078431, 0.56078431, ..., 0.63137255,\\n          0.63529412, 0.63137255],\\n         [0.57647059, 0.56862745, 0.56862745, ..., 0.58823529,\\n          0.52156863, 0.45882353],\\n         ...,\\n         [0.61176471, 0.61176471, 0.61568627, ..., 0.49803922,\\n          0.49411765, 0.53333333],\\n         [0.59607843, 0.6       , 0.61960784, ..., 0.49019608,\\n          0.52156863, 0.51764706],\\n         [0.61960784, 0.61176471, 0.61960784, ..., 0.57254902,\\n          0.59215686, 0.58823529]],\\n\\n        [[0.75294118, 0.74117647, 0.74117647, ..., 0.64705882,\\n          0.63921569, 0.64705882],\\n         [0.7372549 , 0.72941176, 0.72941176, ..., 0.62352941,\\n          0.62352941, 0.61568627],\\n         [0.74117647, 0.72941176, 0.73333333, ..., 0.57647059,\\n          0.50588235, 0.43529412],\\n         ...,\\n         [0.58823529, 0.59215686, 0.59607843, ..., 0.43921569,\\n          0.44705882, 0.49803922],\\n         [0.58431373, 0.58823529, 0.60784314, ..., 0.45882353,\\n          0.48627451, 0.47058824],\\n         [0.60784314, 0.6       , 0.60784314, ..., 0.54509804,\\n          0.55294118, 0.5372549 ]]]]) = needle.Tensor([[[[0.23137255 0.16862745 0.19607843 ... 0.61960784 0.59607843\\n    0.58039216]\\n   [0.0627451  0.         0.07058824 ... 0.48235294 0.46666667\\n    0.47843137]\\n   [0.09803922 0.0627451  0.19215686 ... 0.4627451  0.47058824\\n    0.42745098]\\n   ...\\n   [0.81568627 0.78823529 0.77647059 ... 0.62745098 0.21960784\\n    0.20784314]\\n   [0.70588235 0.67843137 0.72941176 ... 0.72156863 0.38039216\\n    0.3254902 ]\\n   [0.69411765 0.65882353 0.70196078 ... 0.84705882 0.59215686\\n    0.48235294]]\\n\\n  [[0.24313725 0.18039216 0.18823529 ... 0.51764706 0.49019608\\n    0.48627451]\\n   [0.07843137 0.         0.03137255 ... 0.34509804 0.3254902\\n    0.34117647]\\n   [0.09411765 0.02745098 0.10588235 ... 0.32941176 0.32941176\\n    0.28627451]\\n   ...\\n   [0.66666667 0.6        0.63137255 ... 0.52156863 0.12156863\\n    0.13333333]\\n   [0.54509804 0.48235294 0.56470588 ... 0.58039216 0.24313725\\n    0.20784314]\\n   [0.56470588 0.50588235 0.55686275 ... 0.72156863 0.4627451\\n    0.36078431]]\\n\\n  [[0.24705882 0.17647059 0.16862745 ... 0.42352941 0.4\\n    0.40392157]\\n   [0.07843137 0.         0.         ... 0.21568627 0.19607843\\n    0.22352941]\\n   [0.08235294 0.         0.03137255 ... 0.19607843 0.19607843\\n    0....65098039 0.65098039 0.65882353 ... 0.5372549  0.5372549\\n    0.57647059]\\n   [0.63137255 0.63529412 0.65490196 ... 0.51372549 0.54117647\\n    0.5372549 ]\\n   [0.64313725 0.63529412 0.64313725 ... 0.6        0.61568627\\n    0.61176471]]\\n\\n  [[0.57254902 0.56470588 0.56470588 ... 0.64705882 0.63921569\\n    0.64705882]\\n   [0.56862745 0.56078431 0.56078431 ... 0.63137255 0.63529412\\n    0.63137255]\\n   [0.57647059 0.56862745 0.56862745 ... 0.58823529 0.52156863\\n    0.45882353]\\n   ...\\n   [0.61176471 0.61176471 0.61568627 ... 0.49803922 0.49411765\\n    0.53333333]\\n   [0.59607843 0.6        0.61960784 ... 0.49019608 0.52156863\\n    0.51764706]\\n   [0.61960784 0.61176471 0.61960784 ... 0.57254902 0.59215686\\n    0.58823529]]\\n\\n  [[0.75294118 0.74117647 0.74117647 ... 0.64705882 0.63921569\\n    0.64705882]\\n   [0.7372549  0.72941176 0.72941176 ... 0.62352941 0.62352941\\n    0.61568627]\\n   [0.74117647 0.72941176 0.73333333 ... 0.57647059 0.50588235\\n    0.43529412]\\n   ...\\n   [0.58823529 0.59215686 0.59607843 ... 0.43921569 0.44705882\\n    0.49803922]\\n   [0.58431373 0.58823529 0.60784314 ... 0.45882353 0.48627451\\n    0.47058824]\\n   [0.60784314 0.6        0.60784314 ... 0.54509804 0.55294118\\n    0.5372549 ]]]]).cached_data\u001b[0m\n",
      "\u001b[1m\u001b[31mE        +    and   <class 'needle.backend_ndarray.ndarray.NDArray'> = nd.NDArray\u001b[0m\n",
      "\n",
      "X          = needle.Tensor([[[[0.23137255 0.16862745 0.19607843 ... 0.61960784 0.59607843\n",
      "    0.58039216]\n",
      "   [0.0627451  0.        ....45882353 0.48627451\n",
      "    0.47058824]\n",
      "   [0.60784314 0.6        0.60784314 ... 0.54509804 0.55294118\n",
      "    0.5372549 ]]]])\n",
      "batch_size = 15\n",
      "cifar10_train_dataset = <needle.data.datasets.cifar10_dataset.CIFAR10Dataset object at 0x7f9556a0e350>\n",
      "device     = cpu()\n",
      "train      = True\n",
      "train_loader = <needle.data.data_basic.DataLoader object at 0x7f95570bc7d0>\n",
      "y          = needle.Tensor([6 9 9 4 1 1 2 7 8 3 4 7 7 2 9])\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_cifar_ptb_data.py\u001b[0m:43: AssertionError\n",
      "\u001b[31m\u001b[1m_______________________ test_cifar10_loader[cpu-False-1] _______________________\u001b[0m\n",
      "\n",
      "batch_size = 1, train = False, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mtrain\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRAIN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_cifar10_loader\u001b[39;49;00m(batch_size, train, device):\u001b[90m\u001b[39;49;00m\n",
      "        cifar10_train_dataset = ndl.data.CIFAR10Dataset(\u001b[33m\"\u001b[39;49;00m\u001b[33mdata/cifar-10-batches-py\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, train=\u001b[94mTrue\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        train_loader = ndl.data.DataLoader(cifar10_train_dataset, batch_size)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mfor\u001b[39;49;00m (X, y) \u001b[95min\u001b[39;49;00m train_loader:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mbreak\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94massert\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(X.cached_data, nd.NDArray), \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mExpected X.cached_data to be nd.NDArray, but got \u001b[39;49;00m\u001b[33m{\u001b[39;49;00m\u001b[96mtype\u001b[39;49;00m(X.cached_data)\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AssertionError: Expected X.cached_data to be nd.NDArray, but got <class 'numpy.ndarray'>\u001b[0m\n",
      "\u001b[1m\u001b[31mE       assert False\u001b[0m\n",
      "\u001b[1m\u001b[31mE        +  where False = isinstance(array([[[[0.23137255, 0.16862745, 0.19607843, ..., 0.61960784,\\n          0.59607843, 0.58039216],\\n         [0.0627451 , 0.        , 0.07058824, ..., 0.48235294,\\n          0.46666667, 0.47843137],\\n         [0.09803922, 0.0627451 , 0.19215686, ..., 0.4627451 ,\\n          0.47058824, 0.42745098],\\n         ...,\\n         [0.81568627, 0.78823529, 0.77647059, ..., 0.62745098,\\n          0.21960784, 0.20784314],\\n         [0.70588235, 0.67843137, 0.72941176, ..., 0.72156863,\\n          0.38039216, 0.3254902 ],\\n         [0.69411765, 0.65882353, 0.70196078, ..., 0.84705882,\\n          0.59215686, 0.48235294]],\\n\\n        [[0.24313725, 0.18039216, 0.18823529, ..., 0.51764706,\\n          0.49019608, 0.48627451],\\n         [0.07843137, 0.        , 0.03137255, ..., 0.34509804,\\n          0.3254902 , 0.34117647],\\n         [0.09411765, 0.02745098, 0.10588235, ..., 0.32941176,\\n          0.32941176, 0.28627451],\\n         ...,\\n         [0.66666667, 0.6       , 0.63137255, ..., 0.52156863,\\n          0.12156863, 0.13333333],\\n         [0.54509804, 0.48235294, 0.56470588, ..., 0.58039216,\\n          0.24313725, 0.20784314],\\n         [0.56470588, 0.50588235, 0.55686275, ..., 0.72156863,\\n          0.4627451 , 0.36078431]],\\n\\n        [[0.24705882, 0.17647059, 0.16862745, ..., 0.42352941,\\n          0.4       , 0.40392157],\\n         [0.07843137, 0.        , 0.        , ..., 0.21568627,\\n          0.19607843, 0.22352941],\\n         [0.08235294, 0.        , 0.03137255, ..., 0.19607843,\\n          0.19607843, 0.16470588],\\n         ...,\\n         [0.37647059, 0.13333333, 0.10196078, ..., 0.2745098 ,\\n          0.02745098, 0.07843137],\\n         [0.37647059, 0.16470588, 0.11764706, ..., 0.36862745,\\n          0.13333333, 0.13333333],\\n         [0.45490196, 0.36862745, 0.34117647, ..., 0.54901961,\\n          0.32941176, 0.28235294]]]]), <class 'needle.backend_ndarray.ndarray.NDArray'>)\u001b[0m\n",
      "\u001b[1m\u001b[31mE        +    where array([[[[0.23137255, 0.16862745, 0.19607843, ..., 0.61960784,\\n          0.59607843, 0.58039216],\\n         [0.0627451 , 0.        , 0.07058824, ..., 0.48235294,\\n          0.46666667, 0.47843137],\\n         [0.09803922, 0.0627451 , 0.19215686, ..., 0.4627451 ,\\n          0.47058824, 0.42745098],\\n         ...,\\n         [0.81568627, 0.78823529, 0.77647059, ..., 0.62745098,\\n          0.21960784, 0.20784314],\\n         [0.70588235, 0.67843137, 0.72941176, ..., 0.72156863,\\n          0.38039216, 0.3254902 ],\\n         [0.69411765, 0.65882353, 0.70196078, ..., 0.84705882,\\n          0.59215686, 0.48235294]],\\n\\n        [[0.24313725, 0.18039216, 0.18823529, ..., 0.51764706,\\n          0.49019608, 0.48627451],\\n         [0.07843137, 0.        , 0.03137255, ..., 0.34509804,\\n          0.3254902 , 0.34117647],\\n         [0.09411765, 0.02745098, 0.10588235, ..., 0.32941176,\\n          0.32941176, 0.28627451],\\n         ...,\\n         [0.66666667, 0.6       , 0.63137255, ..., 0.52156863,\\n          0.12156863, 0.13333333],\\n         [0.54509804, 0.48235294, 0.56470588, ..., 0.58039216,\\n          0.24313725, 0.20784314],\\n         [0.56470588, 0.50588235, 0.55686275, ..., 0.72156863,\\n          0.4627451 , 0.36078431]],\\n\\n        [[0.24705882, 0.17647059, 0.16862745, ..., 0.42352941,\\n          0.4       , 0.40392157],\\n         [0.07843137, 0.        , 0.        , ..., 0.21568627,\\n          0.19607843, 0.22352941],\\n         [0.08235294, 0.        , 0.03137255, ..., 0.19607843,\\n          0.19607843, 0.16470588],\\n         ...,\\n         [0.37647059, 0.13333333, 0.10196078, ..., 0.2745098 ,\\n          0.02745098, 0.07843137],\\n         [0.37647059, 0.16470588, 0.11764706, ..., 0.36862745,\\n          0.13333333, 0.13333333],\\n         [0.45490196, 0.36862745, 0.34117647, ..., 0.54901961,\\n          0.32941176, 0.28235294]]]]) = needle.Tensor([[[[0.23137255 0.16862745 0.19607843 ... 0.61960784 0.59607843\\n    0.58039216]\\n   [0.0627451  0.         0.07058824 ... 0.48235294 0.46666667\\n    0.47843137]\\n   [0.09803922 0.0627451  0.19215686 ... 0.4627451  0.47058824\\n    0.42745098]\\n   ...\\n   [0.81568627 0.78823529 0.77647059 ... 0.62745098 0.21960784\\n    0.20784314]\\n   [0.70588235 0.67843137 0.72941176 ... 0.72156863 0.38039216\\n    0.3254902 ]\\n   [0.69411765 0.65882353 0.70196078 ... 0.84705882 0.59215686\\n    0.48235294]]\\n\\n  [[0.24313725 0.18039216 0.18823529 ... 0.51764706 0.49019608\\n    0.48627451]\\n   [0.07843137 0.         0.03137255 ... 0.34509804 0.3254902\\n    0.34117647]\\n   [0.09411765 0.02745098 0.10588235 ... 0.32941176 0.32941176\\n    0.28627451]\\n   ...\\n   [0.66666667 0.6        0.63137255 ... 0.52156863 0.12156863\\n    0.13333333]\\n   [0.54509804 0.48235294 0.56470588 ... 0.58039216 0.24313725\\n    0.20784314]\\n   [0.56470588 0.50588235 0.55686275 ... 0.72156863 0.4627451\\n    0.36078431]]\\n\\n  [[0.24705882 0.17647059 0.16862745 ... 0.42352941 0.4\\n    0.40392157]\\n   [0.07843137 0.         0.         ... 0.21568627 0.19607843\\n    0.22352941]\\n   [0.08235294 0.         0.03137255 ... 0.19607843 0.19607843\\n    0.16470588]\\n   ...\\n   [0.37647059 0.13333333 0.10196078 ... 0.2745098  0.02745098\\n    0.07843137]\\n   [0.37647059 0.16470588 0.11764706 ... 0.36862745 0.13333333\\n    0.13333333]\\n   [0.45490196 0.36862745 0.34117647 ... 0.54901961 0.32941176\\n    0.28235294]]]]).cached_data\u001b[0m\n",
      "\u001b[1m\u001b[31mE        +    and   <class 'needle.backend_ndarray.ndarray.NDArray'> = nd.NDArray\u001b[0m\n",
      "\n",
      "X          = needle.Tensor([[[[0.23137255 0.16862745 0.19607843 ... 0.61960784 0.59607843\n",
      "    0.58039216]\n",
      "   [0.0627451  0.        ....36862745 0.13333333\n",
      "    0.13333333]\n",
      "   [0.45490196 0.36862745 0.34117647 ... 0.54901961 0.32941176\n",
      "    0.28235294]]]])\n",
      "batch_size = 1\n",
      "cifar10_train_dataset = <needle.data.datasets.cifar10_dataset.CIFAR10Dataset object at 0x7f95565c99d0>\n",
      "device     = cpu()\n",
      "train      = False\n",
      "train_loader = <needle.data.data_basic.DataLoader object at 0x7f9556943ad0>\n",
      "y          = needle.Tensor([6])\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_cifar_ptb_data.py\u001b[0m:43: AssertionError\n",
      "\u001b[31m\u001b[1m______________________ test_cifar10_loader[cpu-False-15] _______________________\u001b[0m\n",
      "\n",
      "batch_size = 15, train = False, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mtrain\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRAIN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_cifar10_loader\u001b[39;49;00m(batch_size, train, device):\u001b[90m\u001b[39;49;00m\n",
      "        cifar10_train_dataset = ndl.data.CIFAR10Dataset(\u001b[33m\"\u001b[39;49;00m\u001b[33mdata/cifar-10-batches-py\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, train=\u001b[94mTrue\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        train_loader = ndl.data.DataLoader(cifar10_train_dataset, batch_size)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mfor\u001b[39;49;00m (X, y) \u001b[95min\u001b[39;49;00m train_loader:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mbreak\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94massert\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(X.cached_data, nd.NDArray), \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mExpected X.cached_data to be nd.NDArray, but got \u001b[39;49;00m\u001b[33m{\u001b[39;49;00m\u001b[96mtype\u001b[39;49;00m(X.cached_data)\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AssertionError: Expected X.cached_data to be nd.NDArray, but got <class 'numpy.ndarray'>\u001b[0m\n",
      "\u001b[1m\u001b[31mE       assert False\u001b[0m\n",
      "\u001b[1m\u001b[31mE        +  where False = isinstance(array([[[[0.23137255, 0.16862745, 0.19607843, ..., 0.61960784,\\n          0.59607843, 0.58039216],\\n         [0.0627451 , 0.        , 0.07058824, ..., 0.48235294,\\n          0.46666667, 0.47843137],\\n         [0.09803922, 0.0627451 , 0.19215686, ..., 0.4627451 ,\\n          0.47058824, 0.42745098],\\n         ...,\\n         [0.81568627, 0.78823529, 0.77647059, ..., 0.62745098,\\n          0.21960784, 0.20784314],\\n         [0.70588235, 0.67843137, 0.72941176, ..., 0.72156863,\\n          0.38039216, 0.3254902 ],\\n         [0.69411765, 0.65882353, 0.70196078, ..., 0.84705882,\\n          0.59215686, 0.48235294]],\\n\\n        [[0.24313725, 0.18039216, 0.18823529, ..., 0.51764706,\\n          0.49019608, 0.48627451],\\n         [0.07843137, 0.        , 0.03137255, ..., 0.34509804,\\n          0.3254902 , 0.34117647],\\n         [0.09411765, 0.02745098, 0.10588235, ..., 0.32941176,\\n          0.32941176, 0.28627451],\\n         ...,\\n         [0.66666667, 0.6       , 0.63137255, ..., 0.52156863,\\n          0.12156863, 0.13333333],\\n         [0.54509804, 0.48235294, 0.56470588, ..., 0.58039216,\\n          0.24313725, 0.20784314],\\n         [0.56470588, 0.50588235, 0.55686275, ..., 0.72156863,\\n          0.4627451 , 0.360...[0.57254902, 0.56470588, 0.56470588, ..., 0.64705882,\\n          0.63921569, 0.64705882],\\n         [0.56862745, 0.56078431, 0.56078431, ..., 0.63137255,\\n          0.63529412, 0.63137255],\\n         [0.57647059, 0.56862745, 0.56862745, ..., 0.58823529,\\n          0.52156863, 0.45882353],\\n         ...,\\n         [0.61176471, 0.61176471, 0.61568627, ..., 0.49803922,\\n          0.49411765, 0.53333333],\\n         [0.59607843, 0.6       , 0.61960784, ..., 0.49019608,\\n          0.52156863, 0.51764706],\\n         [0.61960784, 0.61176471, 0.61960784, ..., 0.57254902,\\n          0.59215686, 0.58823529]],\\n\\n        [[0.75294118, 0.74117647, 0.74117647, ..., 0.64705882,\\n          0.63921569, 0.64705882],\\n         [0.7372549 , 0.72941176, 0.72941176, ..., 0.62352941,\\n          0.62352941, 0.61568627],\\n         [0.74117647, 0.72941176, 0.73333333, ..., 0.57647059,\\n          0.50588235, 0.43529412],\\n         ...,\\n         [0.58823529, 0.59215686, 0.59607843, ..., 0.43921569,\\n          0.44705882, 0.49803922],\\n         [0.58431373, 0.58823529, 0.60784314, ..., 0.45882353,\\n          0.48627451, 0.47058824],\\n         [0.60784314, 0.6       , 0.60784314, ..., 0.54509804,\\n          0.55294118, 0.5372549 ]]]]), <class 'needle.backend_ndarray.ndarray.NDArray'>)\u001b[0m\n",
      "\u001b[1m\u001b[31mE        +    where array([[[[0.23137255, 0.16862745, 0.19607843, ..., 0.61960784,\\n          0.59607843, 0.58039216],\\n         [0.0627451 , 0.        , 0.07058824, ..., 0.48235294,\\n          0.46666667, 0.47843137],\\n         [0.09803922, 0.0627451 , 0.19215686, ..., 0.4627451 ,\\n          0.47058824, 0.42745098],\\n         ...,\\n         [0.81568627, 0.78823529, 0.77647059, ..., 0.62745098,\\n          0.21960784, 0.20784314],\\n         [0.70588235, 0.67843137, 0.72941176, ..., 0.72156863,\\n          0.38039216, 0.3254902 ],\\n         [0.69411765, 0.65882353, 0.70196078, ..., 0.84705882,\\n          0.59215686, 0.48235294]],\\n\\n        [[0.24313725, 0.18039216, 0.18823529, ..., 0.51764706,\\n          0.49019608, 0.48627451],\\n         [0.07843137, 0.        , 0.03137255, ..., 0.34509804,\\n          0.3254902 , 0.34117647],\\n         [0.09411765, 0.02745098, 0.10588235, ..., 0.32941176,\\n          0.32941176, 0.28627451],\\n         ...,\\n         [0.66666667, 0.6       , 0.63137255, ..., 0.52156863,\\n          0.12156863, 0.13333333],\\n         [0.54509804, 0.48235294, 0.56470588, ..., 0.58039216,\\n          0.24313725, 0.20784314],\\n         [0.56470588, 0.50588235, 0.55686275, ..., 0.72156863,\\n          0.4627451 , 0.360...[0.57254902, 0.56470588, 0.56470588, ..., 0.64705882,\\n          0.63921569, 0.64705882],\\n         [0.56862745, 0.56078431, 0.56078431, ..., 0.63137255,\\n          0.63529412, 0.63137255],\\n         [0.57647059, 0.56862745, 0.56862745, ..., 0.58823529,\\n          0.52156863, 0.45882353],\\n         ...,\\n         [0.61176471, 0.61176471, 0.61568627, ..., 0.49803922,\\n          0.49411765, 0.53333333],\\n         [0.59607843, 0.6       , 0.61960784, ..., 0.49019608,\\n          0.52156863, 0.51764706],\\n         [0.61960784, 0.61176471, 0.61960784, ..., 0.57254902,\\n          0.59215686, 0.58823529]],\\n\\n        [[0.75294118, 0.74117647, 0.74117647, ..., 0.64705882,\\n          0.63921569, 0.64705882],\\n         [0.7372549 , 0.72941176, 0.72941176, ..., 0.62352941,\\n          0.62352941, 0.61568627],\\n         [0.74117647, 0.72941176, 0.73333333, ..., 0.57647059,\\n          0.50588235, 0.43529412],\\n         ...,\\n         [0.58823529, 0.59215686, 0.59607843, ..., 0.43921569,\\n          0.44705882, 0.49803922],\\n         [0.58431373, 0.58823529, 0.60784314, ..., 0.45882353,\\n          0.48627451, 0.47058824],\\n         [0.60784314, 0.6       , 0.60784314, ..., 0.54509804,\\n          0.55294118, 0.5372549 ]]]]) = needle.Tensor([[[[0.23137255 0.16862745 0.19607843 ... 0.61960784 0.59607843\\n    0.58039216]\\n   [0.0627451  0.         0.07058824 ... 0.48235294 0.46666667\\n    0.47843137]\\n   [0.09803922 0.0627451  0.19215686 ... 0.4627451  0.47058824\\n    0.42745098]\\n   ...\\n   [0.81568627 0.78823529 0.77647059 ... 0.62745098 0.21960784\\n    0.20784314]\\n   [0.70588235 0.67843137 0.72941176 ... 0.72156863 0.38039216\\n    0.3254902 ]\\n   [0.69411765 0.65882353 0.70196078 ... 0.84705882 0.59215686\\n    0.48235294]]\\n\\n  [[0.24313725 0.18039216 0.18823529 ... 0.51764706 0.49019608\\n    0.48627451]\\n   [0.07843137 0.         0.03137255 ... 0.34509804 0.3254902\\n    0.34117647]\\n   [0.09411765 0.02745098 0.10588235 ... 0.32941176 0.32941176\\n    0.28627451]\\n   ...\\n   [0.66666667 0.6        0.63137255 ... 0.52156863 0.12156863\\n    0.13333333]\\n   [0.54509804 0.48235294 0.56470588 ... 0.58039216 0.24313725\\n    0.20784314]\\n   [0.56470588 0.50588235 0.55686275 ... 0.72156863 0.4627451\\n    0.36078431]]\\n\\n  [[0.24705882 0.17647059 0.16862745 ... 0.42352941 0.4\\n    0.40392157]\\n   [0.07843137 0.         0.         ... 0.21568627 0.19607843\\n    0.22352941]\\n   [0.08235294 0.         0.03137255 ... 0.19607843 0.19607843\\n    0....65098039 0.65098039 0.65882353 ... 0.5372549  0.5372549\\n    0.57647059]\\n   [0.63137255 0.63529412 0.65490196 ... 0.51372549 0.54117647\\n    0.5372549 ]\\n   [0.64313725 0.63529412 0.64313725 ... 0.6        0.61568627\\n    0.61176471]]\\n\\n  [[0.57254902 0.56470588 0.56470588 ... 0.64705882 0.63921569\\n    0.64705882]\\n   [0.56862745 0.56078431 0.56078431 ... 0.63137255 0.63529412\\n    0.63137255]\\n   [0.57647059 0.56862745 0.56862745 ... 0.58823529 0.52156863\\n    0.45882353]\\n   ...\\n   [0.61176471 0.61176471 0.61568627 ... 0.49803922 0.49411765\\n    0.53333333]\\n   [0.59607843 0.6        0.61960784 ... 0.49019608 0.52156863\\n    0.51764706]\\n   [0.61960784 0.61176471 0.61960784 ... 0.57254902 0.59215686\\n    0.58823529]]\\n\\n  [[0.75294118 0.74117647 0.74117647 ... 0.64705882 0.63921569\\n    0.64705882]\\n   [0.7372549  0.72941176 0.72941176 ... 0.62352941 0.62352941\\n    0.61568627]\\n   [0.74117647 0.72941176 0.73333333 ... 0.57647059 0.50588235\\n    0.43529412]\\n   ...\\n   [0.58823529 0.59215686 0.59607843 ... 0.43921569 0.44705882\\n    0.49803922]\\n   [0.58431373 0.58823529 0.60784314 ... 0.45882353 0.48627451\\n    0.47058824]\\n   [0.60784314 0.6        0.60784314 ... 0.54509804 0.55294118\\n    0.5372549 ]]]]).cached_data\u001b[0m\n",
      "\u001b[1m\u001b[31mE        +    and   <class 'needle.backend_ndarray.ndarray.NDArray'> = nd.NDArray\u001b[0m\n",
      "\n",
      "X          = needle.Tensor([[[[0.23137255 0.16862745 0.19607843 ... 0.61960784 0.59607843\n",
      "    0.58039216]\n",
      "   [0.0627451  0.        ....45882353 0.48627451\n",
      "    0.47058824]\n",
      "   [0.60784314 0.6        0.60784314 ... 0.54509804 0.55294118\n",
      "    0.5372549 ]]]])\n",
      "batch_size = 15\n",
      "cifar10_train_dataset = <needle.data.datasets.cifar10_dataset.CIFAR10Dataset object at 0x7f955688b4d0>\n",
      "device     = cpu()\n",
      "train      = False\n",
      "train_loader = <needle.data.data_basic.DataLoader object at 0x7f9557255b10>\n",
      "y          = needle.Tensor([6 9 9 4 1 1 2 7 8 3 4 7 7 2 9])\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_cifar_ptb_data.py\u001b[0m:43: AssertionError\n",
      "\u001b[31m\u001b[1m_______________________ test_cifar10_loader[cuda-True-1] _______________________\u001b[0m\n",
      "\n",
      "batch_size = 1, train = True, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mtrain\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRAIN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_cifar10_loader\u001b[39;49;00m(batch_size, train, device):\u001b[90m\u001b[39;49;00m\n",
      "        cifar10_train_dataset = ndl.data.CIFAR10Dataset(\u001b[33m\"\u001b[39;49;00m\u001b[33mdata/cifar-10-batches-py\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, train=\u001b[94mTrue\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        train_loader = ndl.data.DataLoader(cifar10_train_dataset, batch_size)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mfor\u001b[39;49;00m (X, y) \u001b[95min\u001b[39;49;00m train_loader:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mbreak\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94massert\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(X.cached_data, nd.NDArray), \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mExpected X.cached_data to be nd.NDArray, but got \u001b[39;49;00m\u001b[33m{\u001b[39;49;00m\u001b[96mtype\u001b[39;49;00m(X.cached_data)\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AssertionError: Expected X.cached_data to be nd.NDArray, but got <class 'numpy.ndarray'>\u001b[0m\n",
      "\u001b[1m\u001b[31mE       assert False\u001b[0m\n",
      "\u001b[1m\u001b[31mE        +  where False = isinstance(array([[[[0.23137255, 0.16862745, 0.19607843, ..., 0.61960784,\\n          0.59607843, 0.58039216],\\n         [0.0627451 , 0.        , 0.07058824, ..., 0.48235294,\\n          0.46666667, 0.47843137],\\n         [0.09803922, 0.0627451 , 0.19215686, ..., 0.4627451 ,\\n          0.47058824, 0.42745098],\\n         ...,\\n         [0.81568627, 0.78823529, 0.77647059, ..., 0.62745098,\\n          0.21960784, 0.20784314],\\n         [0.70588235, 0.67843137, 0.72941176, ..., 0.72156863,\\n          0.38039216, 0.3254902 ],\\n         [0.69411765, 0.65882353, 0.70196078, ..., 0.84705882,\\n          0.59215686, 0.48235294]],\\n\\n        [[0.24313725, 0.18039216, 0.18823529, ..., 0.51764706,\\n          0.49019608, 0.48627451],\\n         [0.07843137, 0.        , 0.03137255, ..., 0.34509804,\\n          0.3254902 , 0.34117647],\\n         [0.09411765, 0.02745098, 0.10588235, ..., 0.32941176,\\n          0.32941176, 0.28627451],\\n         ...,\\n         [0.66666667, 0.6       , 0.63137255, ..., 0.52156863,\\n          0.12156863, 0.13333333],\\n         [0.54509804, 0.48235294, 0.56470588, ..., 0.58039216,\\n          0.24313725, 0.20784314],\\n         [0.56470588, 0.50588235, 0.55686275, ..., 0.72156863,\\n          0.4627451 , 0.36078431]],\\n\\n        [[0.24705882, 0.17647059, 0.16862745, ..., 0.42352941,\\n          0.4       , 0.40392157],\\n         [0.07843137, 0.        , 0.        , ..., 0.21568627,\\n          0.19607843, 0.22352941],\\n         [0.08235294, 0.        , 0.03137255, ..., 0.19607843,\\n          0.19607843, 0.16470588],\\n         ...,\\n         [0.37647059, 0.13333333, 0.10196078, ..., 0.2745098 ,\\n          0.02745098, 0.07843137],\\n         [0.37647059, 0.16470588, 0.11764706, ..., 0.36862745,\\n          0.13333333, 0.13333333],\\n         [0.45490196, 0.36862745, 0.34117647, ..., 0.54901961,\\n          0.32941176, 0.28235294]]]]), <class 'needle.backend_ndarray.ndarray.NDArray'>)\u001b[0m\n",
      "\u001b[1m\u001b[31mE        +    where array([[[[0.23137255, 0.16862745, 0.19607843, ..., 0.61960784,\\n          0.59607843, 0.58039216],\\n         [0.0627451 , 0.        , 0.07058824, ..., 0.48235294,\\n          0.46666667, 0.47843137],\\n         [0.09803922, 0.0627451 , 0.19215686, ..., 0.4627451 ,\\n          0.47058824, 0.42745098],\\n         ...,\\n         [0.81568627, 0.78823529, 0.77647059, ..., 0.62745098,\\n          0.21960784, 0.20784314],\\n         [0.70588235, 0.67843137, 0.72941176, ..., 0.72156863,\\n          0.38039216, 0.3254902 ],\\n         [0.69411765, 0.65882353, 0.70196078, ..., 0.84705882,\\n          0.59215686, 0.48235294]],\\n\\n        [[0.24313725, 0.18039216, 0.18823529, ..., 0.51764706,\\n          0.49019608, 0.48627451],\\n         [0.07843137, 0.        , 0.03137255, ..., 0.34509804,\\n          0.3254902 , 0.34117647],\\n         [0.09411765, 0.02745098, 0.10588235, ..., 0.32941176,\\n          0.32941176, 0.28627451],\\n         ...,\\n         [0.66666667, 0.6       , 0.63137255, ..., 0.52156863,\\n          0.12156863, 0.13333333],\\n         [0.54509804, 0.48235294, 0.56470588, ..., 0.58039216,\\n          0.24313725, 0.20784314],\\n         [0.56470588, 0.50588235, 0.55686275, ..., 0.72156863,\\n          0.4627451 , 0.36078431]],\\n\\n        [[0.24705882, 0.17647059, 0.16862745, ..., 0.42352941,\\n          0.4       , 0.40392157],\\n         [0.07843137, 0.        , 0.        , ..., 0.21568627,\\n          0.19607843, 0.22352941],\\n         [0.08235294, 0.        , 0.03137255, ..., 0.19607843,\\n          0.19607843, 0.16470588],\\n         ...,\\n         [0.37647059, 0.13333333, 0.10196078, ..., 0.2745098 ,\\n          0.02745098, 0.07843137],\\n         [0.37647059, 0.16470588, 0.11764706, ..., 0.36862745,\\n          0.13333333, 0.13333333],\\n         [0.45490196, 0.36862745, 0.34117647, ..., 0.54901961,\\n          0.32941176, 0.28235294]]]]) = needle.Tensor([[[[0.23137255 0.16862745 0.19607843 ... 0.61960784 0.59607843\\n    0.58039216]\\n   [0.0627451  0.         0.07058824 ... 0.48235294 0.46666667\\n    0.47843137]\\n   [0.09803922 0.0627451  0.19215686 ... 0.4627451  0.47058824\\n    0.42745098]\\n   ...\\n   [0.81568627 0.78823529 0.77647059 ... 0.62745098 0.21960784\\n    0.20784314]\\n   [0.70588235 0.67843137 0.72941176 ... 0.72156863 0.38039216\\n    0.3254902 ]\\n   [0.69411765 0.65882353 0.70196078 ... 0.84705882 0.59215686\\n    0.48235294]]\\n\\n  [[0.24313725 0.18039216 0.18823529 ... 0.51764706 0.49019608\\n    0.48627451]\\n   [0.07843137 0.         0.03137255 ... 0.34509804 0.3254902\\n    0.34117647]\\n   [0.09411765 0.02745098 0.10588235 ... 0.32941176 0.32941176\\n    0.28627451]\\n   ...\\n   [0.66666667 0.6        0.63137255 ... 0.52156863 0.12156863\\n    0.13333333]\\n   [0.54509804 0.48235294 0.56470588 ... 0.58039216 0.24313725\\n    0.20784314]\\n   [0.56470588 0.50588235 0.55686275 ... 0.72156863 0.4627451\\n    0.36078431]]\\n\\n  [[0.24705882 0.17647059 0.16862745 ... 0.42352941 0.4\\n    0.40392157]\\n   [0.07843137 0.         0.         ... 0.21568627 0.19607843\\n    0.22352941]\\n   [0.08235294 0.         0.03137255 ... 0.19607843 0.19607843\\n    0.16470588]\\n   ...\\n   [0.37647059 0.13333333 0.10196078 ... 0.2745098  0.02745098\\n    0.07843137]\\n   [0.37647059 0.16470588 0.11764706 ... 0.36862745 0.13333333\\n    0.13333333]\\n   [0.45490196 0.36862745 0.34117647 ... 0.54901961 0.32941176\\n    0.28235294]]]]).cached_data\u001b[0m\n",
      "\u001b[1m\u001b[31mE        +    and   <class 'needle.backend_ndarray.ndarray.NDArray'> = nd.NDArray\u001b[0m\n",
      "\n",
      "X          = needle.Tensor([[[[0.23137255 0.16862745 0.19607843 ... 0.61960784 0.59607843\n",
      "    0.58039216]\n",
      "   [0.0627451  0.        ....36862745 0.13333333\n",
      "    0.13333333]\n",
      "   [0.45490196 0.36862745 0.34117647 ... 0.54901961 0.32941176\n",
      "    0.28235294]]]])\n",
      "batch_size = 1\n",
      "cifar10_train_dataset = <needle.data.datasets.cifar10_dataset.CIFAR10Dataset object at 0x7f95567e9810>\n",
      "device     = cuda()\n",
      "train      = True\n",
      "train_loader = <needle.data.data_basic.DataLoader object at 0x7f95569430d0>\n",
      "y          = needle.Tensor([6])\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_cifar_ptb_data.py\u001b[0m:43: AssertionError\n",
      "\u001b[31m\u001b[1m______________________ test_cifar10_loader[cuda-True-15] _______________________\u001b[0m\n",
      "\n",
      "batch_size = 15, train = True, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mtrain\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRAIN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_cifar10_loader\u001b[39;49;00m(batch_size, train, device):\u001b[90m\u001b[39;49;00m\n",
      "        cifar10_train_dataset = ndl.data.CIFAR10Dataset(\u001b[33m\"\u001b[39;49;00m\u001b[33mdata/cifar-10-batches-py\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, train=\u001b[94mTrue\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        train_loader = ndl.data.DataLoader(cifar10_train_dataset, batch_size)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mfor\u001b[39;49;00m (X, y) \u001b[95min\u001b[39;49;00m train_loader:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mbreak\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94massert\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(X.cached_data, nd.NDArray), \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mExpected X.cached_data to be nd.NDArray, but got \u001b[39;49;00m\u001b[33m{\u001b[39;49;00m\u001b[96mtype\u001b[39;49;00m(X.cached_data)\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AssertionError: Expected X.cached_data to be nd.NDArray, but got <class 'numpy.ndarray'>\u001b[0m\n",
      "\u001b[1m\u001b[31mE       assert False\u001b[0m\n",
      "\u001b[1m\u001b[31mE        +  where False = isinstance(array([[[[0.23137255, 0.16862745, 0.19607843, ..., 0.61960784,\\n          0.59607843, 0.58039216],\\n         [0.0627451 , 0.        , 0.07058824, ..., 0.48235294,\\n          0.46666667, 0.47843137],\\n         [0.09803922, 0.0627451 , 0.19215686, ..., 0.4627451 ,\\n          0.47058824, 0.42745098],\\n         ...,\\n         [0.81568627, 0.78823529, 0.77647059, ..., 0.62745098,\\n          0.21960784, 0.20784314],\\n         [0.70588235, 0.67843137, 0.72941176, ..., 0.72156863,\\n          0.38039216, 0.3254902 ],\\n         [0.69411765, 0.65882353, 0.70196078, ..., 0.84705882,\\n          0.59215686, 0.48235294]],\\n\\n        [[0.24313725, 0.18039216, 0.18823529, ..., 0.51764706,\\n          0.49019608, 0.48627451],\\n         [0.07843137, 0.        , 0.03137255, ..., 0.34509804,\\n          0.3254902 , 0.34117647],\\n         [0.09411765, 0.02745098, 0.10588235, ..., 0.32941176,\\n          0.32941176, 0.28627451],\\n         ...,\\n         [0.66666667, 0.6       , 0.63137255, ..., 0.52156863,\\n          0.12156863, 0.13333333],\\n         [0.54509804, 0.48235294, 0.56470588, ..., 0.58039216,\\n          0.24313725, 0.20784314],\\n         [0.56470588, 0.50588235, 0.55686275, ..., 0.72156863,\\n          0.4627451 , 0.360...[0.57254902, 0.56470588, 0.56470588, ..., 0.64705882,\\n          0.63921569, 0.64705882],\\n         [0.56862745, 0.56078431, 0.56078431, ..., 0.63137255,\\n          0.63529412, 0.63137255],\\n         [0.57647059, 0.56862745, 0.56862745, ..., 0.58823529,\\n          0.52156863, 0.45882353],\\n         ...,\\n         [0.61176471, 0.61176471, 0.61568627, ..., 0.49803922,\\n          0.49411765, 0.53333333],\\n         [0.59607843, 0.6       , 0.61960784, ..., 0.49019608,\\n          0.52156863, 0.51764706],\\n         [0.61960784, 0.61176471, 0.61960784, ..., 0.57254902,\\n          0.59215686, 0.58823529]],\\n\\n        [[0.75294118, 0.74117647, 0.74117647, ..., 0.64705882,\\n          0.63921569, 0.64705882],\\n         [0.7372549 , 0.72941176, 0.72941176, ..., 0.62352941,\\n          0.62352941, 0.61568627],\\n         [0.74117647, 0.72941176, 0.73333333, ..., 0.57647059,\\n          0.50588235, 0.43529412],\\n         ...,\\n         [0.58823529, 0.59215686, 0.59607843, ..., 0.43921569,\\n          0.44705882, 0.49803922],\\n         [0.58431373, 0.58823529, 0.60784314, ..., 0.45882353,\\n          0.48627451, 0.47058824],\\n         [0.60784314, 0.6       , 0.60784314, ..., 0.54509804,\\n          0.55294118, 0.5372549 ]]]]), <class 'needle.backend_ndarray.ndarray.NDArray'>)\u001b[0m\n",
      "\u001b[1m\u001b[31mE        +    where array([[[[0.23137255, 0.16862745, 0.19607843, ..., 0.61960784,\\n          0.59607843, 0.58039216],\\n         [0.0627451 , 0.        , 0.07058824, ..., 0.48235294,\\n          0.46666667, 0.47843137],\\n         [0.09803922, 0.0627451 , 0.19215686, ..., 0.4627451 ,\\n          0.47058824, 0.42745098],\\n         ...,\\n         [0.81568627, 0.78823529, 0.77647059, ..., 0.62745098,\\n          0.21960784, 0.20784314],\\n         [0.70588235, 0.67843137, 0.72941176, ..., 0.72156863,\\n          0.38039216, 0.3254902 ],\\n         [0.69411765, 0.65882353, 0.70196078, ..., 0.84705882,\\n          0.59215686, 0.48235294]],\\n\\n        [[0.24313725, 0.18039216, 0.18823529, ..., 0.51764706,\\n          0.49019608, 0.48627451],\\n         [0.07843137, 0.        , 0.03137255, ..., 0.34509804,\\n          0.3254902 , 0.34117647],\\n         [0.09411765, 0.02745098, 0.10588235, ..., 0.32941176,\\n          0.32941176, 0.28627451],\\n         ...,\\n         [0.66666667, 0.6       , 0.63137255, ..., 0.52156863,\\n          0.12156863, 0.13333333],\\n         [0.54509804, 0.48235294, 0.56470588, ..., 0.58039216,\\n          0.24313725, 0.20784314],\\n         [0.56470588, 0.50588235, 0.55686275, ..., 0.72156863,\\n          0.4627451 , 0.360...[0.57254902, 0.56470588, 0.56470588, ..., 0.64705882,\\n          0.63921569, 0.64705882],\\n         [0.56862745, 0.56078431, 0.56078431, ..., 0.63137255,\\n          0.63529412, 0.63137255],\\n         [0.57647059, 0.56862745, 0.56862745, ..., 0.58823529,\\n          0.52156863, 0.45882353],\\n         ...,\\n         [0.61176471, 0.61176471, 0.61568627, ..., 0.49803922,\\n          0.49411765, 0.53333333],\\n         [0.59607843, 0.6       , 0.61960784, ..., 0.49019608,\\n          0.52156863, 0.51764706],\\n         [0.61960784, 0.61176471, 0.61960784, ..., 0.57254902,\\n          0.59215686, 0.58823529]],\\n\\n        [[0.75294118, 0.74117647, 0.74117647, ..., 0.64705882,\\n          0.63921569, 0.64705882],\\n         [0.7372549 , 0.72941176, 0.72941176, ..., 0.62352941,\\n          0.62352941, 0.61568627],\\n         [0.74117647, 0.72941176, 0.73333333, ..., 0.57647059,\\n          0.50588235, 0.43529412],\\n         ...,\\n         [0.58823529, 0.59215686, 0.59607843, ..., 0.43921569,\\n          0.44705882, 0.49803922],\\n         [0.58431373, 0.58823529, 0.60784314, ..., 0.45882353,\\n          0.48627451, 0.47058824],\\n         [0.60784314, 0.6       , 0.60784314, ..., 0.54509804,\\n          0.55294118, 0.5372549 ]]]]) = needle.Tensor([[[[0.23137255 0.16862745 0.19607843 ... 0.61960784 0.59607843\\n    0.58039216]\\n   [0.0627451  0.         0.07058824 ... 0.48235294 0.46666667\\n    0.47843137]\\n   [0.09803922 0.0627451  0.19215686 ... 0.4627451  0.47058824\\n    0.42745098]\\n   ...\\n   [0.81568627 0.78823529 0.77647059 ... 0.62745098 0.21960784\\n    0.20784314]\\n   [0.70588235 0.67843137 0.72941176 ... 0.72156863 0.38039216\\n    0.3254902 ]\\n   [0.69411765 0.65882353 0.70196078 ... 0.84705882 0.59215686\\n    0.48235294]]\\n\\n  [[0.24313725 0.18039216 0.18823529 ... 0.51764706 0.49019608\\n    0.48627451]\\n   [0.07843137 0.         0.03137255 ... 0.34509804 0.3254902\\n    0.34117647]\\n   [0.09411765 0.02745098 0.10588235 ... 0.32941176 0.32941176\\n    0.28627451]\\n   ...\\n   [0.66666667 0.6        0.63137255 ... 0.52156863 0.12156863\\n    0.13333333]\\n   [0.54509804 0.48235294 0.56470588 ... 0.58039216 0.24313725\\n    0.20784314]\\n   [0.56470588 0.50588235 0.55686275 ... 0.72156863 0.4627451\\n    0.36078431]]\\n\\n  [[0.24705882 0.17647059 0.16862745 ... 0.42352941 0.4\\n    0.40392157]\\n   [0.07843137 0.         0.         ... 0.21568627 0.19607843\\n    0.22352941]\\n   [0.08235294 0.         0.03137255 ... 0.19607843 0.19607843\\n    0....65098039 0.65098039 0.65882353 ... 0.5372549  0.5372549\\n    0.57647059]\\n   [0.63137255 0.63529412 0.65490196 ... 0.51372549 0.54117647\\n    0.5372549 ]\\n   [0.64313725 0.63529412 0.64313725 ... 0.6        0.61568627\\n    0.61176471]]\\n\\n  [[0.57254902 0.56470588 0.56470588 ... 0.64705882 0.63921569\\n    0.64705882]\\n   [0.56862745 0.56078431 0.56078431 ... 0.63137255 0.63529412\\n    0.63137255]\\n   [0.57647059 0.56862745 0.56862745 ... 0.58823529 0.52156863\\n    0.45882353]\\n   ...\\n   [0.61176471 0.61176471 0.61568627 ... 0.49803922 0.49411765\\n    0.53333333]\\n   [0.59607843 0.6        0.61960784 ... 0.49019608 0.52156863\\n    0.51764706]\\n   [0.61960784 0.61176471 0.61960784 ... 0.57254902 0.59215686\\n    0.58823529]]\\n\\n  [[0.75294118 0.74117647 0.74117647 ... 0.64705882 0.63921569\\n    0.64705882]\\n   [0.7372549  0.72941176 0.72941176 ... 0.62352941 0.62352941\\n    0.61568627]\\n   [0.74117647 0.72941176 0.73333333 ... 0.57647059 0.50588235\\n    0.43529412]\\n   ...\\n   [0.58823529 0.59215686 0.59607843 ... 0.43921569 0.44705882\\n    0.49803922]\\n   [0.58431373 0.58823529 0.60784314 ... 0.45882353 0.48627451\\n    0.47058824]\\n   [0.60784314 0.6        0.60784314 ... 0.54509804 0.55294118\\n    0.5372549 ]]]]).cached_data\u001b[0m\n",
      "\u001b[1m\u001b[31mE        +    and   <class 'needle.backend_ndarray.ndarray.NDArray'> = nd.NDArray\u001b[0m\n",
      "\n",
      "X          = needle.Tensor([[[[0.23137255 0.16862745 0.19607843 ... 0.61960784 0.59607843\n",
      "    0.58039216]\n",
      "   [0.0627451  0.        ....45882353 0.48627451\n",
      "    0.47058824]\n",
      "   [0.60784314 0.6        0.60784314 ... 0.54509804 0.55294118\n",
      "    0.5372549 ]]]])\n",
      "batch_size = 15\n",
      "cifar10_train_dataset = <needle.data.datasets.cifar10_dataset.CIFAR10Dataset object at 0x7f955687eb10>\n",
      "device     = cuda()\n",
      "train      = True\n",
      "train_loader = <needle.data.data_basic.DataLoader object at 0x7f955727f050>\n",
      "y          = needle.Tensor([6 9 9 4 1 1 2 7 8 3 4 7 7 2 9])\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_cifar_ptb_data.py\u001b[0m:43: AssertionError\n",
      "\u001b[31m\u001b[1m______________________ test_cifar10_loader[cuda-False-1] _______________________\u001b[0m\n",
      "\n",
      "batch_size = 1, train = False, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mtrain\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRAIN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_cifar10_loader\u001b[39;49;00m(batch_size, train, device):\u001b[90m\u001b[39;49;00m\n",
      "        cifar10_train_dataset = ndl.data.CIFAR10Dataset(\u001b[33m\"\u001b[39;49;00m\u001b[33mdata/cifar-10-batches-py\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, train=\u001b[94mTrue\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        train_loader = ndl.data.DataLoader(cifar10_train_dataset, batch_size)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mfor\u001b[39;49;00m (X, y) \u001b[95min\u001b[39;49;00m train_loader:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mbreak\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94massert\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(X.cached_data, nd.NDArray), \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mExpected X.cached_data to be nd.NDArray, but got \u001b[39;49;00m\u001b[33m{\u001b[39;49;00m\u001b[96mtype\u001b[39;49;00m(X.cached_data)\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AssertionError: Expected X.cached_data to be nd.NDArray, but got <class 'numpy.ndarray'>\u001b[0m\n",
      "\u001b[1m\u001b[31mE       assert False\u001b[0m\n",
      "\u001b[1m\u001b[31mE        +  where False = isinstance(array([[[[0.23137255, 0.16862745, 0.19607843, ..., 0.61960784,\\n          0.59607843, 0.58039216],\\n         [0.0627451 , 0.        , 0.07058824, ..., 0.48235294,\\n          0.46666667, 0.47843137],\\n         [0.09803922, 0.0627451 , 0.19215686, ..., 0.4627451 ,\\n          0.47058824, 0.42745098],\\n         ...,\\n         [0.81568627, 0.78823529, 0.77647059, ..., 0.62745098,\\n          0.21960784, 0.20784314],\\n         [0.70588235, 0.67843137, 0.72941176, ..., 0.72156863,\\n          0.38039216, 0.3254902 ],\\n         [0.69411765, 0.65882353, 0.70196078, ..., 0.84705882,\\n          0.59215686, 0.48235294]],\\n\\n        [[0.24313725, 0.18039216, 0.18823529, ..., 0.51764706,\\n          0.49019608, 0.48627451],\\n         [0.07843137, 0.        , 0.03137255, ..., 0.34509804,\\n          0.3254902 , 0.34117647],\\n         [0.09411765, 0.02745098, 0.10588235, ..., 0.32941176,\\n          0.32941176, 0.28627451],\\n         ...,\\n         [0.66666667, 0.6       , 0.63137255, ..., 0.52156863,\\n          0.12156863, 0.13333333],\\n         [0.54509804, 0.48235294, 0.56470588, ..., 0.58039216,\\n          0.24313725, 0.20784314],\\n         [0.56470588, 0.50588235, 0.55686275, ..., 0.72156863,\\n          0.4627451 , 0.36078431]],\\n\\n        [[0.24705882, 0.17647059, 0.16862745, ..., 0.42352941,\\n          0.4       , 0.40392157],\\n         [0.07843137, 0.        , 0.        , ..., 0.21568627,\\n          0.19607843, 0.22352941],\\n         [0.08235294, 0.        , 0.03137255, ..., 0.19607843,\\n          0.19607843, 0.16470588],\\n         ...,\\n         [0.37647059, 0.13333333, 0.10196078, ..., 0.2745098 ,\\n          0.02745098, 0.07843137],\\n         [0.37647059, 0.16470588, 0.11764706, ..., 0.36862745,\\n          0.13333333, 0.13333333],\\n         [0.45490196, 0.36862745, 0.34117647, ..., 0.54901961,\\n          0.32941176, 0.28235294]]]]), <class 'needle.backend_ndarray.ndarray.NDArray'>)\u001b[0m\n",
      "\u001b[1m\u001b[31mE        +    where array([[[[0.23137255, 0.16862745, 0.19607843, ..., 0.61960784,\\n          0.59607843, 0.58039216],\\n         [0.0627451 , 0.        , 0.07058824, ..., 0.48235294,\\n          0.46666667, 0.47843137],\\n         [0.09803922, 0.0627451 , 0.19215686, ..., 0.4627451 ,\\n          0.47058824, 0.42745098],\\n         ...,\\n         [0.81568627, 0.78823529, 0.77647059, ..., 0.62745098,\\n          0.21960784, 0.20784314],\\n         [0.70588235, 0.67843137, 0.72941176, ..., 0.72156863,\\n          0.38039216, 0.3254902 ],\\n         [0.69411765, 0.65882353, 0.70196078, ..., 0.84705882,\\n          0.59215686, 0.48235294]],\\n\\n        [[0.24313725, 0.18039216, 0.18823529, ..., 0.51764706,\\n          0.49019608, 0.48627451],\\n         [0.07843137, 0.        , 0.03137255, ..., 0.34509804,\\n          0.3254902 , 0.34117647],\\n         [0.09411765, 0.02745098, 0.10588235, ..., 0.32941176,\\n          0.32941176, 0.28627451],\\n         ...,\\n         [0.66666667, 0.6       , 0.63137255, ..., 0.52156863,\\n          0.12156863, 0.13333333],\\n         [0.54509804, 0.48235294, 0.56470588, ..., 0.58039216,\\n          0.24313725, 0.20784314],\\n         [0.56470588, 0.50588235, 0.55686275, ..., 0.72156863,\\n          0.4627451 , 0.36078431]],\\n\\n        [[0.24705882, 0.17647059, 0.16862745, ..., 0.42352941,\\n          0.4       , 0.40392157],\\n         [0.07843137, 0.        , 0.        , ..., 0.21568627,\\n          0.19607843, 0.22352941],\\n         [0.08235294, 0.        , 0.03137255, ..., 0.19607843,\\n          0.19607843, 0.16470588],\\n         ...,\\n         [0.37647059, 0.13333333, 0.10196078, ..., 0.2745098 ,\\n          0.02745098, 0.07843137],\\n         [0.37647059, 0.16470588, 0.11764706, ..., 0.36862745,\\n          0.13333333, 0.13333333],\\n         [0.45490196, 0.36862745, 0.34117647, ..., 0.54901961,\\n          0.32941176, 0.28235294]]]]) = needle.Tensor([[[[0.23137255 0.16862745 0.19607843 ... 0.61960784 0.59607843\\n    0.58039216]\\n   [0.0627451  0.         0.07058824 ... 0.48235294 0.46666667\\n    0.47843137]\\n   [0.09803922 0.0627451  0.19215686 ... 0.4627451  0.47058824\\n    0.42745098]\\n   ...\\n   [0.81568627 0.78823529 0.77647059 ... 0.62745098 0.21960784\\n    0.20784314]\\n   [0.70588235 0.67843137 0.72941176 ... 0.72156863 0.38039216\\n    0.3254902 ]\\n   [0.69411765 0.65882353 0.70196078 ... 0.84705882 0.59215686\\n    0.48235294]]\\n\\n  [[0.24313725 0.18039216 0.18823529 ... 0.51764706 0.49019608\\n    0.48627451]\\n   [0.07843137 0.         0.03137255 ... 0.34509804 0.3254902\\n    0.34117647]\\n   [0.09411765 0.02745098 0.10588235 ... 0.32941176 0.32941176\\n    0.28627451]\\n   ...\\n   [0.66666667 0.6        0.63137255 ... 0.52156863 0.12156863\\n    0.13333333]\\n   [0.54509804 0.48235294 0.56470588 ... 0.58039216 0.24313725\\n    0.20784314]\\n   [0.56470588 0.50588235 0.55686275 ... 0.72156863 0.4627451\\n    0.36078431]]\\n\\n  [[0.24705882 0.17647059 0.16862745 ... 0.42352941 0.4\\n    0.40392157]\\n   [0.07843137 0.         0.         ... 0.21568627 0.19607843\\n    0.22352941]\\n   [0.08235294 0.         0.03137255 ... 0.19607843 0.19607843\\n    0.16470588]\\n   ...\\n   [0.37647059 0.13333333 0.10196078 ... 0.2745098  0.02745098\\n    0.07843137]\\n   [0.37647059 0.16470588 0.11764706 ... 0.36862745 0.13333333\\n    0.13333333]\\n   [0.45490196 0.36862745 0.34117647 ... 0.54901961 0.32941176\\n    0.28235294]]]]).cached_data\u001b[0m\n",
      "\u001b[1m\u001b[31mE        +    and   <class 'needle.backend_ndarray.ndarray.NDArray'> = nd.NDArray\u001b[0m\n",
      "\n",
      "X          = needle.Tensor([[[[0.23137255 0.16862745 0.19607843 ... 0.61960784 0.59607843\n",
      "    0.58039216]\n",
      "   [0.0627451  0.        ....36862745 0.13333333\n",
      "    0.13333333]\n",
      "   [0.45490196 0.36862745 0.34117647 ... 0.54901961 0.32941176\n",
      "    0.28235294]]]])\n",
      "batch_size = 1\n",
      "cifar10_train_dataset = <needle.data.datasets.cifar10_dataset.CIFAR10Dataset object at 0x7f95565cd390>\n",
      "device     = cuda()\n",
      "train      = False\n",
      "train_loader = <needle.data.data_basic.DataLoader object at 0x7f9a56a3ff90>\n",
      "y          = needle.Tensor([6])\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_cifar_ptb_data.py\u001b[0m:43: AssertionError\n",
      "\u001b[31m\u001b[1m______________________ test_cifar10_loader[cuda-False-15] ______________________\u001b[0m\n",
      "\n",
      "batch_size = 15, train = False, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mtrain\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRAIN)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_cifar10_loader\u001b[39;49;00m(batch_size, train, device):\u001b[90m\u001b[39;49;00m\n",
      "        cifar10_train_dataset = ndl.data.CIFAR10Dataset(\u001b[33m\"\u001b[39;49;00m\u001b[33mdata/cifar-10-batches-py\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, train=\u001b[94mTrue\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        train_loader = ndl.data.DataLoader(cifar10_train_dataset, batch_size)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mfor\u001b[39;49;00m (X, y) \u001b[95min\u001b[39;49;00m train_loader:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mbreak\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94massert\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(X.cached_data, nd.NDArray), \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mExpected X.cached_data to be nd.NDArray, but got \u001b[39;49;00m\u001b[33m{\u001b[39;49;00m\u001b[96mtype\u001b[39;49;00m(X.cached_data)\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AssertionError: Expected X.cached_data to be nd.NDArray, but got <class 'numpy.ndarray'>\u001b[0m\n",
      "\u001b[1m\u001b[31mE       assert False\u001b[0m\n",
      "\u001b[1m\u001b[31mE        +  where False = isinstance(array([[[[0.23137255, 0.16862745, 0.19607843, ..., 0.61960784,\\n          0.59607843, 0.58039216],\\n         [0.0627451 , 0.        , 0.07058824, ..., 0.48235294,\\n          0.46666667, 0.47843137],\\n         [0.09803922, 0.0627451 , 0.19215686, ..., 0.4627451 ,\\n          0.47058824, 0.42745098],\\n         ...,\\n         [0.81568627, 0.78823529, 0.77647059, ..., 0.62745098,\\n          0.21960784, 0.20784314],\\n         [0.70588235, 0.67843137, 0.72941176, ..., 0.72156863,\\n          0.38039216, 0.3254902 ],\\n         [0.69411765, 0.65882353, 0.70196078, ..., 0.84705882,\\n          0.59215686, 0.48235294]],\\n\\n        [[0.24313725, 0.18039216, 0.18823529, ..., 0.51764706,\\n          0.49019608, 0.48627451],\\n         [0.07843137, 0.        , 0.03137255, ..., 0.34509804,\\n          0.3254902 , 0.34117647],\\n         [0.09411765, 0.02745098, 0.10588235, ..., 0.32941176,\\n          0.32941176, 0.28627451],\\n         ...,\\n         [0.66666667, 0.6       , 0.63137255, ..., 0.52156863,\\n          0.12156863, 0.13333333],\\n         [0.54509804, 0.48235294, 0.56470588, ..., 0.58039216,\\n          0.24313725, 0.20784314],\\n         [0.56470588, 0.50588235, 0.55686275, ..., 0.72156863,\\n          0.4627451 , 0.360...[0.57254902, 0.56470588, 0.56470588, ..., 0.64705882,\\n          0.63921569, 0.64705882],\\n         [0.56862745, 0.56078431, 0.56078431, ..., 0.63137255,\\n          0.63529412, 0.63137255],\\n         [0.57647059, 0.56862745, 0.56862745, ..., 0.58823529,\\n          0.52156863, 0.45882353],\\n         ...,\\n         [0.61176471, 0.61176471, 0.61568627, ..., 0.49803922,\\n          0.49411765, 0.53333333],\\n         [0.59607843, 0.6       , 0.61960784, ..., 0.49019608,\\n          0.52156863, 0.51764706],\\n         [0.61960784, 0.61176471, 0.61960784, ..., 0.57254902,\\n          0.59215686, 0.58823529]],\\n\\n        [[0.75294118, 0.74117647, 0.74117647, ..., 0.64705882,\\n          0.63921569, 0.64705882],\\n         [0.7372549 , 0.72941176, 0.72941176, ..., 0.62352941,\\n          0.62352941, 0.61568627],\\n         [0.74117647, 0.72941176, 0.73333333, ..., 0.57647059,\\n          0.50588235, 0.43529412],\\n         ...,\\n         [0.58823529, 0.59215686, 0.59607843, ..., 0.43921569,\\n          0.44705882, 0.49803922],\\n         [0.58431373, 0.58823529, 0.60784314, ..., 0.45882353,\\n          0.48627451, 0.47058824],\\n         [0.60784314, 0.6       , 0.60784314, ..., 0.54509804,\\n          0.55294118, 0.5372549 ]]]]), <class 'needle.backend_ndarray.ndarray.NDArray'>)\u001b[0m\n",
      "\u001b[1m\u001b[31mE        +    where array([[[[0.23137255, 0.16862745, 0.19607843, ..., 0.61960784,\\n          0.59607843, 0.58039216],\\n         [0.0627451 , 0.        , 0.07058824, ..., 0.48235294,\\n          0.46666667, 0.47843137],\\n         [0.09803922, 0.0627451 , 0.19215686, ..., 0.4627451 ,\\n          0.47058824, 0.42745098],\\n         ...,\\n         [0.81568627, 0.78823529, 0.77647059, ..., 0.62745098,\\n          0.21960784, 0.20784314],\\n         [0.70588235, 0.67843137, 0.72941176, ..., 0.72156863,\\n          0.38039216, 0.3254902 ],\\n         [0.69411765, 0.65882353, 0.70196078, ..., 0.84705882,\\n          0.59215686, 0.48235294]],\\n\\n        [[0.24313725, 0.18039216, 0.18823529, ..., 0.51764706,\\n          0.49019608, 0.48627451],\\n         [0.07843137, 0.        , 0.03137255, ..., 0.34509804,\\n          0.3254902 , 0.34117647],\\n         [0.09411765, 0.02745098, 0.10588235, ..., 0.32941176,\\n          0.32941176, 0.28627451],\\n         ...,\\n         [0.66666667, 0.6       , 0.63137255, ..., 0.52156863,\\n          0.12156863, 0.13333333],\\n         [0.54509804, 0.48235294, 0.56470588, ..., 0.58039216,\\n          0.24313725, 0.20784314],\\n         [0.56470588, 0.50588235, 0.55686275, ..., 0.72156863,\\n          0.4627451 , 0.360...[0.57254902, 0.56470588, 0.56470588, ..., 0.64705882,\\n          0.63921569, 0.64705882],\\n         [0.56862745, 0.56078431, 0.56078431, ..., 0.63137255,\\n          0.63529412, 0.63137255],\\n         [0.57647059, 0.56862745, 0.56862745, ..., 0.58823529,\\n          0.52156863, 0.45882353],\\n         ...,\\n         [0.61176471, 0.61176471, 0.61568627, ..., 0.49803922,\\n          0.49411765, 0.53333333],\\n         [0.59607843, 0.6       , 0.61960784, ..., 0.49019608,\\n          0.52156863, 0.51764706],\\n         [0.61960784, 0.61176471, 0.61960784, ..., 0.57254902,\\n          0.59215686, 0.58823529]],\\n\\n        [[0.75294118, 0.74117647, 0.74117647, ..., 0.64705882,\\n          0.63921569, 0.64705882],\\n         [0.7372549 , 0.72941176, 0.72941176, ..., 0.62352941,\\n          0.62352941, 0.61568627],\\n         [0.74117647, 0.72941176, 0.73333333, ..., 0.57647059,\\n          0.50588235, 0.43529412],\\n         ...,\\n         [0.58823529, 0.59215686, 0.59607843, ..., 0.43921569,\\n          0.44705882, 0.49803922],\\n         [0.58431373, 0.58823529, 0.60784314, ..., 0.45882353,\\n          0.48627451, 0.47058824],\\n         [0.60784314, 0.6       , 0.60784314, ..., 0.54509804,\\n          0.55294118, 0.5372549 ]]]]) = needle.Tensor([[[[0.23137255 0.16862745 0.19607843 ... 0.61960784 0.59607843\\n    0.58039216]\\n   [0.0627451  0.         0.07058824 ... 0.48235294 0.46666667\\n    0.47843137]\\n   [0.09803922 0.0627451  0.19215686 ... 0.4627451  0.47058824\\n    0.42745098]\\n   ...\\n   [0.81568627 0.78823529 0.77647059 ... 0.62745098 0.21960784\\n    0.20784314]\\n   [0.70588235 0.67843137 0.72941176 ... 0.72156863 0.38039216\\n    0.3254902 ]\\n   [0.69411765 0.65882353 0.70196078 ... 0.84705882 0.59215686\\n    0.48235294]]\\n\\n  [[0.24313725 0.18039216 0.18823529 ... 0.51764706 0.49019608\\n    0.48627451]\\n   [0.07843137 0.         0.03137255 ... 0.34509804 0.3254902\\n    0.34117647]\\n   [0.09411765 0.02745098 0.10588235 ... 0.32941176 0.32941176\\n    0.28627451]\\n   ...\\n   [0.66666667 0.6        0.63137255 ... 0.52156863 0.12156863\\n    0.13333333]\\n   [0.54509804 0.48235294 0.56470588 ... 0.58039216 0.24313725\\n    0.20784314]\\n   [0.56470588 0.50588235 0.55686275 ... 0.72156863 0.4627451\\n    0.36078431]]\\n\\n  [[0.24705882 0.17647059 0.16862745 ... 0.42352941 0.4\\n    0.40392157]\\n   [0.07843137 0.         0.         ... 0.21568627 0.19607843\\n    0.22352941]\\n   [0.08235294 0.         0.03137255 ... 0.19607843 0.19607843\\n    0....65098039 0.65098039 0.65882353 ... 0.5372549  0.5372549\\n    0.57647059]\\n   [0.63137255 0.63529412 0.65490196 ... 0.51372549 0.54117647\\n    0.5372549 ]\\n   [0.64313725 0.63529412 0.64313725 ... 0.6        0.61568627\\n    0.61176471]]\\n\\n  [[0.57254902 0.56470588 0.56470588 ... 0.64705882 0.63921569\\n    0.64705882]\\n   [0.56862745 0.56078431 0.56078431 ... 0.63137255 0.63529412\\n    0.63137255]\\n   [0.57647059 0.56862745 0.56862745 ... 0.58823529 0.52156863\\n    0.45882353]\\n   ...\\n   [0.61176471 0.61176471 0.61568627 ... 0.49803922 0.49411765\\n    0.53333333]\\n   [0.59607843 0.6        0.61960784 ... 0.49019608 0.52156863\\n    0.51764706]\\n   [0.61960784 0.61176471 0.61960784 ... 0.57254902 0.59215686\\n    0.58823529]]\\n\\n  [[0.75294118 0.74117647 0.74117647 ... 0.64705882 0.63921569\\n    0.64705882]\\n   [0.7372549  0.72941176 0.72941176 ... 0.62352941 0.62352941\\n    0.61568627]\\n   [0.74117647 0.72941176 0.73333333 ... 0.57647059 0.50588235\\n    0.43529412]\\n   ...\\n   [0.58823529 0.59215686 0.59607843 ... 0.43921569 0.44705882\\n    0.49803922]\\n   [0.58431373 0.58823529 0.60784314 ... 0.45882353 0.48627451\\n    0.47058824]\\n   [0.60784314 0.6        0.60784314 ... 0.54509804 0.55294118\\n    0.5372549 ]]]]).cached_data\u001b[0m\n",
      "\u001b[1m\u001b[31mE        +    and   <class 'needle.backend_ndarray.ndarray.NDArray'> = nd.NDArray\u001b[0m\n",
      "\n",
      "X          = needle.Tensor([[[[0.23137255 0.16862745 0.19607843 ... 0.61960784 0.59607843\n",
      "    0.58039216]\n",
      "   [0.0627451  0.        ....45882353 0.48627451\n",
      "    0.47058824]\n",
      "   [0.60784314 0.6        0.60784314 ... 0.54509804 0.55294118\n",
      "    0.5372549 ]]]])\n",
      "batch_size = 15\n",
      "cifar10_train_dataset = <needle.data.datasets.cifar10_dataset.CIFAR10Dataset object at 0x7f9556941e50>\n",
      "device     = cuda()\n",
      "train      = False\n",
      "train_loader = <needle.data.data_basic.DataLoader object at 0x7f955683b210>\n",
      "y          = needle.Tensor([6 9 9 4 1 1 2 7 8 3 4 7 7 2 9])\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_cifar_ptb_data.py\u001b[0m:43: AssertionError\n",
      "\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_cifar_ptb_data.py::\u001b[1mtest_cifar10_loader[cpu-True-1]\u001b[0m - AssertionError: Expected X.cached_data to be nd.NDArray, but got <class 'nu...\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_cifar_ptb_data.py::\u001b[1mtest_cifar10_loader[cpu-True-15]\u001b[0m - AssertionError: Expected X.cached_data to be nd.NDArray, but got <class 'nu...\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_cifar_ptb_data.py::\u001b[1mtest_cifar10_loader[cpu-False-1]\u001b[0m - AssertionError: Expected X.cached_data to be nd.NDArray, but got <class 'nu...\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_cifar_ptb_data.py::\u001b[1mtest_cifar10_loader[cpu-False-15]\u001b[0m - AssertionError: Expected X.cached_data to be nd.NDArray, but got <class 'nu...\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_cifar_ptb_data.py::\u001b[1mtest_cifar10_loader[cuda-True-1]\u001b[0m - AssertionError: Expected X.cached_data to be nd.NDArray, but got <class 'nu...\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_cifar_ptb_data.py::\u001b[1mtest_cifar10_loader[cuda-True-15]\u001b[0m - AssertionError: Expected X.cached_data to be nd.NDArray, but got <class 'nu...\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_cifar_ptb_data.py::\u001b[1mtest_cifar10_loader[cuda-False-1]\u001b[0m - AssertionError: Expected X.cached_data to be nd.NDArray, but got <class 'nu...\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_cifar_ptb_data.py::\u001b[1mtest_cifar10_loader[cuda-False-15]\u001b[0m - AssertionError: Expected X.cached_data to be nd.NDArray, but got <class 'nu...\n",
      "\u001b[31m================= \u001b[31m\u001b[1m8 failed\u001b[0m, \u001b[32m2 passed\u001b[0m, \u001b[33m1793 deselected\u001b[0m\u001b[31m in 8.22s\u001b[0m\u001b[31m =================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m pytest -l -v -k \"test_cifar10\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Part 3: Convolutional neural network [40 points]\n",
    "\n",
    "Here's an outline of what you will do in this task.\n",
    "\n",
    "In `python/needle/backend_ndarray/ndarray.py`, implement:\n",
    "- `flip`\n",
    "- `pad`\n",
    "\n",
    "In `python/needle/ops_mathematic.py`, implement (forward and backward):\n",
    "- `Flip`\n",
    "- `Dilate`\n",
    "- `UnDilate`\n",
    "- `Conv`\n",
    "\n",
    "In `python/needle/nn/nn_conv.py`, implement:\n",
    "- `Conv`\n",
    "\n",
    "In `apps/models.py`, fill in the `ResNet9` class.  \n",
    "\n",
    "In `apps/simple_ml.py`, fill in:\n",
    "- `epoch_general_cifar10`,\n",
    "- `train_cifar10`\n",
    "- `evaluate_cifar10`\n",
    "\n",
    "We have provided a `BatchNorm2d` implementation in `python/needle/nn/nn_basic.py` for you as a wrapper around your previous `BatchNorm1d` implementation. \n",
    "\n",
    "**Note**: Remember to copy the solution of `nn_basic.py` from previous homework, make sure to not overwrite the `BatchNorm2d` module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding ndarrays\n",
    "\n",
    "Convolution as typically implemented in deep learning libraries cuts down the size of inputs;\n",
    "e.g., a (1, 32, 32, 3) image convolved with a 3x3 filter would give a (1, 30, 30, c) output.\n",
    "A way around this is to pad the input ndarray before performing convolution, e.g., pad with zeros to get a (1, 34, 34, 3) ndarray so that the result is (1, 32, 32, 3). \n",
    "\n",
    "Padding is also required for the backward pass of convolution.\n",
    "\n",
    "You should implement `pad` in `ndarray.py` to closely reflect the behavior of `np.pad`.\n",
    "That is, `pad` should take a tuple of 2-tuples with length equal to the number of dimensions of the array,\n",
    "where each element in the 2-tuple corresponds to \"left padding\" and \"right padding\", respectively.\n",
    "\n",
    "For example, if `A` is a (10, 32, 32, 8) ndarray (think NHWC), then `A.pad( (0, 0), (2, 2), (2, 2), (0, 0) )` would be a (10, 36, 36, 8) ndarray where the \"spatial\" dimension has been padded by two zeros on all sides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.11.10, pytest-8.3.4, pluggy-1.5.0 -- /opt/conda/envs/train/bin/python\n",
      "cachedir: .pytest_cache\n",
      "hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase(PosixPath('/mnt/zhumingzhu/work/00test/hw4/.hypothesis/examples'))\n",
      "rootdir: /mnt/zhumingzhu/work/00test/hw4\n",
      "plugins: hypothesis-6.115.5\n",
      "collected 1803 items / 1801 deselected / 2 selected                            \u001b[0m\u001b[1m\n",
      "\n",
      "tests/hw4/test_conv.py::test_pad_forward[params0-needle.backend_ndarray.ndarray_backend_cpu] \u001b[31mFAILED\u001b[0m\u001b[31m [ 50%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_pad_forward[params1-needle.backend_ndarray.ndarray_backend_cpu] \u001b[31mFAILED\u001b[0m\u001b[31m [100%]\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m_____ test_pad_forward[params0-needle.backend_ndarray.ndarray_backend_cpu] _____\u001b[0m\n",
      "\n",
      "params = {'padding': ((0, 0), (2, 2), (2, 2), (0, 0)), 'shape': (10, 32, 32, 8)}\n",
      "device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [nd.cpu()])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, pad_params)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_pad_forward\u001b[39;49;00m(params, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        shape, padding = params[\u001b[33m'\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33mpadding\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape)\u001b[90m\u001b[39;49;00m\n",
      "        _B = np.pad(_A, padding)\u001b[90m\u001b[39;49;00m\n",
      "        A = nd.NDArray(_A, device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       B = A.pad(padding)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "A          = NDArray([[[[ 1.76405239e+00  4.00157213e-01  9.78738010e-01 ... -9.77277875e-01\n",
      "     9.50088441e-01 -1.51357204e-01]\n",
      " ...08956736e-01 -2.06575304e-01  1.22084665e+00 ...  3.40539336e-01\n",
      "     6.47998601e-02  8.28751862e-01]]]], device=cpu())\n",
      "_A         = array([[[[ 1.76405235e+00,  4.00157208e-01,  9.78737984e-01, ...,\n",
      "          -9.77277880e-01,  9.50088418e-01, -1.51357...4.08956722e-01, -2.06575306e-01,  1.22084664e+00, ...,\n",
      "           3.40539329e-01,  6.47998581e-02,  8.28751864e-01]]]])\n",
      "_B         = array([[[[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
      "           0.        ,  0.        ],\n",
      "         [ 0. ...0.        ],\n",
      "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
      "           0.        ,  0.        ]]]])\n",
      "device     = cpu()\n",
      "padding    = ((0, 0), (2, 2), (2, 2), (0, 0))\n",
      "params     = {'padding': ((0, 0), (2, 2), (2, 2), (0, 0)), 'shape': (10, 32, 32, 8)}\n",
      "shape      = (10, 32, 32, 8)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:98: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[[[ 1.76405239e+00  4.00157213e-01  9.78738010e-01 ... -9.77277875e-01\n",
      "     9.50088441e-01 -1.51357204e-01]\n",
      " ...08956736e-01 -2.06575304e-01  1.22084665e+00 ...  3.40539336e-01\n",
      "     6.47998601e-02  8.28751862e-01]]]], device=cpu())\n",
      "axes = ((0, 0), (2, 2), (2, 2), (0, 0))\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mpad\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, axes):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m\n",
      "    \u001b[33m    Pad this ndarray by zeros by the specified amount in `axes`,\u001b[39;49;00m\n",
      "    \u001b[33m    which lists for _all_ axes the left and right padding amount, e.g.,\u001b[39;49;00m\n",
      "    \u001b[33m    axes = ( (0, 0), (1, 1), (0, 0)) pads the middle axis with a 0 on the left and right side.\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "axes       = ((0, 0), (2, 2), (2, 2), (0, 0))\n",
      "self       = NDArray([[[[ 1.76405239e+00  4.00157213e-01  9.78738010e-01 ... -9.77277875e-01\n",
      "     9.50088441e-01 -1.51357204e-01]\n",
      " ...08956736e-01 -2.06575304e-01  1.22084665e+00 ...  3.40539336e-01\n",
      "     6.47998601e-02  8.28751862e-01]]]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:616: NotImplementedError\n",
      "\u001b[31m\u001b[1m_____ test_pad_forward[params1-needle.backend_ndarray.ndarray_backend_cpu] _____\u001b[0m\n",
      "\n",
      "params = {'padding': ((0, 0), (0, 0), (0, 0), (0, 0)), 'shape': (10, 32, 32, 8)}\n",
      "device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [nd.cpu()])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, pad_params)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_pad_forward\u001b[39;49;00m(params, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        shape, padding = params[\u001b[33m'\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33mpadding\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape)\u001b[90m\u001b[39;49;00m\n",
      "        _B = np.pad(_A, padding)\u001b[90m\u001b[39;49;00m\n",
      "        A = nd.NDArray(_A, device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       B = A.pad(padding)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "A          = NDArray([[[[ 1.76405239e+00  4.00157213e-01  9.78738010e-01 ... -9.77277875e-01\n",
      "     9.50088441e-01 -1.51357204e-01]\n",
      " ...08956736e-01 -2.06575304e-01  1.22084665e+00 ...  3.40539336e-01\n",
      "     6.47998601e-02  8.28751862e-01]]]], device=cpu())\n",
      "_A         = array([[[[ 1.76405235e+00,  4.00157208e-01,  9.78737984e-01, ...,\n",
      "          -9.77277880e-01,  9.50088418e-01, -1.51357...4.08956722e-01, -2.06575306e-01,  1.22084664e+00, ...,\n",
      "           3.40539329e-01,  6.47998581e-02,  8.28751864e-01]]]])\n",
      "_B         = array([[[[ 1.76405235e+00,  4.00157208e-01,  9.78737984e-01, ...,\n",
      "          -9.77277880e-01,  9.50088418e-01, -1.51357...4.08956722e-01, -2.06575306e-01,  1.22084664e+00, ...,\n",
      "           3.40539329e-01,  6.47998581e-02,  8.28751864e-01]]]])\n",
      "device     = cpu()\n",
      "padding    = ((0, 0), (0, 0), (0, 0), (0, 0))\n",
      "params     = {'padding': ((0, 0), (0, 0), (0, 0), (0, 0)), 'shape': (10, 32, 32, 8)}\n",
      "shape      = (10, 32, 32, 8)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:98: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[[[ 1.76405239e+00  4.00157213e-01  9.78738010e-01 ... -9.77277875e-01\n",
      "     9.50088441e-01 -1.51357204e-01]\n",
      " ...08956736e-01 -2.06575304e-01  1.22084665e+00 ...  3.40539336e-01\n",
      "     6.47998601e-02  8.28751862e-01]]]], device=cpu())\n",
      "axes = ((0, 0), (0, 0), (0, 0), (0, 0))\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mpad\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, axes):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m\n",
      "    \u001b[33m    Pad this ndarray by zeros by the specified amount in `axes`,\u001b[39;49;00m\n",
      "    \u001b[33m    which lists for _all_ axes the left and right padding amount, e.g.,\u001b[39;49;00m\n",
      "    \u001b[33m    axes = ( (0, 0), (1, 1), (0, 0)) pads the middle axis with a 0 on the left and right side.\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "axes       = ((0, 0), (0, 0), (0, 0), (0, 0))\n",
      "self       = NDArray([[[[ 1.76405239e+00  4.00157213e-01  9.78738010e-01 ... -9.77277875e-01\n",
      "     9.50088441e-01 -1.51357204e-01]\n",
      " ...08956736e-01 -2.06575304e-01  1.22084665e+00 ...  3.40539336e-01\n",
      "     6.47998601e-02  8.28751862e-01]]]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:616: NotImplementedError\n",
      "\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_pad_forward[params0-needle.backend_ndarray.ndarray_backend_cpu]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_pad_forward[params1-needle.backend_ndarray.ndarray_backend_cpu]\u001b[0m - NotImplementedError\n",
      "\u001b[31m====================== \u001b[31m\u001b[1m2 failed\u001b[0m, \u001b[33m1801 deselected\u001b[0m\u001b[31m in 1.98s\u001b[0m\u001b[31m ======================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m pytest -l -v -k \"pad_forward\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flipping ndarrays & FlipOp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import ctypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some utility code for a demonstration below which you can probably ignore. It might be instructive to check out the `offset` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reads off the underlying data array in order (i.e., offset 0, offset 1, ..., offset n)\n",
    "# i.e., ignoring strides\n",
    "def raw_data(X):\n",
    "    X = np.array(X) # copy, thus compact X\n",
    "    return np.frombuffer(ctypes.string_at(X.ctypes.data, X.nbytes), dtype=X.dtype, count=X.size)\n",
    "\n",
    "# Xold and Xnew should reference the same underlying data\n",
    "def offset(Xold, Xnew):\n",
    "    assert Xold.itemsize == Xnew.itemsize\n",
    "    # compare addresses to the beginning of the arrays\n",
    "    return (Xnew.ctypes.data - Xold.ctypes.data)//Xnew.itemsize\n",
    "\n",
    "def strides(X):\n",
    "    return ', '.join([str(x//X.itemsize) for x in X.strides])\n",
    "\n",
    "def format_array(X, shape):\n",
    "    assert len(shape) == 3, \"I only made this formatting work for ndims = 3\"\n",
    "    def chunks(l, n):\n",
    "        n = max(1, n)\n",
    "        return (l[i:i+n] for i in range(0, len(l), n))\n",
    "    a = [str(x) if x >= 10 else ' ' + str(x) for x in X]\n",
    "    a = ['(' + ' '.join(y) + ')' for y in [x for x in chunks(a, shape[-1])]]\n",
    "    a = ['|' + ' '.join(y) + '|' for y in [x for x in chunks(a, shape[-2])]]\n",
    "    return '  '.join(a)\n",
    "\n",
    "def inspect_array(X, *, is_a_copy_of):\n",
    "    # compacts X, then reads it off in order\n",
    "    print('Data: %s' % format_array(raw_data(X), X.shape))\n",
    "    # compares address of X to copy_of, thus finding X's offset\n",
    "    print('Offset: %s' % offset(is_a_copy_of, X))\n",
    "    print('Strides: %s' % strides(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "In order to implement the backwards pass of 2D convolution, we will (probably) need a function which _flips_\n",
    "axes of ndarrays. We say \"probably\" because you could probably cleverly implement your convolution forward\n",
    "function to avoid this. However, we think it is easiest to think about this if you have the ability to \"flip\" the kernel along its vertical and horizontal dimensions.\n",
    "\n",
    "We will try to build up your intuition for the \"flip\" operation below in order to help you figure out how to implement it in `ndarray.py`. To do that, we explore numpy's `np.flip` function below. One thing to note is that\n",
    "`flip` is typically implemented by using negative strides and changing the _offset_ of the underlying array.\n",
    "\n",
    "For example, flipping an array on _all_ of its axes is equivalent to reversing the array. In this case, you can imagine that we would want all the strides to be negative, and the offset to be the length of the array (to start at the end of the array and \"stride\" backwards).\n",
    "\n",
    "Since we did not explicitly support negative strides in our implementation for the last homework, we will merely call `NDArray.make` with them to make our \"flipped\" array and then immediately call `.compact()`. Other than changing unsigned ints to signed ints in a few places, we suspect your existing `compact` function should not have to change at all to accomodate negative strides. In the .cc and .cu files we distributed, we have already changed the function signatures to reflect this.\n",
    "\n",
    "Alternatively, you could simply implement `flip` in the CPU backend by copying memory, which you _may_ find more intuitive. We suggest following our mini tutorial below to keep your implementation Python-focused, since we believe it is involves approximately the same amount of effort to implement it slightly more naively in C."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this array as reference for the other examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data: |( 1  2  3  4) ( 5  6  7  8)|  |( 9 10 11 12) (13 14 15 16)|  |(17 18 19 20) (21 22 23 24)|\n",
      "Offset: 0\n",
      "Strides: 8, 4, 1\n"
     ]
    }
   ],
   "source": [
    "A = np.arange(1, 25).reshape(3, 2, 4)\n",
    "inspect_array(A, is_a_copy_of=A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have put brackets around each axis of the array. Notice that for this array, the offset is 0 and the strides are all positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See what happens when you flip the array along the last axis below. \n",
    "Note that the `inspect_array` function compacts the array after flipping it so you can see the\n",
    "\"logical\" order of the data, and the offset is calculated by comparing the address of the **non**-compacted\n",
    "flipped array with that of `is_copy_of`, i.e., the array `A` we looked at above.\n",
    "\n",
    "That is, we are looking at how numpy calculates the strides and offset for flipped arrays in order\n",
    "to copy this behavior in our own implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data: |( 4  3  2  1) ( 8  7  6  5)|  |(12 11 10  9) (16 15 14 13)|  |(20 19 18 17) (24 23 22 21)|\n",
      "Offset: 3\n",
      "Strides: 8, 4, -1\n"
     ]
    }
   ],
   "source": [
    "inspect_array(np.flip(A, (2,)), is_a_copy_of=A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So flipping the last axis reverses the order of the elements within each 4-dimensional \"cell\", as you can see above. The stride corresponding to the axis we flipped has been negated. And the offset is 3 -- this makes sense, e.g., because we want the new \"first\" element of the array to be 4, which was at index 3 in `A`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data: |( 5  6  7  8) ( 1  2  3  4)|  |(13 14 15 16) ( 9 10 11 12)|  |(21 22 23 24) (17 18 19 20)|\n",
      "Offset: 4\n",
      "Strides: 8, -4, 1\n"
     ]
    }
   ],
   "source": [
    "inspect_array(np.flip(A, (1,)), is_a_copy_of=A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again for the middle axis: we negate the middle stride, and the offset is 4, which seems reasonable since we now want the first element to be 5, which was at index 4 in the original array `A`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data: |(17 18 19 20) (21 22 23 24)|  |( 9 10 11 12) (13 14 15 16)|  |( 1  2  3  4) ( 5  6  7  8)|\n",
      "Offset: 16\n",
      "Strides: -8, 4, 1\n"
     ]
    }
   ],
   "source": [
    "inspect_array(np.flip(A, (0,)), is_a_copy_of=A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to infer the more general algorithm for computing the offset given the axis to flip."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe what happens when we flip _all_ axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data: |(24 23 22 21) (20 19 18 17)|  |(16 15 14 13) (12 11 10  9)|  |( 8  7  6  5) ( 4  3  2  1)|\n",
      "Offset: 23\n",
      "Strides: -8, -4, -1\n"
     ]
    }
   ],
   "source": [
    "inspect_array(np.flip(A, (0, 1, 2)), is_a_copy_of=A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned earlier, the offset is then sufficient to point to the last element of the array, and this is just the \"reverse order\" version of `A`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we flip just axes 1 and 0..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data: |(21 22 23 24) (17 18 19 20)|  |(13 14 15 16) ( 9 10 11 12)|  |( 5  6  7  8) ( 1  2  3  4)|\n",
      "Offset: 20\n",
      "Strides: -8, -4, 1\n"
     ]
    }
   ],
   "source": [
    "inspect_array(np.flip(A, (0, 1)), is_a_copy_of=A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The offset is 20. Looking back on our previous offset computations, do you notice something?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------\n",
    "\n",
    "With this exploration of numpy's ndarray flipping functionality, which uses negative strides and a custom offset,\n",
    "try to implement `flip` in `ndarray.py`. You also must implement \"flip\" forward and backward functions in `ops_mathematic.py`; note that these should be extremely short.\n",
    "\n",
    "**Important:** You should call NDArray.make with the new strides and offset, and then immediately `.compact()` this array. The resulting array is then copied and has positive strides. We want this (less-than-optimal) behavior because we did not account for negative strides in our previous implementation. _Aside:_ If you want, consider where/if negative strides break your implementation. `__getitem__` definitely doesn't work due to how we processed slices; is there anything else? (_Note_: this isn't graded.)\n",
    "\n",
    "Also, if you want to add a `flip` operator implementation on the CPU/CUDA backends instead, that's also okay.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.11.10, pytest-8.3.4, pluggy-1.5.0 -- /opt/conda/envs/train/bin/python\n",
      "cachedir: .pytest_cache\n",
      "hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase(PosixPath('/mnt/zhumingzhu/work/00test/hw4/.hypothesis/examples'))\n",
      "rootdir: /mnt/zhumingzhu/work/00test/hw4\n",
      "plugins: hypothesis-6.115.5\n",
      "collected 1803 items / 1763 deselected / 40 selected                           \u001b[0m\u001b[1m\n",
      "\n",
      "tests/hw4/test_conv.py::test_flip_forward[params0-needle.backend_ndarray.ndarray_backend_cpu] \u001b[31mFAILED\u001b[0m\u001b[31m [  2%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_flip_forward[params0-needle.backend_ndarray.ndarray_backend_cuda] \u001b[31mFAILED\u001b[0m\u001b[31m [  5%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_flip_forward[params1-needle.backend_ndarray.ndarray_backend_cpu] \u001b[31mFAILED\u001b[0m\u001b[31m [  7%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_flip_forward[params1-needle.backend_ndarray.ndarray_backend_cuda] \u001b[31mFAILED\u001b[0m\u001b[31m [ 10%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_flip_forward[params2-needle.backend_ndarray.ndarray_backend_cpu] \u001b[31mFAILED\u001b[0m\u001b[31m [ 12%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_flip_forward[params2-needle.backend_ndarray.ndarray_backend_cuda] \u001b[31mFAILED\u001b[0m\u001b[31m [ 15%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_flip_forward[params3-needle.backend_ndarray.ndarray_backend_cpu] \u001b[31mFAILED\u001b[0m\u001b[31m [ 17%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_flip_forward[params3-needle.backend_ndarray.ndarray_backend_cuda] \u001b[31mFAILED\u001b[0m\u001b[31m [ 20%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_flip_forward[params4-needle.backend_ndarray.ndarray_backend_cpu] \u001b[31mFAILED\u001b[0m\u001b[31m [ 22%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_flip_forward[params4-needle.backend_ndarray.ndarray_backend_cuda] \u001b[31mFAILED\u001b[0m\u001b[31m [ 25%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_flip_forward[params5-needle.backend_ndarray.ndarray_backend_cpu] \u001b[31mFAILED\u001b[0m\u001b[31m [ 27%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_flip_forward[params5-needle.backend_ndarray.ndarray_backend_cuda] \u001b[31mFAILED\u001b[0m\u001b[31m [ 30%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_flip_forward[params6-needle.backend_ndarray.ndarray_backend_cpu] \u001b[31mFAILED\u001b[0m\u001b[31m [ 32%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_flip_forward[params6-needle.backend_ndarray.ndarray_backend_cuda] \u001b[31mFAILED\u001b[0m\u001b[31m [ 35%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_flip_forward[params7-needle.backend_ndarray.ndarray_backend_cpu] \u001b[31mFAILED\u001b[0m\u001b[31m [ 37%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_flip_forward[params7-needle.backend_ndarray.ndarray_backend_cuda] \u001b[31mFAILED\u001b[0m\u001b[31m [ 40%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_flip_forward[params8-needle.backend_ndarray.ndarray_backend_cpu] \u001b[31mFAILED\u001b[0m\u001b[31m [ 42%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_flip_forward[params8-needle.backend_ndarray.ndarray_backend_cuda] \u001b[31mFAILED\u001b[0m\u001b[31m [ 45%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_flip_forward[params9-needle.backend_ndarray.ndarray_backend_cpu] \u001b[31mFAILED\u001b[0m\u001b[31m [ 47%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_flip_forward[params9-needle.backend_ndarray.ndarray_backend_cuda] \u001b[31mFAILED\u001b[0m\u001b[31m [ 50%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_flip_backward[params0-needle.backend_ndarray.ndarray_backend_cpu] \u001b[31mFAILED\u001b[0m\u001b[31m [ 52%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_flip_backward[params0-needle.backend_ndarray.ndarray_backend_cuda] \u001b[31mFAILED\u001b[0m\u001b[31m [ 55%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_flip_backward[params1-needle.backend_ndarray.ndarray_backend_cpu] \u001b[31mFAILED\u001b[0m\u001b[31m [ 57%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_flip_backward[params1-needle.backend_ndarray.ndarray_backend_cuda] \u001b[31mFAILED\u001b[0m\u001b[31m [ 60%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_flip_backward[params2-needle.backend_ndarray.ndarray_backend_cpu] \u001b[31mFAILED\u001b[0m\u001b[31m [ 62%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_flip_backward[params2-needle.backend_ndarray.ndarray_backend_cuda] \u001b[31mFAILED\u001b[0m\u001b[31m [ 65%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_flip_backward[params3-needle.backend_ndarray.ndarray_backend_cpu] \u001b[31mFAILED\u001b[0m\u001b[31m [ 67%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_flip_backward[params3-needle.backend_ndarray.ndarray_backend_cuda] \u001b[31mFAILED\u001b[0m\u001b[31m [ 70%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_flip_backward[params4-needle.backend_ndarray.ndarray_backend_cpu] \u001b[31mFAILED\u001b[0m\u001b[31m [ 72%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_flip_backward[params4-needle.backend_ndarray.ndarray_backend_cuda] \u001b[31mFAILED\u001b[0m\u001b[31m [ 75%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_flip_backward[params5-needle.backend_ndarray.ndarray_backend_cpu] \u001b[31mFAILED\u001b[0m\u001b[31m [ 77%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_flip_backward[params5-needle.backend_ndarray.ndarray_backend_cuda] \u001b[31mFAILED\u001b[0m\u001b[31m [ 80%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_flip_backward[params6-needle.backend_ndarray.ndarray_backend_cpu] \u001b[31mFAILED\u001b[0m\u001b[31m [ 82%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_flip_backward[params6-needle.backend_ndarray.ndarray_backend_cuda] \u001b[31mFAILED\u001b[0m\u001b[31m [ 85%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_flip_backward[params7-needle.backend_ndarray.ndarray_backend_cpu] \u001b[31mFAILED\u001b[0m\u001b[31m [ 87%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_flip_backward[params7-needle.backend_ndarray.ndarray_backend_cuda] \u001b[31mFAILED\u001b[0m\u001b[31m [ 90%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_flip_backward[params8-needle.backend_ndarray.ndarray_backend_cpu] \u001b[31mFAILED\u001b[0m\u001b[31m [ 92%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_flip_backward[params8-needle.backend_ndarray.ndarray_backend_cuda] \u001b[31mFAILED\u001b[0m\u001b[31m [ 95%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_flip_backward[params9-needle.backend_ndarray.ndarray_backend_cpu] \u001b[31mFAILED\u001b[0m\u001b[31m [ 97%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_flip_backward[params9-needle.backend_ndarray.ndarray_backend_cuda] \u001b[31mFAILED\u001b[0m\u001b[31m [100%]\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m____ test_flip_forward[params0-needle.backend_ndarray.ndarray_backend_cpu] _____\u001b[0m\n",
      "\n",
      "params = {'axes': (0,), 'shape': (10, 5)}, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, flip_forward_params)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flip_forward\u001b[39;49;00m(params, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        shape, axes = params[\u001b[33m'\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape)\u001b[90m\u001b[39;49;00m\n",
      "        _B = np.flip(_A, axes)\u001b[90m\u001b[39;49;00m\n",
      "        A = ndl.Tensor(_A, device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       B = ndl.flip(A, axes=axes)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "A          = needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0....553   -1.420018   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]])\n",
      "_A         = array([[ 1.76405235,  0.40015721,  0.97873798,  2.2408932 ,  1.86755799],\n",
      "       [-0.97727788,  0.95008842, -0.1513572...794, -1.70627019,  1.9507754 , -0.50965218],\n",
      "       [-0.4380743 , -1.25279536,  0.77749036, -1.61389785, -0.21274028]])\n",
      "_B         = array([[-0.4380743 , -1.25279536,  0.77749036, -1.61389785, -0.21274028],\n",
      "       [-1.04855297, -1.42001794, -1.7062701...842, -0.15135721, -0.10321885,  0.4105985 ],\n",
      "       [ 1.76405235,  0.40015721,  0.97873798,  2.2408932 ,  1.86755799]])\n",
      "axes       = (0,)\n",
      "device     = cpu()\n",
      "params     = {'axes': (0,), 'shape': (10, 5)}\n",
      "shape      = (10, 5)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:123: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:454: in flip\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Flip(axes)(a)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0....553   -1.420018   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]])\n",
      "        axes       = (0,)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0...3   -1.420018   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]]),)\n",
      "        self       = <needle.ops.ops_mathematic.Flip object at 0x7fe4da0f7710>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0...3   -1.420018   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]]),)\n",
      "        op         = <needle.ops.ops_mathematic.Flip object at 0x7fe4da0f7710>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fe4d9ec3b50>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fe4d9ec3b50>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Flip object at 0x7fe4da0f7710>\n",
      "a = NDArray([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0.103218...8   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]], device=cpu())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "a          = NDArray([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0.103218...8   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]], device=cpu())\n",
      "self       = <needle.ops.ops_mathematic.Flip object at 0x7fe4da0f7710>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:444: NotImplementedError\n",
      "\u001b[31m\u001b[1m____ test_flip_forward[params0-needle.backend_ndarray.ndarray_backend_cuda] ____\u001b[0m\n",
      "\n",
      "params = {'axes': (0,), 'shape': (10, 5)}, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, flip_forward_params)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flip_forward\u001b[39;49;00m(params, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        shape, axes = params[\u001b[33m'\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape)\u001b[90m\u001b[39;49;00m\n",
      "        _B = np.flip(_A, axes)\u001b[90m\u001b[39;49;00m\n",
      "        A = ndl.Tensor(_A, device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       B = ndl.flip(A, axes=axes)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "A          = needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0....553   -1.420018   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]])\n",
      "_A         = array([[ 1.76405235,  0.40015721,  0.97873798,  2.2408932 ,  1.86755799],\n",
      "       [-0.97727788,  0.95008842, -0.1513572...794, -1.70627019,  1.9507754 , -0.50965218],\n",
      "       [-0.4380743 , -1.25279536,  0.77749036, -1.61389785, -0.21274028]])\n",
      "_B         = array([[-0.4380743 , -1.25279536,  0.77749036, -1.61389785, -0.21274028],\n",
      "       [-1.04855297, -1.42001794, -1.7062701...842, -0.15135721, -0.10321885,  0.4105985 ],\n",
      "       [ 1.76405235,  0.40015721,  0.97873798,  2.2408932 ,  1.86755799]])\n",
      "axes       = (0,)\n",
      "device     = cuda()\n",
      "params     = {'axes': (0,), 'shape': (10, 5)}\n",
      "shape      = (10, 5)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:123: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:454: in flip\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Flip(axes)(a)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0....553   -1.420018   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]])\n",
      "        axes       = (0,)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0...3   -1.420018   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]]),)\n",
      "        self       = <needle.ops.ops_mathematic.Flip object at 0x7fe4d92b88d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0...3   -1.420018   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]]),)\n",
      "        op         = <needle.ops.ops_mathematic.Flip object at 0x7fe4d92b88d0>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fe4d92b85d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fe4d92b85d0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Flip object at 0x7fe4d92b88d0>\n",
      "a = NDArray([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0.103218...   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]], device=cuda())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "a          = NDArray([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0.103218...   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]], device=cuda())\n",
      "self       = <needle.ops.ops_mathematic.Flip object at 0x7fe4d92b88d0>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:444: NotImplementedError\n",
      "\u001b[31m\u001b[1m____ test_flip_forward[params1-needle.backend_ndarray.ndarray_backend_cpu] _____\u001b[0m\n",
      "\n",
      "params = {'axes': (1,), 'shape': (10, 5)}, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, flip_forward_params)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flip_forward\u001b[39;49;00m(params, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        shape, axes = params[\u001b[33m'\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape)\u001b[90m\u001b[39;49;00m\n",
      "        _B = np.flip(_A, axes)\u001b[90m\u001b[39;49;00m\n",
      "        A = ndl.Tensor(_A, device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       B = ndl.flip(A, axes=axes)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "A          = needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0....553   -1.420018   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]])\n",
      "_A         = array([[ 1.76405235,  0.40015721,  0.97873798,  2.2408932 ,  1.86755799],\n",
      "       [-0.97727788,  0.95008842, -0.1513572...794, -1.70627019,  1.9507754 , -0.50965218],\n",
      "       [-0.4380743 , -1.25279536,  0.77749036, -1.61389785, -0.21274028]])\n",
      "_B         = array([[ 1.86755799,  2.2408932 ,  0.97873798,  0.40015721,  1.76405235],\n",
      "       [ 0.4105985 , -0.10321885, -0.1513572...54 , -1.70627019, -1.42001794, -1.04855297],\n",
      "       [-0.21274028, -1.61389785,  0.77749036, -1.25279536, -0.4380743 ]])\n",
      "axes       = (1,)\n",
      "device     = cpu()\n",
      "params     = {'axes': (1,), 'shape': (10, 5)}\n",
      "shape      = (10, 5)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:123: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:454: in flip\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Flip(axes)(a)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0....553   -1.420018   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]])\n",
      "        axes       = (1,)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0...3   -1.420018   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]]),)\n",
      "        self       = <needle.ops.ops_mathematic.Flip object at 0x7fe4d93cd9d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0...3   -1.420018   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]]),)\n",
      "        op         = <needle.ops.ops_mathematic.Flip object at 0x7fe4d93cd9d0>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fe4d93cd790>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fe4d93cd790>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Flip object at 0x7fe4d93cd9d0>\n",
      "a = NDArray([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0.103218...8   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]], device=cpu())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "a          = NDArray([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0.103218...8   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]], device=cpu())\n",
      "self       = <needle.ops.ops_mathematic.Flip object at 0x7fe4d93cd9d0>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:444: NotImplementedError\n",
      "\u001b[31m\u001b[1m____ test_flip_forward[params1-needle.backend_ndarray.ndarray_backend_cuda] ____\u001b[0m\n",
      "\n",
      "params = {'axes': (1,), 'shape': (10, 5)}, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, flip_forward_params)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flip_forward\u001b[39;49;00m(params, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        shape, axes = params[\u001b[33m'\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape)\u001b[90m\u001b[39;49;00m\n",
      "        _B = np.flip(_A, axes)\u001b[90m\u001b[39;49;00m\n",
      "        A = ndl.Tensor(_A, device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       B = ndl.flip(A, axes=axes)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "A          = needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0....553   -1.420018   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]])\n",
      "_A         = array([[ 1.76405235,  0.40015721,  0.97873798,  2.2408932 ,  1.86755799],\n",
      "       [-0.97727788,  0.95008842, -0.1513572...794, -1.70627019,  1.9507754 , -0.50965218],\n",
      "       [-0.4380743 , -1.25279536,  0.77749036, -1.61389785, -0.21274028]])\n",
      "_B         = array([[ 1.86755799,  2.2408932 ,  0.97873798,  0.40015721,  1.76405235],\n",
      "       [ 0.4105985 , -0.10321885, -0.1513572...54 , -1.70627019, -1.42001794, -1.04855297],\n",
      "       [-0.21274028, -1.61389785,  0.77749036, -1.25279536, -0.4380743 ]])\n",
      "axes       = (1,)\n",
      "device     = cuda()\n",
      "params     = {'axes': (1,), 'shape': (10, 5)}\n",
      "shape      = (10, 5)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:123: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:454: in flip\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Flip(axes)(a)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0....553   -1.420018   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]])\n",
      "        axes       = (1,)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0...3   -1.420018   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]]),)\n",
      "        self       = <needle.ops.ops_mathematic.Flip object at 0x7fe4d9418310>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0...3   -1.420018   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]]),)\n",
      "        op         = <needle.ops.ops_mathematic.Flip object at 0x7fe4d9418310>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fe4d9418690>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fe4d9418690>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Flip object at 0x7fe4d9418310>\n",
      "a = NDArray([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0.103218...   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]], device=cuda())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "a          = NDArray([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0.103218...   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]], device=cuda())\n",
      "self       = <needle.ops.ops_mathematic.Flip object at 0x7fe4d9418310>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:444: NotImplementedError\n",
      "\u001b[31m\u001b[1m____ test_flip_forward[params2-needle.backend_ndarray.ndarray_backend_cpu] _____\u001b[0m\n",
      "\n",
      "params = {'axes': (0, 1), 'shape': (10, 5)}, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, flip_forward_params)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flip_forward\u001b[39;49;00m(params, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        shape, axes = params[\u001b[33m'\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape)\u001b[90m\u001b[39;49;00m\n",
      "        _B = np.flip(_A, axes)\u001b[90m\u001b[39;49;00m\n",
      "        A = ndl.Tensor(_A, device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       B = ndl.flip(A, axes=axes)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "A          = needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0....553   -1.420018   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]])\n",
      "_A         = array([[ 1.76405235,  0.40015721,  0.97873798,  2.2408932 ,  1.86755799],\n",
      "       [-0.97727788,  0.95008842, -0.1513572...794, -1.70627019,  1.9507754 , -0.50965218],\n",
      "       [-0.4380743 , -1.25279536,  0.77749036, -1.61389785, -0.21274028]])\n",
      "_B         = array([[-0.21274028, -1.61389785,  0.77749036, -1.25279536, -0.4380743 ],\n",
      "       [-0.50965218,  1.9507754 , -1.7062701...885, -0.15135721,  0.95008842, -0.97727788],\n",
      "       [ 1.86755799,  2.2408932 ,  0.97873798,  0.40015721,  1.76405235]])\n",
      "axes       = (0, 1)\n",
      "device     = cpu()\n",
      "params     = {'axes': (0, 1), 'shape': (10, 5)}\n",
      "shape      = (10, 5)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:123: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:454: in flip\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Flip(axes)(a)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0....553   -1.420018   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]])\n",
      "        axes       = (0, 1)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0...3   -1.420018   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]]),)\n",
      "        self       = <needle.ops.ops_mathematic.Flip object at 0x7fe4d936d590>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0...3   -1.420018   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]]),)\n",
      "        op         = <needle.ops.ops_mathematic.Flip object at 0x7fe4d936d590>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fe4d936d550>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fe4d936d550>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Flip object at 0x7fe4d936d590>\n",
      "a = NDArray([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0.103218...8   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]], device=cpu())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "a          = NDArray([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0.103218...8   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]], device=cpu())\n",
      "self       = <needle.ops.ops_mathematic.Flip object at 0x7fe4d936d590>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:444: NotImplementedError\n",
      "\u001b[31m\u001b[1m____ test_flip_forward[params2-needle.backend_ndarray.ndarray_backend_cuda] ____\u001b[0m\n",
      "\n",
      "params = {'axes': (0, 1), 'shape': (10, 5)}, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, flip_forward_params)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flip_forward\u001b[39;49;00m(params, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        shape, axes = params[\u001b[33m'\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape)\u001b[90m\u001b[39;49;00m\n",
      "        _B = np.flip(_A, axes)\u001b[90m\u001b[39;49;00m\n",
      "        A = ndl.Tensor(_A, device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       B = ndl.flip(A, axes=axes)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "A          = needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0....553   -1.420018   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]])\n",
      "_A         = array([[ 1.76405235,  0.40015721,  0.97873798,  2.2408932 ,  1.86755799],\n",
      "       [-0.97727788,  0.95008842, -0.1513572...794, -1.70627019,  1.9507754 , -0.50965218],\n",
      "       [-0.4380743 , -1.25279536,  0.77749036, -1.61389785, -0.21274028]])\n",
      "_B         = array([[-0.21274028, -1.61389785,  0.77749036, -1.25279536, -0.4380743 ],\n",
      "       [-0.50965218,  1.9507754 , -1.7062701...885, -0.15135721,  0.95008842, -0.97727788],\n",
      "       [ 1.86755799,  2.2408932 ,  0.97873798,  0.40015721,  1.76405235]])\n",
      "axes       = (0, 1)\n",
      "device     = cuda()\n",
      "params     = {'axes': (0, 1), 'shape': (10, 5)}\n",
      "shape      = (10, 5)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:123: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:454: in flip\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Flip(axes)(a)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0....553   -1.420018   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]])\n",
      "        axes       = (0, 1)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0...3   -1.420018   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]]),)\n",
      "        self       = <needle.ops.ops_mathematic.Flip object at 0x7fe4d94543d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0...3   -1.420018   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]]),)\n",
      "        op         = <needle.ops.ops_mathematic.Flip object at 0x7fe4d94543d0>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fe4d9454810>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fe4d9454810>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Flip object at 0x7fe4d94543d0>\n",
      "a = NDArray([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0.103218...   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]], device=cuda())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "a          = NDArray([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0.103218...   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]], device=cuda())\n",
      "self       = <needle.ops.ops_mathematic.Flip object at 0x7fe4d94543d0>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:444: NotImplementedError\n",
      "\u001b[31m\u001b[1m____ test_flip_forward[params3-needle.backend_ndarray.ndarray_backend_cpu] _____\u001b[0m\n",
      "\n",
      "params = {'axes': (0, 1), 'shape': (10, 32, 32, 8)}, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, flip_forward_params)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flip_forward\u001b[39;49;00m(params, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        shape, axes = params[\u001b[33m'\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape)\u001b[90m\u001b[39;49;00m\n",
      "        _B = np.flip(_A, axes)\u001b[90m\u001b[39;49;00m\n",
      "        A = ndl.Tensor(_A, device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       B = ndl.flip(A, axes=axes)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "A          = needle.Tensor([[[[ 1.76405239e+00  4.00157213e-01  9.78738010e-01 ... -9.77277875e-01\n",
      "     9.50088441e-01 -1.51357204e...7e+00]\n",
      "   [ 4.08956736e-01 -2.06575304e-01  1.22084665e+00 ...  3.40539336e-01\n",
      "     6.47998601e-02  8.28751862e-01]]]])\n",
      "_A         = array([[[[ 1.76405235e+00,  4.00157208e-01,  9.78737984e-01, ...,\n",
      "          -9.77277880e-01,  9.50088418e-01, -1.51357...4.08956722e-01, -2.06575306e-01,  1.22084664e+00, ...,\n",
      "           3.40539329e-01,  6.47998581e-02,  8.28751864e-01]]]])\n",
      "_B         = array([[[[ 1.67386460e+00,  1.09525980e+00, -4.17631359e-01, ...,\n",
      "           9.36113484e-01,  5.84919249e-01, -8.18787...8.13364259e-01, -1.46642433e+00,  5.21064876e-01, ...,\n",
      "          -3.19328417e-01,  6.91538751e-01,  6.94749144e-01]]]])\n",
      "axes       = (0, 1)\n",
      "device     = cpu()\n",
      "params     = {'axes': (0, 1), 'shape': (10, 32, 32, 8)}\n",
      "shape      = (10, 32, 32, 8)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:123: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:454: in flip\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Flip(axes)(a)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[[ 1.76405239e+00  4.00157213e-01  9.78738010e-01 ... -9.77277875e-01\n",
      "     9.50088441e-01 -1.51357204e...7e+00]\n",
      "   [ 4.08956736e-01 -2.06575304e-01  1.22084665e+00 ...  3.40539336e-01\n",
      "     6.47998601e-02  8.28751862e-01]]]])\n",
      "        axes       = (0, 1)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[ 1.76405239e+00  4.00157213e-01  9.78738010e-01 ... -9.77277875e-01\n",
      "     9.50088441e-01 -1.51357204...+00]\n",
      "   [ 4.08956736e-01 -2.06575304e-01  1.22084665e+00 ...  3.40539336e-01\n",
      "     6.47998601e-02  8.28751862e-01]]]]),)\n",
      "        self       = <needle.ops.ops_mathematic.Flip object at 0x7fe4d93fcd50>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[[ 1.76405239e+00  4.00157213e-01  9.78738010e-01 ... -9.77277875e-01\n",
      "     9.50088441e-01 -1.51357204...+00]\n",
      "   [ 4.08956736e-01 -2.06575304e-01  1.22084665e+00 ...  3.40539336e-01\n",
      "     6.47998601e-02  8.28751862e-01]]]]),)\n",
      "        op         = <needle.ops.ops_mathematic.Flip object at 0x7fe4d93fcd50>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fe4d93fd2d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fe4d93fd2d0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Flip object at 0x7fe4d93fcd50>\n",
      "a = NDArray([[[[ 1.76405239e+00  4.00157213e-01  9.78738010e-01 ... -9.77277875e-01\n",
      "     9.50088441e-01 -1.51357204e-01]\n",
      " ...08956736e-01 -2.06575304e-01  1.22084665e+00 ...  3.40539336e-01\n",
      "     6.47998601e-02  8.28751862e-01]]]], device=cpu())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "a          = NDArray([[[[ 1.76405239e+00  4.00157213e-01  9.78738010e-01 ... -9.77277875e-01\n",
      "     9.50088441e-01 -1.51357204e-01]\n",
      " ...08956736e-01 -2.06575304e-01  1.22084665e+00 ...  3.40539336e-01\n",
      "     6.47998601e-02  8.28751862e-01]]]], device=cpu())\n",
      "self       = <needle.ops.ops_mathematic.Flip object at 0x7fe4d93fcd50>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:444: NotImplementedError\n",
      "\u001b[31m\u001b[1m____ test_flip_forward[params3-needle.backend_ndarray.ndarray_backend_cuda] ____\u001b[0m\n",
      "\n",
      "params = {'axes': (0, 1), 'shape': (10, 32, 32, 8)}, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, flip_forward_params)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flip_forward\u001b[39;49;00m(params, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        shape, axes = params[\u001b[33m'\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape)\u001b[90m\u001b[39;49;00m\n",
      "        _B = np.flip(_A, axes)\u001b[90m\u001b[39;49;00m\n",
      "        A = ndl.Tensor(_A, device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       B = ndl.flip(A, axes=axes)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "A          = needle.Tensor([[[[ 1.76405239e+00  4.00157213e-01  9.78738010e-01 ... -9.77277875e-01\n",
      "     9.50088441e-01 -1.51357204e...7e+00]\n",
      "   [ 4.08956736e-01 -2.06575304e-01  1.22084665e+00 ...  3.40539336e-01\n",
      "     6.47998601e-02  8.28751862e-01]]]])\n",
      "_A         = array([[[[ 1.76405235e+00,  4.00157208e-01,  9.78737984e-01, ...,\n",
      "          -9.77277880e-01,  9.50088418e-01, -1.51357...4.08956722e-01, -2.06575306e-01,  1.22084664e+00, ...,\n",
      "           3.40539329e-01,  6.47998581e-02,  8.28751864e-01]]]])\n",
      "_B         = array([[[[ 1.67386460e+00,  1.09525980e+00, -4.17631359e-01, ...,\n",
      "           9.36113484e-01,  5.84919249e-01, -8.18787...8.13364259e-01, -1.46642433e+00,  5.21064876e-01, ...,\n",
      "          -3.19328417e-01,  6.91538751e-01,  6.94749144e-01]]]])\n",
      "axes       = (0, 1)\n",
      "device     = cuda()\n",
      "params     = {'axes': (0, 1), 'shape': (10, 32, 32, 8)}\n",
      "shape      = (10, 32, 32, 8)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:123: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:454: in flip\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Flip(axes)(a)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[[ 1.76405239e+00  4.00157213e-01  9.78738010e-01 ... -9.77277875e-01\n",
      "     9.50088441e-01 -1.51357204e...7e+00]\n",
      "   [ 4.08956736e-01 -2.06575304e-01  1.22084665e+00 ...  3.40539336e-01\n",
      "     6.47998601e-02  8.28751862e-01]]]])\n",
      "        axes       = (0, 1)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[ 1.76405239e+00  4.00157213e-01  9.78738010e-01 ... -9.77277875e-01\n",
      "     9.50088441e-01 -1.51357204...+00]\n",
      "   [ 4.08956736e-01 -2.06575304e-01  1.22084665e+00 ...  3.40539336e-01\n",
      "     6.47998601e-02  8.28751862e-01]]]]),)\n",
      "        self       = <needle.ops.ops_mathematic.Flip object at 0x7fe4d957d390>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[[ 1.76405239e+00  4.00157213e-01  9.78738010e-01 ... -9.77277875e-01\n",
      "     9.50088441e-01 -1.51357204...+00]\n",
      "   [ 4.08956736e-01 -2.06575304e-01  1.22084665e+00 ...  3.40539336e-01\n",
      "     6.47998601e-02  8.28751862e-01]]]]),)\n",
      "        op         = <needle.ops.ops_mathematic.Flip object at 0x7fe4d957d390>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fe4d957eb90>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fe4d957eb90>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Flip object at 0x7fe4d957d390>\n",
      "a = NDArray([[[[ 1.76405239e+00  4.00157213e-01  9.78738010e-01 ... -9.77277875e-01\n",
      "     9.50088441e-01 -1.51357204e-01]\n",
      " ...8956736e-01 -2.06575304e-01  1.22084665e+00 ...  3.40539336e-01\n",
      "     6.47998601e-02  8.28751862e-01]]]], device=cuda())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "a          = NDArray([[[[ 1.76405239e+00  4.00157213e-01  9.78738010e-01 ... -9.77277875e-01\n",
      "     9.50088441e-01 -1.51357204e-01]\n",
      " ...8956736e-01 -2.06575304e-01  1.22084665e+00 ...  3.40539336e-01\n",
      "     6.47998601e-02  8.28751862e-01]]]], device=cuda())\n",
      "self       = <needle.ops.ops_mathematic.Flip object at 0x7fe4d957d390>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:444: NotImplementedError\n",
      "\u001b[31m\u001b[1m____ test_flip_forward[params4-needle.backend_ndarray.ndarray_backend_cpu] _____\u001b[0m\n",
      "\n",
      "params = {'axes': (0, 1), 'shape': (3, 3, 6, 8)}, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, flip_forward_params)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flip_forward\u001b[39;49;00m(params, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        shape, axes = params[\u001b[33m'\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape)\u001b[90m\u001b[39;49;00m\n",
      "        _B = np.flip(_A, axes)\u001b[90m\u001b[39;49;00m\n",
      "        A = ndl.Tensor(_A, device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       B = ndl.flip(A, axes=axes)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "A          = needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558\n",
      "    -0.9772779   0.95008844 -0.1513572 ]\n",
      " ....12372191]\n",
      "   [-0.13010696  0.09395323  0.9430461  -2.7396772  -0.56931204\n",
      "     0.26990435 -0.46684554 -1.4169061 ]]]])\n",
      "_A         = array([[[[ 1.76405235,  0.40015721,  0.97873798,  2.2408932 ,\n",
      "           1.86755799, -0.97727788,  0.95008842, -0.1513... [-0.13010695,  0.09395323,  0.94304609, -2.73967717,\n",
      "          -0.56931205,  0.26990435, -0.46684555, -1.41690611]]]])\n",
      "_B         = array([[[[ 0.03863055, -1.6567151 , -0.98551074, -1.47183501,\n",
      "           1.64813493,  0.16422776,  0.56729028, -0.2226... [-1.04855297, -1.42001794, -1.70627019,  1.9507754 ,\n",
      "          -0.50965218, -0.4380743 , -1.25279536,  0.77749036]]]])\n",
      "axes       = (0, 1)\n",
      "device     = cpu()\n",
      "params     = {'axes': (0, 1), 'shape': (3, 3, 6, 8)}\n",
      "shape      = (3, 3, 6, 8)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:123: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:454: in flip\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Flip(axes)(a)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558\n",
      "    -0.9772779   0.95008844 -0.1513572 ]\n",
      " ....12372191]\n",
      "   [-0.13010696  0.09395323  0.9430461  -2.7396772  -0.56931204\n",
      "     0.26990435 -0.46684554 -1.4169061 ]]]])\n",
      "        axes       = (0, 1)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558\n",
      "    -0.9772779   0.95008844 -0.1513572 ]\n",
      "...2372191]\n",
      "   [-0.13010696  0.09395323  0.9430461  -2.7396772  -0.56931204\n",
      "     0.26990435 -0.46684554 -1.4169061 ]]]]),)\n",
      "        self       = <needle.ops.ops_mathematic.Flip object at 0x7fe4d9442150>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558\n",
      "    -0.9772779   0.95008844 -0.1513572 ]\n",
      "...2372191]\n",
      "   [-0.13010696  0.09395323  0.9430461  -2.7396772  -0.56931204\n",
      "     0.26990435 -0.46684554 -1.4169061 ]]]]),)\n",
      "        op         = <needle.ops.ops_mathematic.Flip object at 0x7fe4d9442150>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fe4d94407d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fe4d94407d0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Flip object at 0x7fe4d9442150>\n",
      "a = NDArray([[[[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558\n",
      "    -0.9772779   0.95008844 -0.1513572 ]\n",
      "   [-0....[-0.13010696  0.09395323  0.9430461  -2.7396772  -0.56931204\n",
      "     0.26990435 -0.46684554 -1.4169061 ]]]], device=cpu())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "a          = NDArray([[[[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558\n",
      "    -0.9772779   0.95008844 -0.1513572 ]\n",
      "   [-0....[-0.13010696  0.09395323  0.9430461  -2.7396772  -0.56931204\n",
      "     0.26990435 -0.46684554 -1.4169061 ]]]], device=cpu())\n",
      "self       = <needle.ops.ops_mathematic.Flip object at 0x7fe4d9442150>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:444: NotImplementedError\n",
      "\u001b[31m\u001b[1m____ test_flip_forward[params4-needle.backend_ndarray.ndarray_backend_cuda] ____\u001b[0m\n",
      "\n",
      "params = {'axes': (0, 1), 'shape': (3, 3, 6, 8)}, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, flip_forward_params)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flip_forward\u001b[39;49;00m(params, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        shape, axes = params[\u001b[33m'\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape)\u001b[90m\u001b[39;49;00m\n",
      "        _B = np.flip(_A, axes)\u001b[90m\u001b[39;49;00m\n",
      "        A = ndl.Tensor(_A, device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       B = ndl.flip(A, axes=axes)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "A          = needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558\n",
      "    -0.9772779   0.95008844 -0.1513572 ]\n",
      " ....12372191]\n",
      "   [-0.13010696  0.09395323  0.9430461  -2.7396772  -0.56931204\n",
      "     0.26990435 -0.46684554 -1.4169061 ]]]])\n",
      "_A         = array([[[[ 1.76405235,  0.40015721,  0.97873798,  2.2408932 ,\n",
      "           1.86755799, -0.97727788,  0.95008842, -0.1513... [-0.13010695,  0.09395323,  0.94304609, -2.73967717,\n",
      "          -0.56931205,  0.26990435, -0.46684555, -1.41690611]]]])\n",
      "_B         = array([[[[ 0.03863055, -1.6567151 , -0.98551074, -1.47183501,\n",
      "           1.64813493,  0.16422776,  0.56729028, -0.2226... [-1.04855297, -1.42001794, -1.70627019,  1.9507754 ,\n",
      "          -0.50965218, -0.4380743 , -1.25279536,  0.77749036]]]])\n",
      "axes       = (0, 1)\n",
      "device     = cuda()\n",
      "params     = {'axes': (0, 1), 'shape': (3, 3, 6, 8)}\n",
      "shape      = (3, 3, 6, 8)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:123: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:454: in flip\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Flip(axes)(a)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558\n",
      "    -0.9772779   0.95008844 -0.1513572 ]\n",
      " ....12372191]\n",
      "   [-0.13010696  0.09395323  0.9430461  -2.7396772  -0.56931204\n",
      "     0.26990435 -0.46684554 -1.4169061 ]]]])\n",
      "        axes       = (0, 1)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558\n",
      "    -0.9772779   0.95008844 -0.1513572 ]\n",
      "...2372191]\n",
      "   [-0.13010696  0.09395323  0.9430461  -2.7396772  -0.56931204\n",
      "     0.26990435 -0.46684554 -1.4169061 ]]]]),)\n",
      "        self       = <needle.ops.ops_mathematic.Flip object at 0x7fe2b6be7fd0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558\n",
      "    -0.9772779   0.95008844 -0.1513572 ]\n",
      "...2372191]\n",
      "   [-0.13010696  0.09395323  0.9430461  -2.7396772  -0.56931204\n",
      "     0.26990435 -0.46684554 -1.4169061 ]]]]),)\n",
      "        op         = <needle.ops.ops_mathematic.Flip object at 0x7fe2b6be7fd0>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fe2b6be4b90>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fe2b6be4b90>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Flip object at 0x7fe2b6be7fd0>\n",
      "a = NDArray([[[[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558\n",
      "    -0.9772779   0.95008844 -0.1513572 ]\n",
      "   [-0....-0.13010696  0.09395323  0.9430461  -2.7396772  -0.56931204\n",
      "     0.26990435 -0.46684554 -1.4169061 ]]]], device=cuda())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "a          = NDArray([[[[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558\n",
      "    -0.9772779   0.95008844 -0.1513572 ]\n",
      "   [-0....-0.13010696  0.09395323  0.9430461  -2.7396772  -0.56931204\n",
      "     0.26990435 -0.46684554 -1.4169061 ]]]], device=cuda())\n",
      "self       = <needle.ops.ops_mathematic.Flip object at 0x7fe2b6be7fd0>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:444: NotImplementedError\n",
      "\u001b[31m\u001b[1m____ test_flip_forward[params5-needle.backend_ndarray.ndarray_backend_cpu] _____\u001b[0m\n",
      "\n",
      "params = {'axes': (1, 2), 'shape': (10, 32, 32, 8)}, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, flip_forward_params)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flip_forward\u001b[39;49;00m(params, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        shape, axes = params[\u001b[33m'\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape)\u001b[90m\u001b[39;49;00m\n",
      "        _B = np.flip(_A, axes)\u001b[90m\u001b[39;49;00m\n",
      "        A = ndl.Tensor(_A, device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       B = ndl.flip(A, axes=axes)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "A          = needle.Tensor([[[[ 1.76405239e+00  4.00157213e-01  9.78738010e-01 ... -9.77277875e-01\n",
      "     9.50088441e-01 -1.51357204e...7e+00]\n",
      "   [ 4.08956736e-01 -2.06575304e-01  1.22084665e+00 ...  3.40539336e-01\n",
      "     6.47998601e-02  8.28751862e-01]]]])\n",
      "_A         = array([[[[ 1.76405235e+00,  4.00157208e-01,  9.78737984e-01, ...,\n",
      "          -9.77277880e-01,  9.50088418e-01, -1.51357...4.08956722e-01, -2.06575306e-01,  1.22084664e+00, ...,\n",
      "           3.40539329e-01,  6.47998581e-02,  8.28751864e-01]]]])\n",
      "_B         = array([[[[ 6.85915746e-01,  1.85270697e+00,  1.58169123e+00, ...,\n",
      "           6.66044853e-01,  5.32038766e-01, -1.05080...1.02348043e+00,  8.92468343e-01,  9.79886230e-01, ...,\n",
      "          -1.92371106e+00, -1.58578942e+00, -8.67523255e-02]]]])\n",
      "axes       = (1, 2)\n",
      "device     = cpu()\n",
      "params     = {'axes': (1, 2), 'shape': (10, 32, 32, 8)}\n",
      "shape      = (10, 32, 32, 8)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:123: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:454: in flip\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Flip(axes)(a)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[[ 1.76405239e+00  4.00157213e-01  9.78738010e-01 ... -9.77277875e-01\n",
      "     9.50088441e-01 -1.51357204e...7e+00]\n",
      "   [ 4.08956736e-01 -2.06575304e-01  1.22084665e+00 ...  3.40539336e-01\n",
      "     6.47998601e-02  8.28751862e-01]]]])\n",
      "        axes       = (1, 2)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[ 1.76405239e+00  4.00157213e-01  9.78738010e-01 ... -9.77277875e-01\n",
      "     9.50088441e-01 -1.51357204...+00]\n",
      "   [ 4.08956736e-01 -2.06575304e-01  1.22084665e+00 ...  3.40539336e-01\n",
      "     6.47998601e-02  8.28751862e-01]]]]),)\n",
      "        self       = <needle.ops.ops_mathematic.Flip object at 0x7fe4d93b95d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[[ 1.76405239e+00  4.00157213e-01  9.78738010e-01 ... -9.77277875e-01\n",
      "     9.50088441e-01 -1.51357204...+00]\n",
      "   [ 4.08956736e-01 -2.06575304e-01  1.22084665e+00 ...  3.40539336e-01\n",
      "     6.47998601e-02  8.28751862e-01]]]]),)\n",
      "        op         = <needle.ops.ops_mathematic.Flip object at 0x7fe4d93b95d0>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fe4d93baed0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fe4d93baed0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Flip object at 0x7fe4d93b95d0>\n",
      "a = NDArray([[[[ 1.76405239e+00  4.00157213e-01  9.78738010e-01 ... -9.77277875e-01\n",
      "     9.50088441e-01 -1.51357204e-01]\n",
      " ...08956736e-01 -2.06575304e-01  1.22084665e+00 ...  3.40539336e-01\n",
      "     6.47998601e-02  8.28751862e-01]]]], device=cpu())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "a          = NDArray([[[[ 1.76405239e+00  4.00157213e-01  9.78738010e-01 ... -9.77277875e-01\n",
      "     9.50088441e-01 -1.51357204e-01]\n",
      " ...08956736e-01 -2.06575304e-01  1.22084665e+00 ...  3.40539336e-01\n",
      "     6.47998601e-02  8.28751862e-01]]]], device=cpu())\n",
      "self       = <needle.ops.ops_mathematic.Flip object at 0x7fe4d93b95d0>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:444: NotImplementedError\n",
      "\u001b[31m\u001b[1m____ test_flip_forward[params5-needle.backend_ndarray.ndarray_backend_cuda] ____\u001b[0m\n",
      "\n",
      "params = {'axes': (1, 2), 'shape': (10, 32, 32, 8)}, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, flip_forward_params)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flip_forward\u001b[39;49;00m(params, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        shape, axes = params[\u001b[33m'\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape)\u001b[90m\u001b[39;49;00m\n",
      "        _B = np.flip(_A, axes)\u001b[90m\u001b[39;49;00m\n",
      "        A = ndl.Tensor(_A, device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       B = ndl.flip(A, axes=axes)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "A          = needle.Tensor([[[[ 1.76405239e+00  4.00157213e-01  9.78738010e-01 ... -9.77277875e-01\n",
      "     9.50088441e-01 -1.51357204e...7e+00]\n",
      "   [ 4.08956736e-01 -2.06575304e-01  1.22084665e+00 ...  3.40539336e-01\n",
      "     6.47998601e-02  8.28751862e-01]]]])\n",
      "_A         = array([[[[ 1.76405235e+00,  4.00157208e-01,  9.78737984e-01, ...,\n",
      "          -9.77277880e-01,  9.50088418e-01, -1.51357...4.08956722e-01, -2.06575306e-01,  1.22084664e+00, ...,\n",
      "           3.40539329e-01,  6.47998581e-02,  8.28751864e-01]]]])\n",
      "_B         = array([[[[ 6.85915746e-01,  1.85270697e+00,  1.58169123e+00, ...,\n",
      "           6.66044853e-01,  5.32038766e-01, -1.05080...1.02348043e+00,  8.92468343e-01,  9.79886230e-01, ...,\n",
      "          -1.92371106e+00, -1.58578942e+00, -8.67523255e-02]]]])\n",
      "axes       = (1, 2)\n",
      "device     = cuda()\n",
      "params     = {'axes': (1, 2), 'shape': (10, 32, 32, 8)}\n",
      "shape      = (10, 32, 32, 8)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:123: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:454: in flip\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Flip(axes)(a)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[[ 1.76405239e+00  4.00157213e-01  9.78738010e-01 ... -9.77277875e-01\n",
      "     9.50088441e-01 -1.51357204e...7e+00]\n",
      "   [ 4.08956736e-01 -2.06575304e-01  1.22084665e+00 ...  3.40539336e-01\n",
      "     6.47998601e-02  8.28751862e-01]]]])\n",
      "        axes       = (1, 2)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[ 1.76405239e+00  4.00157213e-01  9.78738010e-01 ... -9.77277875e-01\n",
      "     9.50088441e-01 -1.51357204...+00]\n",
      "   [ 4.08956736e-01 -2.06575304e-01  1.22084665e+00 ...  3.40539336e-01\n",
      "     6.47998601e-02  8.28751862e-01]]]]),)\n",
      "        self       = <needle.ops.ops_mathematic.Flip object at 0x7fe4d9426210>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[[ 1.76405239e+00  4.00157213e-01  9.78738010e-01 ... -9.77277875e-01\n",
      "     9.50088441e-01 -1.51357204...+00]\n",
      "   [ 4.08956736e-01 -2.06575304e-01  1.22084665e+00 ...  3.40539336e-01\n",
      "     6.47998601e-02  8.28751862e-01]]]]),)\n",
      "        op         = <needle.ops.ops_mathematic.Flip object at 0x7fe4d9426210>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fe4d9425b90>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fe4d9425b90>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Flip object at 0x7fe4d9426210>\n",
      "a = NDArray([[[[ 1.76405239e+00  4.00157213e-01  9.78738010e-01 ... -9.77277875e-01\n",
      "     9.50088441e-01 -1.51357204e-01]\n",
      " ...8956736e-01 -2.06575304e-01  1.22084665e+00 ...  3.40539336e-01\n",
      "     6.47998601e-02  8.28751862e-01]]]], device=cuda())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "a          = NDArray([[[[ 1.76405239e+00  4.00157213e-01  9.78738010e-01 ... -9.77277875e-01\n",
      "     9.50088441e-01 -1.51357204e-01]\n",
      " ...8956736e-01 -2.06575304e-01  1.22084665e+00 ...  3.40539336e-01\n",
      "     6.47998601e-02  8.28751862e-01]]]], device=cuda())\n",
      "self       = <needle.ops.ops_mathematic.Flip object at 0x7fe4d9426210>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:444: NotImplementedError\n",
      "\u001b[31m\u001b[1m____ test_flip_forward[params6-needle.backend_ndarray.ndarray_backend_cpu] _____\u001b[0m\n",
      "\n",
      "params = {'axes': (1, 2), 'shape': (3, 3, 6, 8)}, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, flip_forward_params)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flip_forward\u001b[39;49;00m(params, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        shape, axes = params[\u001b[33m'\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape)\u001b[90m\u001b[39;49;00m\n",
      "        _B = np.flip(_A, axes)\u001b[90m\u001b[39;49;00m\n",
      "        A = ndl.Tensor(_A, device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       B = ndl.flip(A, axes=axes)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "A          = needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558\n",
      "    -0.9772779   0.95008844 -0.1513572 ]\n",
      " ....12372191]\n",
      "   [-0.13010696  0.09395323  0.9430461  -2.7396772  -0.56931204\n",
      "     0.26990435 -0.46684554 -1.4169061 ]]]])\n",
      "_A         = array([[[[ 1.76405235,  0.40015721,  0.97873798,  2.2408932 ,\n",
      "           1.86755799, -0.97727788,  0.95008842, -0.1513... [-0.13010695,  0.09395323,  0.94304609, -2.73967717,\n",
      "          -0.56931205,  0.26990435, -0.46684555, -1.41690611]]]])\n",
      "_B         = array([[[[ 0.57659082, -0.20829876,  0.39600671, -1.09306151,\n",
      "          -1.49125759,  0.4393917 ,  0.1666735 ,  0.6350... [ 0.68981816,  1.30184623, -0.62808756, -0.48102712,\n",
      "           2.3039167 , -1.06001582, -0.1359497 ,  1.13689136]]]])\n",
      "axes       = (1, 2)\n",
      "device     = cpu()\n",
      "params     = {'axes': (1, 2), 'shape': (3, 3, 6, 8)}\n",
      "shape      = (3, 3, 6, 8)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:123: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:454: in flip\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Flip(axes)(a)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558\n",
      "    -0.9772779   0.95008844 -0.1513572 ]\n",
      " ....12372191]\n",
      "   [-0.13010696  0.09395323  0.9430461  -2.7396772  -0.56931204\n",
      "     0.26990435 -0.46684554 -1.4169061 ]]]])\n",
      "        axes       = (1, 2)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558\n",
      "    -0.9772779   0.95008844 -0.1513572 ]\n",
      "...2372191]\n",
      "   [-0.13010696  0.09395323  0.9430461  -2.7396772  -0.56931204\n",
      "     0.26990435 -0.46684554 -1.4169061 ]]]]),)\n",
      "        self       = <needle.ops.ops_mathematic.Flip object at 0x7fe4d93c5790>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558\n",
      "    -0.9772779   0.95008844 -0.1513572 ]\n",
      "...2372191]\n",
      "   [-0.13010696  0.09395323  0.9430461  -2.7396772  -0.56931204\n",
      "     0.26990435 -0.46684554 -1.4169061 ]]]]),)\n",
      "        op         = <needle.ops.ops_mathematic.Flip object at 0x7fe4d93c5790>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fe4d93c7390>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fe4d93c7390>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Flip object at 0x7fe4d93c5790>\n",
      "a = NDArray([[[[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558\n",
      "    -0.9772779   0.95008844 -0.1513572 ]\n",
      "   [-0....[-0.13010696  0.09395323  0.9430461  -2.7396772  -0.56931204\n",
      "     0.26990435 -0.46684554 -1.4169061 ]]]], device=cpu())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "a          = NDArray([[[[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558\n",
      "    -0.9772779   0.95008844 -0.1513572 ]\n",
      "   [-0....[-0.13010696  0.09395323  0.9430461  -2.7396772  -0.56931204\n",
      "     0.26990435 -0.46684554 -1.4169061 ]]]], device=cpu())\n",
      "self       = <needle.ops.ops_mathematic.Flip object at 0x7fe4d93c5790>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:444: NotImplementedError\n",
      "\u001b[31m\u001b[1m____ test_flip_forward[params6-needle.backend_ndarray.ndarray_backend_cuda] ____\u001b[0m\n",
      "\n",
      "params = {'axes': (1, 2), 'shape': (3, 3, 6, 8)}, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, flip_forward_params)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flip_forward\u001b[39;49;00m(params, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        shape, axes = params[\u001b[33m'\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape)\u001b[90m\u001b[39;49;00m\n",
      "        _B = np.flip(_A, axes)\u001b[90m\u001b[39;49;00m\n",
      "        A = ndl.Tensor(_A, device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       B = ndl.flip(A, axes=axes)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "A          = needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558\n",
      "    -0.9772779   0.95008844 -0.1513572 ]\n",
      " ....12372191]\n",
      "   [-0.13010696  0.09395323  0.9430461  -2.7396772  -0.56931204\n",
      "     0.26990435 -0.46684554 -1.4169061 ]]]])\n",
      "_A         = array([[[[ 1.76405235,  0.40015721,  0.97873798,  2.2408932 ,\n",
      "           1.86755799, -0.97727788,  0.95008842, -0.1513... [-0.13010695,  0.09395323,  0.94304609, -2.73967717,\n",
      "          -0.56931205,  0.26990435, -0.46684555, -1.41690611]]]])\n",
      "_B         = array([[[[ 0.57659082, -0.20829876,  0.39600671, -1.09306151,\n",
      "          -1.49125759,  0.4393917 ,  0.1666735 ,  0.6350... [ 0.68981816,  1.30184623, -0.62808756, -0.48102712,\n",
      "           2.3039167 , -1.06001582, -0.1359497 ,  1.13689136]]]])\n",
      "axes       = (1, 2)\n",
      "device     = cuda()\n",
      "params     = {'axes': (1, 2), 'shape': (3, 3, 6, 8)}\n",
      "shape      = (3, 3, 6, 8)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:123: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:454: in flip\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Flip(axes)(a)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558\n",
      "    -0.9772779   0.95008844 -0.1513572 ]\n",
      " ....12372191]\n",
      "   [-0.13010696  0.09395323  0.9430461  -2.7396772  -0.56931204\n",
      "     0.26990435 -0.46684554 -1.4169061 ]]]])\n",
      "        axes       = (1, 2)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558\n",
      "    -0.9772779   0.95008844 -0.1513572 ]\n",
      "...2372191]\n",
      "   [-0.13010696  0.09395323  0.9430461  -2.7396772  -0.56931204\n",
      "     0.26990435 -0.46684554 -1.4169061 ]]]]),)\n",
      "        self       = <needle.ops.ops_mathematic.Flip object at 0x7fe4d93dfc50>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558\n",
      "    -0.9772779   0.95008844 -0.1513572 ]\n",
      "...2372191]\n",
      "   [-0.13010696  0.09395323  0.9430461  -2.7396772  -0.56931204\n",
      "     0.26990435 -0.46684554 -1.4169061 ]]]]),)\n",
      "        op         = <needle.ops.ops_mathematic.Flip object at 0x7fe4d93dfc50>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fe4d93dfd10>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fe4d93dfd10>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Flip object at 0x7fe4d93dfc50>\n",
      "a = NDArray([[[[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558\n",
      "    -0.9772779   0.95008844 -0.1513572 ]\n",
      "   [-0....-0.13010696  0.09395323  0.9430461  -2.7396772  -0.56931204\n",
      "     0.26990435 -0.46684554 -1.4169061 ]]]], device=cuda())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "a          = NDArray([[[[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558\n",
      "    -0.9772779   0.95008844 -0.1513572 ]\n",
      "   [-0....-0.13010696  0.09395323  0.9430461  -2.7396772  -0.56931204\n",
      "     0.26990435 -0.46684554 -1.4169061 ]]]], device=cuda())\n",
      "self       = <needle.ops.ops_mathematic.Flip object at 0x7fe4d93dfc50>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:444: NotImplementedError\n",
      "\u001b[31m\u001b[1m____ test_flip_forward[params7-needle.backend_ndarray.ndarray_backend_cpu] _____\u001b[0m\n",
      "\n",
      "params = {'axes': (2, 3), 'shape': (10, 32, 32, 8)}, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, flip_forward_params)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flip_forward\u001b[39;49;00m(params, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        shape, axes = params[\u001b[33m'\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape)\u001b[90m\u001b[39;49;00m\n",
      "        _B = np.flip(_A, axes)\u001b[90m\u001b[39;49;00m\n",
      "        A = ndl.Tensor(_A, device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       B = ndl.flip(A, axes=axes)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "A          = needle.Tensor([[[[ 1.76405239e+00  4.00157213e-01  9.78738010e-01 ... -9.77277875e-01\n",
      "     9.50088441e-01 -1.51357204e...7e+00]\n",
      "   [ 4.08956736e-01 -2.06575304e-01  1.22084665e+00 ...  3.40539336e-01\n",
      "     6.47998601e-02  8.28751862e-01]]]])\n",
      "_A         = array([[[[ 1.76405235e+00,  4.00157208e-01,  9.78737984e-01, ...,\n",
      "          -9.77277880e-01,  9.50088418e-01, -1.51357...4.08956722e-01, -2.06575306e-01,  1.22084664e+00, ...,\n",
      "           3.40539329e-01,  6.47998581e-02,  8.28751864e-01]]]])\n",
      "_B         = array([[[[ 6.94749144e-01,  6.91538751e-01, -3.19328417e-01, ...,\n",
      "           5.21064876e-01, -1.46642433e+00, -8.13364...8.18787218e-01,  5.84919249e-01,  9.36113484e-01, ...,\n",
      "          -4.17631359e-01,  1.09525980e+00,  1.67386460e+00]]]])\n",
      "axes       = (2, 3)\n",
      "device     = cpu()\n",
      "params     = {'axes': (2, 3), 'shape': (10, 32, 32, 8)}\n",
      "shape      = (10, 32, 32, 8)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:123: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:454: in flip\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Flip(axes)(a)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[[ 1.76405239e+00  4.00157213e-01  9.78738010e-01 ... -9.77277875e-01\n",
      "     9.50088441e-01 -1.51357204e...7e+00]\n",
      "   [ 4.08956736e-01 -2.06575304e-01  1.22084665e+00 ...  3.40539336e-01\n",
      "     6.47998601e-02  8.28751862e-01]]]])\n",
      "        axes       = (2, 3)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[ 1.76405239e+00  4.00157213e-01  9.78738010e-01 ... -9.77277875e-01\n",
      "     9.50088441e-01 -1.51357204...+00]\n",
      "   [ 4.08956736e-01 -2.06575304e-01  1.22084665e+00 ...  3.40539336e-01\n",
      "     6.47998601e-02  8.28751862e-01]]]]),)\n",
      "        self       = <needle.ops.ops_mathematic.Flip object at 0x7fe4d92e68d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[[ 1.76405239e+00  4.00157213e-01  9.78738010e-01 ... -9.77277875e-01\n",
      "     9.50088441e-01 -1.51357204...+00]\n",
      "   [ 4.08956736e-01 -2.06575304e-01  1.22084665e+00 ...  3.40539336e-01\n",
      "     6.47998601e-02  8.28751862e-01]]]]),)\n",
      "        op         = <needle.ops.ops_mathematic.Flip object at 0x7fe4d92e68d0>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fe4d92e4310>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fe4d92e4310>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Flip object at 0x7fe4d92e68d0>\n",
      "a = NDArray([[[[ 1.76405239e+00  4.00157213e-01  9.78738010e-01 ... -9.77277875e-01\n",
      "     9.50088441e-01 -1.51357204e-01]\n",
      " ...08956736e-01 -2.06575304e-01  1.22084665e+00 ...  3.40539336e-01\n",
      "     6.47998601e-02  8.28751862e-01]]]], device=cpu())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "a          = NDArray([[[[ 1.76405239e+00  4.00157213e-01  9.78738010e-01 ... -9.77277875e-01\n",
      "     9.50088441e-01 -1.51357204e-01]\n",
      " ...08956736e-01 -2.06575304e-01  1.22084665e+00 ...  3.40539336e-01\n",
      "     6.47998601e-02  8.28751862e-01]]]], device=cpu())\n",
      "self       = <needle.ops.ops_mathematic.Flip object at 0x7fe4d92e68d0>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:444: NotImplementedError\n",
      "\u001b[31m\u001b[1m____ test_flip_forward[params7-needle.backend_ndarray.ndarray_backend_cuda] ____\u001b[0m\n",
      "\n",
      "params = {'axes': (2, 3), 'shape': (10, 32, 32, 8)}, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, flip_forward_params)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flip_forward\u001b[39;49;00m(params, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        shape, axes = params[\u001b[33m'\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape)\u001b[90m\u001b[39;49;00m\n",
      "        _B = np.flip(_A, axes)\u001b[90m\u001b[39;49;00m\n",
      "        A = ndl.Tensor(_A, device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       B = ndl.flip(A, axes=axes)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "A          = needle.Tensor([[[[ 1.76405239e+00  4.00157213e-01  9.78738010e-01 ... -9.77277875e-01\n",
      "     9.50088441e-01 -1.51357204e...7e+00]\n",
      "   [ 4.08956736e-01 -2.06575304e-01  1.22084665e+00 ...  3.40539336e-01\n",
      "     6.47998601e-02  8.28751862e-01]]]])\n",
      "_A         = array([[[[ 1.76405235e+00,  4.00157208e-01,  9.78737984e-01, ...,\n",
      "          -9.77277880e-01,  9.50088418e-01, -1.51357...4.08956722e-01, -2.06575306e-01,  1.22084664e+00, ...,\n",
      "           3.40539329e-01,  6.47998581e-02,  8.28751864e-01]]]])\n",
      "_B         = array([[[[ 6.94749144e-01,  6.91538751e-01, -3.19328417e-01, ...,\n",
      "           5.21064876e-01, -1.46642433e+00, -8.13364...8.18787218e-01,  5.84919249e-01,  9.36113484e-01, ...,\n",
      "          -4.17631359e-01,  1.09525980e+00,  1.67386460e+00]]]])\n",
      "axes       = (2, 3)\n",
      "device     = cuda()\n",
      "params     = {'axes': (2, 3), 'shape': (10, 32, 32, 8)}\n",
      "shape      = (10, 32, 32, 8)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:123: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:454: in flip\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Flip(axes)(a)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[[ 1.76405239e+00  4.00157213e-01  9.78738010e-01 ... -9.77277875e-01\n",
      "     9.50088441e-01 -1.51357204e...7e+00]\n",
      "   [ 4.08956736e-01 -2.06575304e-01  1.22084665e+00 ...  3.40539336e-01\n",
      "     6.47998601e-02  8.28751862e-01]]]])\n",
      "        axes       = (2, 3)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[ 1.76405239e+00  4.00157213e-01  9.78738010e-01 ... -9.77277875e-01\n",
      "     9.50088441e-01 -1.51357204...+00]\n",
      "   [ 4.08956736e-01 -2.06575304e-01  1.22084665e+00 ...  3.40539336e-01\n",
      "     6.47998601e-02  8.28751862e-01]]]]),)\n",
      "        self       = <needle.ops.ops_mathematic.Flip object at 0x7fe4d9636310>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[[ 1.76405239e+00  4.00157213e-01  9.78738010e-01 ... -9.77277875e-01\n",
      "     9.50088441e-01 -1.51357204...+00]\n",
      "   [ 4.08956736e-01 -2.06575304e-01  1.22084665e+00 ...  3.40539336e-01\n",
      "     6.47998601e-02  8.28751862e-01]]]]),)\n",
      "        op         = <needle.ops.ops_mathematic.Flip object at 0x7fe4d9636310>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fe4d9635110>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fe4d9635110>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Flip object at 0x7fe4d9636310>\n",
      "a = NDArray([[[[ 1.76405239e+00  4.00157213e-01  9.78738010e-01 ... -9.77277875e-01\n",
      "     9.50088441e-01 -1.51357204e-01]\n",
      " ...8956736e-01 -2.06575304e-01  1.22084665e+00 ...  3.40539336e-01\n",
      "     6.47998601e-02  8.28751862e-01]]]], device=cuda())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "a          = NDArray([[[[ 1.76405239e+00  4.00157213e-01  9.78738010e-01 ... -9.77277875e-01\n",
      "     9.50088441e-01 -1.51357204e-01]\n",
      " ...8956736e-01 -2.06575304e-01  1.22084665e+00 ...  3.40539336e-01\n",
      "     6.47998601e-02  8.28751862e-01]]]], device=cuda())\n",
      "self       = <needle.ops.ops_mathematic.Flip object at 0x7fe4d9636310>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:444: NotImplementedError\n",
      "\u001b[31m\u001b[1m____ test_flip_forward[params8-needle.backend_ndarray.ndarray_backend_cpu] _____\u001b[0m\n",
      "\n",
      "params = {'axes': (2, 3), 'shape': (3, 3, 6, 8)}, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, flip_forward_params)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flip_forward\u001b[39;49;00m(params, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        shape, axes = params[\u001b[33m'\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape)\u001b[90m\u001b[39;49;00m\n",
      "        _B = np.flip(_A, axes)\u001b[90m\u001b[39;49;00m\n",
      "        A = ndl.Tensor(_A, device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       B = ndl.flip(A, axes=axes)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "A          = needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558\n",
      "    -0.9772779   0.95008844 -0.1513572 ]\n",
      " ....12372191]\n",
      "   [-0.13010696  0.09395323  0.9430461  -2.7396772  -0.56931204\n",
      "     0.26990435 -0.46684554 -1.4169061 ]]]])\n",
      "_A         = array([[[[ 1.76405235,  0.40015721,  0.97873798,  2.2408932 ,\n",
      "           1.86755799, -0.97727788,  0.95008842, -0.1513... [-0.13010695,  0.09395323,  0.94304609, -2.73967717,\n",
      "          -0.56931205,  0.26990435, -0.46684555, -1.41690611]]]])\n",
      "_B         = array([[[[ 0.77749036, -1.25279536, -0.4380743 , -0.50965218,\n",
      "           1.9507754 , -1.70627019, -1.42001794, -1.0485... [-0.2226751 ,  0.56729028,  0.16422776,  1.64813493,\n",
      "          -1.47183501, -0.98551074, -1.6567151 ,  0.03863055]]]])\n",
      "axes       = (2, 3)\n",
      "device     = cpu()\n",
      "params     = {'axes': (2, 3), 'shape': (3, 3, 6, 8)}\n",
      "shape      = (3, 3, 6, 8)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:123: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:454: in flip\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Flip(axes)(a)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558\n",
      "    -0.9772779   0.95008844 -0.1513572 ]\n",
      " ....12372191]\n",
      "   [-0.13010696  0.09395323  0.9430461  -2.7396772  -0.56931204\n",
      "     0.26990435 -0.46684554 -1.4169061 ]]]])\n",
      "        axes       = (2, 3)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558\n",
      "    -0.9772779   0.95008844 -0.1513572 ]\n",
      "...2372191]\n",
      "   [-0.13010696  0.09395323  0.9430461  -2.7396772  -0.56931204\n",
      "     0.26990435 -0.46684554 -1.4169061 ]]]]),)\n",
      "        self       = <needle.ops.ops_mathematic.Flip object at 0x7fe2b6c01c90>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558\n",
      "    -0.9772779   0.95008844 -0.1513572 ]\n",
      "...2372191]\n",
      "   [-0.13010696  0.09395323  0.9430461  -2.7396772  -0.56931204\n",
      "     0.26990435 -0.46684554 -1.4169061 ]]]]),)\n",
      "        op         = <needle.ops.ops_mathematic.Flip object at 0x7fe2b6c01c90>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fe2b6c03910>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fe2b6c03910>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Flip object at 0x7fe2b6c01c90>\n",
      "a = NDArray([[[[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558\n",
      "    -0.9772779   0.95008844 -0.1513572 ]\n",
      "   [-0....[-0.13010696  0.09395323  0.9430461  -2.7396772  -0.56931204\n",
      "     0.26990435 -0.46684554 -1.4169061 ]]]], device=cpu())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "a          = NDArray([[[[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558\n",
      "    -0.9772779   0.95008844 -0.1513572 ]\n",
      "   [-0....[-0.13010696  0.09395323  0.9430461  -2.7396772  -0.56931204\n",
      "     0.26990435 -0.46684554 -1.4169061 ]]]], device=cpu())\n",
      "self       = <needle.ops.ops_mathematic.Flip object at 0x7fe2b6c01c90>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:444: NotImplementedError\n",
      "\u001b[31m\u001b[1m____ test_flip_forward[params8-needle.backend_ndarray.ndarray_backend_cuda] ____\u001b[0m\n",
      "\n",
      "params = {'axes': (2, 3), 'shape': (3, 3, 6, 8)}, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, flip_forward_params)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flip_forward\u001b[39;49;00m(params, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        shape, axes = params[\u001b[33m'\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape)\u001b[90m\u001b[39;49;00m\n",
      "        _B = np.flip(_A, axes)\u001b[90m\u001b[39;49;00m\n",
      "        A = ndl.Tensor(_A, device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       B = ndl.flip(A, axes=axes)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "A          = needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558\n",
      "    -0.9772779   0.95008844 -0.1513572 ]\n",
      " ....12372191]\n",
      "   [-0.13010696  0.09395323  0.9430461  -2.7396772  -0.56931204\n",
      "     0.26990435 -0.46684554 -1.4169061 ]]]])\n",
      "_A         = array([[[[ 1.76405235,  0.40015721,  0.97873798,  2.2408932 ,\n",
      "           1.86755799, -0.97727788,  0.95008842, -0.1513... [-0.13010695,  0.09395323,  0.94304609, -2.73967717,\n",
      "          -0.56931205,  0.26990435, -0.46684555, -1.41690611]]]])\n",
      "_B         = array([[[[ 0.77749036, -1.25279536, -0.4380743 , -0.50965218,\n",
      "           1.9507754 , -1.70627019, -1.42001794, -1.0485... [-0.2226751 ,  0.56729028,  0.16422776,  1.64813493,\n",
      "          -1.47183501, -0.98551074, -1.6567151 ,  0.03863055]]]])\n",
      "axes       = (2, 3)\n",
      "device     = cuda()\n",
      "params     = {'axes': (2, 3), 'shape': (3, 3, 6, 8)}\n",
      "shape      = (3, 3, 6, 8)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:123: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:454: in flip\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Flip(axes)(a)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558\n",
      "    -0.9772779   0.95008844 -0.1513572 ]\n",
      " ....12372191]\n",
      "   [-0.13010696  0.09395323  0.9430461  -2.7396772  -0.56931204\n",
      "     0.26990435 -0.46684554 -1.4169061 ]]]])\n",
      "        axes       = (2, 3)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558\n",
      "    -0.9772779   0.95008844 -0.1513572 ]\n",
      "...2372191]\n",
      "   [-0.13010696  0.09395323  0.9430461  -2.7396772  -0.56931204\n",
      "     0.26990435 -0.46684554 -1.4169061 ]]]]),)\n",
      "        self       = <needle.ops.ops_mathematic.Flip object at 0x7fe4d93f7050>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558\n",
      "    -0.9772779   0.95008844 -0.1513572 ]\n",
      "...2372191]\n",
      "   [-0.13010696  0.09395323  0.9430461  -2.7396772  -0.56931204\n",
      "     0.26990435 -0.46684554 -1.4169061 ]]]]),)\n",
      "        op         = <needle.ops.ops_mathematic.Flip object at 0x7fe4d93f7050>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fe4d93f7990>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fe4d93f7990>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Flip object at 0x7fe4d93f7050>\n",
      "a = NDArray([[[[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558\n",
      "    -0.9772779   0.95008844 -0.1513572 ]\n",
      "   [-0....-0.13010696  0.09395323  0.9430461  -2.7396772  -0.56931204\n",
      "     0.26990435 -0.46684554 -1.4169061 ]]]], device=cuda())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "a          = NDArray([[[[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558\n",
      "    -0.9772779   0.95008844 -0.1513572 ]\n",
      "   [-0....-0.13010696  0.09395323  0.9430461  -2.7396772  -0.56931204\n",
      "     0.26990435 -0.46684554 -1.4169061 ]]]], device=cuda())\n",
      "self       = <needle.ops.ops_mathematic.Flip object at 0x7fe4d93f7050>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:444: NotImplementedError\n",
      "\u001b[31m\u001b[1m____ test_flip_forward[params9-needle.backend_ndarray.ndarray_backend_cpu] _____\u001b[0m\n",
      "\n",
      "params = {'axes': (0, 1, 2, 3), 'shape': (10, 32, 32, 8)}, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, flip_forward_params)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flip_forward\u001b[39;49;00m(params, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        shape, axes = params[\u001b[33m'\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape)\u001b[90m\u001b[39;49;00m\n",
      "        _B = np.flip(_A, axes)\u001b[90m\u001b[39;49;00m\n",
      "        A = ndl.Tensor(_A, device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       B = ndl.flip(A, axes=axes)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "A          = needle.Tensor([[[[ 1.76405239e+00  4.00157213e-01  9.78738010e-01 ... -9.77277875e-01\n",
      "     9.50088441e-01 -1.51357204e...7e+00]\n",
      "   [ 4.08956736e-01 -2.06575304e-01  1.22084665e+00 ...  3.40539336e-01\n",
      "     6.47998601e-02  8.28751862e-01]]]])\n",
      "_A         = array([[[[ 1.76405235e+00,  4.00157208e-01,  9.78737984e-01, ...,\n",
      "          -9.77277880e-01,  9.50088418e-01, -1.51357...4.08956722e-01, -2.06575306e-01,  1.22084664e+00, ...,\n",
      "           3.40539329e-01,  6.47998581e-02,  8.28751864e-01]]]])\n",
      "_B         = array([[[[ 8.28751864e-01,  6.47998581e-02,  3.40539329e-01, ...,\n",
      "           1.22084664e+00, -2.06575306e-01,  4.08956...1.51357208e-01,  9.50088418e-01, -9.77277880e-01, ...,\n",
      "           9.78737984e-01,  4.00157208e-01,  1.76405235e+00]]]])\n",
      "axes       = (0, 1, 2, 3)\n",
      "device     = cpu()\n",
      "params     = {'axes': (0, 1, 2, 3), 'shape': (10, 32, 32, 8)}\n",
      "shape      = (10, 32, 32, 8)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:123: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:454: in flip\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Flip(axes)(a)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[[ 1.76405239e+00  4.00157213e-01  9.78738010e-01 ... -9.77277875e-01\n",
      "     9.50088441e-01 -1.51357204e...7e+00]\n",
      "   [ 4.08956736e-01 -2.06575304e-01  1.22084665e+00 ...  3.40539336e-01\n",
      "     6.47998601e-02  8.28751862e-01]]]])\n",
      "        axes       = (0, 1, 2, 3)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[ 1.76405239e+00  4.00157213e-01  9.78738010e-01 ... -9.77277875e-01\n",
      "     9.50088441e-01 -1.51357204...+00]\n",
      "   [ 4.08956736e-01 -2.06575304e-01  1.22084665e+00 ...  3.40539336e-01\n",
      "     6.47998601e-02  8.28751862e-01]]]]),)\n",
      "        self       = <needle.ops.ops_mathematic.Flip object at 0x7fe4d9426e10>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[[ 1.76405239e+00  4.00157213e-01  9.78738010e-01 ... -9.77277875e-01\n",
      "     9.50088441e-01 -1.51357204...+00]\n",
      "   [ 4.08956736e-01 -2.06575304e-01  1.22084665e+00 ...  3.40539336e-01\n",
      "     6.47998601e-02  8.28751862e-01]]]]),)\n",
      "        op         = <needle.ops.ops_mathematic.Flip object at 0x7fe4d9426e10>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fe4d9426850>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fe4d9426850>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Flip object at 0x7fe4d9426e10>\n",
      "a = NDArray([[[[ 1.76405239e+00  4.00157213e-01  9.78738010e-01 ... -9.77277875e-01\n",
      "     9.50088441e-01 -1.51357204e-01]\n",
      " ...08956736e-01 -2.06575304e-01  1.22084665e+00 ...  3.40539336e-01\n",
      "     6.47998601e-02  8.28751862e-01]]]], device=cpu())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "a          = NDArray([[[[ 1.76405239e+00  4.00157213e-01  9.78738010e-01 ... -9.77277875e-01\n",
      "     9.50088441e-01 -1.51357204e-01]\n",
      " ...08956736e-01 -2.06575304e-01  1.22084665e+00 ...  3.40539336e-01\n",
      "     6.47998601e-02  8.28751862e-01]]]], device=cpu())\n",
      "self       = <needle.ops.ops_mathematic.Flip object at 0x7fe4d9426e10>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:444: NotImplementedError\n",
      "\u001b[31m\u001b[1m____ test_flip_forward[params9-needle.backend_ndarray.ndarray_backend_cuda] ____\u001b[0m\n",
      "\n",
      "params = {'axes': (0, 1, 2, 3), 'shape': (10, 32, 32, 8)}, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, flip_forward_params)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flip_forward\u001b[39;49;00m(params, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        shape, axes = params[\u001b[33m'\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape)\u001b[90m\u001b[39;49;00m\n",
      "        _B = np.flip(_A, axes)\u001b[90m\u001b[39;49;00m\n",
      "        A = ndl.Tensor(_A, device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       B = ndl.flip(A, axes=axes)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "A          = needle.Tensor([[[[ 1.76405239e+00  4.00157213e-01  9.78738010e-01 ... -9.77277875e-01\n",
      "     9.50088441e-01 -1.51357204e...7e+00]\n",
      "   [ 4.08956736e-01 -2.06575304e-01  1.22084665e+00 ...  3.40539336e-01\n",
      "     6.47998601e-02  8.28751862e-01]]]])\n",
      "_A         = array([[[[ 1.76405235e+00,  4.00157208e-01,  9.78737984e-01, ...,\n",
      "          -9.77277880e-01,  9.50088418e-01, -1.51357...4.08956722e-01, -2.06575306e-01,  1.22084664e+00, ...,\n",
      "           3.40539329e-01,  6.47998581e-02,  8.28751864e-01]]]])\n",
      "_B         = array([[[[ 8.28751864e-01,  6.47998581e-02,  3.40539329e-01, ...,\n",
      "           1.22084664e+00, -2.06575306e-01,  4.08956...1.51357208e-01,  9.50088418e-01, -9.77277880e-01, ...,\n",
      "           9.78737984e-01,  4.00157208e-01,  1.76405235e+00]]]])\n",
      "axes       = (0, 1, 2, 3)\n",
      "device     = cuda()\n",
      "params     = {'axes': (0, 1, 2, 3), 'shape': (10, 32, 32, 8)}\n",
      "shape      = (10, 32, 32, 8)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:123: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:454: in flip\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Flip(axes)(a)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[[ 1.76405239e+00  4.00157213e-01  9.78738010e-01 ... -9.77277875e-01\n",
      "     9.50088441e-01 -1.51357204e...7e+00]\n",
      "   [ 4.08956736e-01 -2.06575304e-01  1.22084665e+00 ...  3.40539336e-01\n",
      "     6.47998601e-02  8.28751862e-01]]]])\n",
      "        axes       = (0, 1, 2, 3)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[ 1.76405239e+00  4.00157213e-01  9.78738010e-01 ... -9.77277875e-01\n",
      "     9.50088441e-01 -1.51357204...+00]\n",
      "   [ 4.08956736e-01 -2.06575304e-01  1.22084665e+00 ...  3.40539336e-01\n",
      "     6.47998601e-02  8.28751862e-01]]]]),)\n",
      "        self       = <needle.ops.ops_mathematic.Flip object at 0x7fe4d94ed210>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[[ 1.76405239e+00  4.00157213e-01  9.78738010e-01 ... -9.77277875e-01\n",
      "     9.50088441e-01 -1.51357204...+00]\n",
      "   [ 4.08956736e-01 -2.06575304e-01  1.22084665e+00 ...  3.40539336e-01\n",
      "     6.47998601e-02  8.28751862e-01]]]]),)\n",
      "        op         = <needle.ops.ops_mathematic.Flip object at 0x7fe4d94ed210>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fe4d94ec9d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fe4d94ec9d0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Flip object at 0x7fe4d94ed210>\n",
      "a = NDArray([[[[ 1.76405239e+00  4.00157213e-01  9.78738010e-01 ... -9.77277875e-01\n",
      "     9.50088441e-01 -1.51357204e-01]\n",
      " ...8956736e-01 -2.06575304e-01  1.22084665e+00 ...  3.40539336e-01\n",
      "     6.47998601e-02  8.28751862e-01]]]], device=cuda())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "a          = NDArray([[[[ 1.76405239e+00  4.00157213e-01  9.78738010e-01 ... -9.77277875e-01\n",
      "     9.50088441e-01 -1.51357204e-01]\n",
      " ...8956736e-01 -2.06575304e-01  1.22084665e+00 ...  3.40539336e-01\n",
      "     6.47998601e-02  8.28751862e-01]]]], device=cuda())\n",
      "self       = <needle.ops.ops_mathematic.Flip object at 0x7fe4d94ed210>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:444: NotImplementedError\n",
      "\u001b[31m\u001b[1m____ test_flip_backward[params0-needle.backend_ndarray.ndarray_backend_cpu] ____\u001b[0m\n",
      "\n",
      "params = {'axes': (0,), 'shape': (10, 5)}, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, flip_backward_params)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flip_backward\u001b[39;49;00m(params, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        shape, axes = params[\u001b[33m'\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      ">       backward_check(ndl.flip, ndl.Tensor(np.random.randn(*shape), device=device), axes=axes)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "axes       = (0,)\n",
      "device     = cpu()\n",
      "params     = {'axes': (0,), 'shape': (10, 5)}\n",
      "shape      = (10, 5)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:145: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:16: in backward_check\n",
      "    \u001b[0mout = f(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0...3   -1.420018   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]]),)\n",
      "        eps        = 0.001\n",
      "        f          = <function flip at 0x7fe5968b4e00>\n",
      "        kwargs     = {'axes': (0,)}\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:454: in flip\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Flip(axes)(a)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0....553   -1.420018   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]])\n",
      "        axes       = (0,)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0...3   -1.420018   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]]),)\n",
      "        self       = <needle.ops.ops_mathematic.Flip object at 0x7fe4d9539650>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0...3   -1.420018   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]]),)\n",
      "        op         = <needle.ops.ops_mathematic.Flip object at 0x7fe4d9539650>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fe4d9538f50>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fe4d9538f50>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Flip object at 0x7fe4d9539650>\n",
      "a = NDArray([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0.103218...8   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]], device=cpu())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "a          = NDArray([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0.103218...8   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]], device=cpu())\n",
      "self       = <needle.ops.ops_mathematic.Flip object at 0x7fe4d9539650>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:444: NotImplementedError\n",
      "\u001b[31m\u001b[1m___ test_flip_backward[params0-needle.backend_ndarray.ndarray_backend_cuda] ____\u001b[0m\n",
      "\n",
      "params = {'axes': (0,), 'shape': (10, 5)}, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, flip_backward_params)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flip_backward\u001b[39;49;00m(params, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        shape, axes = params[\u001b[33m'\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      ">       backward_check(ndl.flip, ndl.Tensor(np.random.randn(*shape), device=device), axes=axes)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "axes       = (0,)\n",
      "device     = cuda()\n",
      "params     = {'axes': (0,), 'shape': (10, 5)}\n",
      "shape      = (10, 5)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:145: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:16: in backward_check\n",
      "    \u001b[0mout = f(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0...3   -1.420018   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]]),)\n",
      "        eps        = 0.001\n",
      "        f          = <function flip at 0x7fe5968b4e00>\n",
      "        kwargs     = {'axes': (0,)}\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:454: in flip\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Flip(axes)(a)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0....553   -1.420018   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]])\n",
      "        axes       = (0,)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0...3   -1.420018   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]]),)\n",
      "        self       = <needle.ops.ops_mathematic.Flip object at 0x7fe2b6c08d90>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0...3   -1.420018   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]]),)\n",
      "        op         = <needle.ops.ops_mathematic.Flip object at 0x7fe2b6c08d90>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fe2b6c09c50>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fe2b6c09c50>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Flip object at 0x7fe2b6c08d90>\n",
      "a = NDArray([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0.103218...   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]], device=cuda())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "a          = NDArray([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0.103218...   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]], device=cuda())\n",
      "self       = <needle.ops.ops_mathematic.Flip object at 0x7fe2b6c08d90>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:444: NotImplementedError\n",
      "\u001b[31m\u001b[1m____ test_flip_backward[params1-needle.backend_ndarray.ndarray_backend_cpu] ____\u001b[0m\n",
      "\n",
      "params = {'axes': (1,), 'shape': (10, 5)}, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, flip_backward_params)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flip_backward\u001b[39;49;00m(params, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        shape, axes = params[\u001b[33m'\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      ">       backward_check(ndl.flip, ndl.Tensor(np.random.randn(*shape), device=device), axes=axes)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "axes       = (1,)\n",
      "device     = cpu()\n",
      "params     = {'axes': (1,), 'shape': (10, 5)}\n",
      "shape      = (10, 5)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:145: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:16: in backward_check\n",
      "    \u001b[0mout = f(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0...3   -1.420018   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]]),)\n",
      "        eps        = 0.001\n",
      "        f          = <function flip at 0x7fe5968b4e00>\n",
      "        kwargs     = {'axes': (1,)}\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:454: in flip\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Flip(axes)(a)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0....553   -1.420018   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]])\n",
      "        axes       = (1,)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0...3   -1.420018   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]]),)\n",
      "        self       = <needle.ops.ops_mathematic.Flip object at 0x7fe4d92f31d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0...3   -1.420018   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]]),)\n",
      "        op         = <needle.ops.ops_mathematic.Flip object at 0x7fe4d92f31d0>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fe4d92f0bd0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fe4d92f0bd0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Flip object at 0x7fe4d92f31d0>\n",
      "a = NDArray([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0.103218...8   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]], device=cpu())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "a          = NDArray([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0.103218...8   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]], device=cpu())\n",
      "self       = <needle.ops.ops_mathematic.Flip object at 0x7fe4d92f31d0>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:444: NotImplementedError\n",
      "\u001b[31m\u001b[1m___ test_flip_backward[params1-needle.backend_ndarray.ndarray_backend_cuda] ____\u001b[0m\n",
      "\n",
      "params = {'axes': (1,), 'shape': (10, 5)}, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, flip_backward_params)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flip_backward\u001b[39;49;00m(params, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        shape, axes = params[\u001b[33m'\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      ">       backward_check(ndl.flip, ndl.Tensor(np.random.randn(*shape), device=device), axes=axes)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "axes       = (1,)\n",
      "device     = cuda()\n",
      "params     = {'axes': (1,), 'shape': (10, 5)}\n",
      "shape      = (10, 5)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:145: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:16: in backward_check\n",
      "    \u001b[0mout = f(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0...3   -1.420018   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]]),)\n",
      "        eps        = 0.001\n",
      "        f          = <function flip at 0x7fe5968b4e00>\n",
      "        kwargs     = {'axes': (1,)}\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:454: in flip\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Flip(axes)(a)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0....553   -1.420018   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]])\n",
      "        axes       = (1,)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0...3   -1.420018   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]]),)\n",
      "        self       = <needle.ops.ops_mathematic.Flip object at 0x7fe4d93df4d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0...3   -1.420018   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]]),)\n",
      "        op         = <needle.ops.ops_mathematic.Flip object at 0x7fe4d93df4d0>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fe4d93dc490>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fe4d93dc490>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Flip object at 0x7fe4d93df4d0>\n",
      "a = NDArray([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0.103218...   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]], device=cuda())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "a          = NDArray([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0.103218...   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]], device=cuda())\n",
      "self       = <needle.ops.ops_mathematic.Flip object at 0x7fe4d93df4d0>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:444: NotImplementedError\n",
      "\u001b[31m\u001b[1m____ test_flip_backward[params2-needle.backend_ndarray.ndarray_backend_cpu] ____\u001b[0m\n",
      "\n",
      "params = {'axes': (0, 1), 'shape': (10, 5)}, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, flip_backward_params)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flip_backward\u001b[39;49;00m(params, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        shape, axes = params[\u001b[33m'\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      ">       backward_check(ndl.flip, ndl.Tensor(np.random.randn(*shape), device=device), axes=axes)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "axes       = (0, 1)\n",
      "device     = cpu()\n",
      "params     = {'axes': (0, 1), 'shape': (10, 5)}\n",
      "shape      = (10, 5)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:145: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:16: in backward_check\n",
      "    \u001b[0mout = f(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0...3   -1.420018   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]]),)\n",
      "        eps        = 0.001\n",
      "        f          = <function flip at 0x7fe5968b4e00>\n",
      "        kwargs     = {'axes': (0, 1)}\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:454: in flip\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Flip(axes)(a)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0....553   -1.420018   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]])\n",
      "        axes       = (0, 1)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0...3   -1.420018   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]]),)\n",
      "        self       = <needle.ops.ops_mathematic.Flip object at 0x7fe4d94eedd0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0...3   -1.420018   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]]),)\n",
      "        op         = <needle.ops.ops_mathematic.Flip object at 0x7fe4d94eedd0>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fe4d94ef410>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fe4d94ef410>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Flip object at 0x7fe4d94eedd0>\n",
      "a = NDArray([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0.103218...8   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]], device=cpu())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "a          = NDArray([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0.103218...8   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]], device=cpu())\n",
      "self       = <needle.ops.ops_mathematic.Flip object at 0x7fe4d94eedd0>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:444: NotImplementedError\n",
      "\u001b[31m\u001b[1m___ test_flip_backward[params2-needle.backend_ndarray.ndarray_backend_cuda] ____\u001b[0m\n",
      "\n",
      "params = {'axes': (0, 1), 'shape': (10, 5)}, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, flip_backward_params)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flip_backward\u001b[39;49;00m(params, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        shape, axes = params[\u001b[33m'\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      ">       backward_check(ndl.flip, ndl.Tensor(np.random.randn(*shape), device=device), axes=axes)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "axes       = (0, 1)\n",
      "device     = cuda()\n",
      "params     = {'axes': (0, 1), 'shape': (10, 5)}\n",
      "shape      = (10, 5)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:145: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:16: in backward_check\n",
      "    \u001b[0mout = f(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0...3   -1.420018   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]]),)\n",
      "        eps        = 0.001\n",
      "        f          = <function flip at 0x7fe5968b4e00>\n",
      "        kwargs     = {'axes': (0, 1)}\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:454: in flip\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Flip(axes)(a)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0....553   -1.420018   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]])\n",
      "        axes       = (0, 1)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0...3   -1.420018   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]]),)\n",
      "        self       = <needle.ops.ops_mathematic.Flip object at 0x7fe4d93a7010>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0...3   -1.420018   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]]),)\n",
      "        op         = <needle.ops.ops_mathematic.Flip object at 0x7fe4d93a7010>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fe4d93a78d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fe4d93a78d0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Flip object at 0x7fe4d93a7010>\n",
      "a = NDArray([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0.103218...   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]], device=cuda())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "a          = NDArray([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0.103218...   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]], device=cuda())\n",
      "self       = <needle.ops.ops_mathematic.Flip object at 0x7fe4d93a7010>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:444: NotImplementedError\n",
      "\u001b[31m\u001b[1m____ test_flip_backward[params3-needle.backend_ndarray.ndarray_backend_cpu] ____\u001b[0m\n",
      "\n",
      "params = {'axes': (0, 1), 'shape': (2, 3, 3, 8)}, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, flip_backward_params)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flip_backward\u001b[39;49;00m(params, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        shape, axes = params[\u001b[33m'\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      ">       backward_check(ndl.flip, ndl.Tensor(np.random.randn(*shape), device=device), axes=axes)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "axes       = (0, 1)\n",
      "device     = cpu()\n",
      "params     = {'axes': (0, 1), 'shape': (2, 3, 3, 8)}\n",
      "shape      = (2, 3, 3, 8)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:145: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:16: in backward_check\n",
      "    \u001b[0mout = f(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558\n",
      "    -0.9772779   0.95008844 -0.1513572 ]\n",
      "...67643327]\n",
      "   [ 0.57659084 -0.20829876  0.3960067  -1.0930616  -1.4912575\n",
      "     0.4393917   0.1666735   0.63503146]]]]),)\n",
      "        eps        = 0.001\n",
      "        f          = <function flip at 0x7fe5968b4e00>\n",
      "        kwargs     = {'axes': (0, 1)}\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:454: in flip\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Flip(axes)(a)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558\n",
      "    -0.9772779   0.95008844 -0.1513572 ]\n",
      " ...0.67643327]\n",
      "   [ 0.57659084 -0.20829876  0.3960067  -1.0930616  -1.4912575\n",
      "     0.4393917   0.1666735   0.63503146]]]])\n",
      "        axes       = (0, 1)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558\n",
      "    -0.9772779   0.95008844 -0.1513572 ]\n",
      "...67643327]\n",
      "   [ 0.57659084 -0.20829876  0.3960067  -1.0930616  -1.4912575\n",
      "     0.4393917   0.1666735   0.63503146]]]]),)\n",
      "        self       = <needle.ops.ops_mathematic.Flip object at 0x7fe4d9388250>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558\n",
      "    -0.9772779   0.95008844 -0.1513572 ]\n",
      "...67643327]\n",
      "   [ 0.57659084 -0.20829876  0.3960067  -1.0930616  -1.4912575\n",
      "     0.4393917   0.1666735   0.63503146]]]]),)\n",
      "        op         = <needle.ops.ops_mathematic.Flip object at 0x7fe4d9388250>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fe4d93894d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fe4d93894d0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Flip object at 0x7fe4d9388250>\n",
      "a = NDArray([[[[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558\n",
      "    -0.9772779   0.95008844 -0.1513572 ]\n",
      "   [-0.... [ 0.57659084 -0.20829876  0.3960067  -1.0930616  -1.4912575\n",
      "     0.4393917   0.1666735   0.63503146]]]], device=cpu())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "a          = NDArray([[[[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558\n",
      "    -0.9772779   0.95008844 -0.1513572 ]\n",
      "   [-0.... [ 0.57659084 -0.20829876  0.3960067  -1.0930616  -1.4912575\n",
      "     0.4393917   0.1666735   0.63503146]]]], device=cpu())\n",
      "self       = <needle.ops.ops_mathematic.Flip object at 0x7fe4d9388250>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:444: NotImplementedError\n",
      "\u001b[31m\u001b[1m___ test_flip_backward[params3-needle.backend_ndarray.ndarray_backend_cuda] ____\u001b[0m\n",
      "\n",
      "params = {'axes': (0, 1), 'shape': (2, 3, 3, 8)}, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, flip_backward_params)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flip_backward\u001b[39;49;00m(params, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        shape, axes = params[\u001b[33m'\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      ">       backward_check(ndl.flip, ndl.Tensor(np.random.randn(*shape), device=device), axes=axes)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "axes       = (0, 1)\n",
      "device     = cuda()\n",
      "params     = {'axes': (0, 1), 'shape': (2, 3, 3, 8)}\n",
      "shape      = (2, 3, 3, 8)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:145: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:16: in backward_check\n",
      "    \u001b[0mout = f(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558\n",
      "    -0.9772779   0.95008844 -0.1513572 ]\n",
      "...67643327]\n",
      "   [ 0.57659084 -0.20829876  0.3960067  -1.0930616  -1.4912575\n",
      "     0.4393917   0.1666735   0.63503146]]]]),)\n",
      "        eps        = 0.001\n",
      "        f          = <function flip at 0x7fe5968b4e00>\n",
      "        kwargs     = {'axes': (0, 1)}\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:454: in flip\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Flip(axes)(a)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558\n",
      "    -0.9772779   0.95008844 -0.1513572 ]\n",
      " ...0.67643327]\n",
      "   [ 0.57659084 -0.20829876  0.3960067  -1.0930616  -1.4912575\n",
      "     0.4393917   0.1666735   0.63503146]]]])\n",
      "        axes       = (0, 1)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558\n",
      "    -0.9772779   0.95008844 -0.1513572 ]\n",
      "...67643327]\n",
      "   [ 0.57659084 -0.20829876  0.3960067  -1.0930616  -1.4912575\n",
      "     0.4393917   0.1666735   0.63503146]]]]),)\n",
      "        self       = <needle.ops.ops_mathematic.Flip object at 0x7fe4d94edf90>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558\n",
      "    -0.9772779   0.95008844 -0.1513572 ]\n",
      "...67643327]\n",
      "   [ 0.57659084 -0.20829876  0.3960067  -1.0930616  -1.4912575\n",
      "     0.4393917   0.1666735   0.63503146]]]]),)\n",
      "        op         = <needle.ops.ops_mathematic.Flip object at 0x7fe4d94edf90>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fe4d94ef110>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fe4d94ef110>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Flip object at 0x7fe4d94edf90>\n",
      "a = NDArray([[[[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558\n",
      "    -0.9772779   0.95008844 -0.1513572 ]\n",
      "   [-0....[ 0.57659084 -0.20829876  0.3960067  -1.0930616  -1.4912575\n",
      "     0.4393917   0.1666735   0.63503146]]]], device=cuda())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "a          = NDArray([[[[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558\n",
      "    -0.9772779   0.95008844 -0.1513572 ]\n",
      "   [-0....[ 0.57659084 -0.20829876  0.3960067  -1.0930616  -1.4912575\n",
      "     0.4393917   0.1666735   0.63503146]]]], device=cuda())\n",
      "self       = <needle.ops.ops_mathematic.Flip object at 0x7fe4d94edf90>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:444: NotImplementedError\n",
      "\u001b[31m\u001b[1m____ test_flip_backward[params4-needle.backend_ndarray.ndarray_backend_cpu] ____\u001b[0m\n",
      "\n",
      "params = {'axes': (0, 1), 'shape': (3, 3, 6, 4)}, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, flip_backward_params)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flip_backward\u001b[39;49;00m(params, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        shape, axes = params[\u001b[33m'\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      ">       backward_check(ndl.flip, ndl.Tensor(np.random.randn(*shape), device=device), axes=axes)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "axes       = (0, 1)\n",
      "device     = cpu()\n",
      "params     = {'axes': (0, 1), 'shape': (3, 3, 6, 4)}\n",
      "shape      = (3, 3, 6, 4)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:145: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:16: in backward_check\n",
      "    \u001b[0mout = f(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572...80309 ]\n",
      "   [ 0.2799246  -0.09815039  0.9101789   0.3172182 ]\n",
      "   [ 0.78632796 -0.4664191  -0.94444627 -0.4100497 ]]]]),)\n",
      "        eps        = 0.001\n",
      "        f          = <function flip at 0x7fe5968b4e00>\n",
      "        kwargs     = {'axes': (0, 1)}\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:454: in flip\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Flip(axes)(a)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572 ...7380309 ]\n",
      "   [ 0.2799246  -0.09815039  0.9101789   0.3172182 ]\n",
      "   [ 0.78632796 -0.4664191  -0.94444627 -0.4100497 ]]]])\n",
      "        axes       = (0, 1)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572...80309 ]\n",
      "   [ 0.2799246  -0.09815039  0.9101789   0.3172182 ]\n",
      "   [ 0.78632796 -0.4664191  -0.94444627 -0.4100497 ]]]]),)\n",
      "        self       = <needle.ops.ops_mathematic.Flip object at 0x7fe4d93c7810>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572...80309 ]\n",
      "   [ 0.2799246  -0.09815039  0.9101789   0.3172182 ]\n",
      "   [ 0.78632796 -0.4664191  -0.94444627 -0.4100497 ]]]]),)\n",
      "        op         = <needle.ops.ops_mathematic.Flip object at 0x7fe4d93c7810>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fe4d93c4e90>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fe4d93c4e90>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Flip object at 0x7fe4d93c7810>\n",
      "a = NDArray([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572 ]\n",
      "   [... 0.2799246  -0.09815039  0.9101789   0.3172182 ]\n",
      "   [ 0.78632796 -0.4664191  -0.94444627 -0.4100497 ]]]], device=cpu())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "a          = NDArray([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572 ]\n",
      "   [... 0.2799246  -0.09815039  0.9101789   0.3172182 ]\n",
      "   [ 0.78632796 -0.4664191  -0.94444627 -0.4100497 ]]]], device=cpu())\n",
      "self       = <needle.ops.ops_mathematic.Flip object at 0x7fe4d93c7810>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:444: NotImplementedError\n",
      "\u001b[31m\u001b[1m___ test_flip_backward[params4-needle.backend_ndarray.ndarray_backend_cuda] ____\u001b[0m\n",
      "\n",
      "params = {'axes': (0, 1), 'shape': (3, 3, 6, 4)}, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, flip_backward_params)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flip_backward\u001b[39;49;00m(params, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        shape, axes = params[\u001b[33m'\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      ">       backward_check(ndl.flip, ndl.Tensor(np.random.randn(*shape), device=device), axes=axes)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "axes       = (0, 1)\n",
      "device     = cuda()\n",
      "params     = {'axes': (0, 1), 'shape': (3, 3, 6, 4)}\n",
      "shape      = (3, 3, 6, 4)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:145: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:16: in backward_check\n",
      "    \u001b[0mout = f(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572...80309 ]\n",
      "   [ 0.2799246  -0.09815039  0.9101789   0.3172182 ]\n",
      "   [ 0.78632796 -0.4664191  -0.94444627 -0.4100497 ]]]]),)\n",
      "        eps        = 0.001\n",
      "        f          = <function flip at 0x7fe5968b4e00>\n",
      "        kwargs     = {'axes': (0, 1)}\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:454: in flip\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Flip(axes)(a)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572 ...7380309 ]\n",
      "   [ 0.2799246  -0.09815039  0.9101789   0.3172182 ]\n",
      "   [ 0.78632796 -0.4664191  -0.94444627 -0.4100497 ]]]])\n",
      "        axes       = (0, 1)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572...80309 ]\n",
      "   [ 0.2799246  -0.09815039  0.9101789   0.3172182 ]\n",
      "   [ 0.78632796 -0.4664191  -0.94444627 -0.4100497 ]]]]),)\n",
      "        self       = <needle.ops.ops_mathematic.Flip object at 0x7fe4d92e4650>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572...80309 ]\n",
      "   [ 0.2799246  -0.09815039  0.9101789   0.3172182 ]\n",
      "   [ 0.78632796 -0.4664191  -0.94444627 -0.4100497 ]]]]),)\n",
      "        op         = <needle.ops.ops_mathematic.Flip object at 0x7fe4d92e4650>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fe4d92e6d50>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fe4d92e6d50>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Flip object at 0x7fe4d92e4650>\n",
      "a = NDArray([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572 ]\n",
      "   [...0.2799246  -0.09815039  0.9101789   0.3172182 ]\n",
      "   [ 0.78632796 -0.4664191  -0.94444627 -0.4100497 ]]]], device=cuda())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "a          = NDArray([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572 ]\n",
      "   [...0.2799246  -0.09815039  0.9101789   0.3172182 ]\n",
      "   [ 0.78632796 -0.4664191  -0.94444627 -0.4100497 ]]]], device=cuda())\n",
      "self       = <needle.ops.ops_mathematic.Flip object at 0x7fe4d92e4650>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:444: NotImplementedError\n",
      "\u001b[31m\u001b[1m____ test_flip_backward[params5-needle.backend_ndarray.ndarray_backend_cpu] ____\u001b[0m\n",
      "\n",
      "params = {'axes': (1, 2), 'shape': (2, 3, 3, 4)}, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, flip_backward_params)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flip_backward\u001b[39;49;00m(params, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        shape, axes = params[\u001b[33m'\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      ">       backward_check(ndl.flip, ndl.Tensor(np.random.randn(*shape), device=device), axes=axes)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "axes       = (1, 2)\n",
      "device     = cpu()\n",
      "params     = {'axes': (1, 2), 'shape': (2, 3, 3, 4)}\n",
      "shape      = (2, 3, 3, 4)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:145: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:16: in backward_check\n",
      "    \u001b[0mout = f(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572...62826 ]\n",
      "   [ 0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]]),)\n",
      "        eps        = 0.001\n",
      "        f          = <function flip at 0x7fe5968b4e00>\n",
      "        kwargs     = {'axes': (1, 2)}\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:454: in flip\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Flip(axes)(a)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572 ...7262826 ]\n",
      "   [ 0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]])\n",
      "        axes       = (1, 2)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572...62826 ]\n",
      "   [ 0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]]),)\n",
      "        self       = <needle.ops.ops_mathematic.Flip object at 0x7fe4d95091d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572...62826 ]\n",
      "   [ 0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]]),)\n",
      "        op         = <needle.ops.ops_mathematic.Flip object at 0x7fe4d95091d0>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fe4d950ba50>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fe4d950ba50>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Flip object at 0x7fe4d95091d0>\n",
      "a = NDArray([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572 ]\n",
      "   [... 0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]], device=cpu())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "a          = NDArray([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572 ]\n",
      "   [... 0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]], device=cpu())\n",
      "self       = <needle.ops.ops_mathematic.Flip object at 0x7fe4d95091d0>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:444: NotImplementedError\n",
      "\u001b[31m\u001b[1m___ test_flip_backward[params5-needle.backend_ndarray.ndarray_backend_cuda] ____\u001b[0m\n",
      "\n",
      "params = {'axes': (1, 2), 'shape': (2, 3, 3, 4)}, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, flip_backward_params)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flip_backward\u001b[39;49;00m(params, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        shape, axes = params[\u001b[33m'\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      ">       backward_check(ndl.flip, ndl.Tensor(np.random.randn(*shape), device=device), axes=axes)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "axes       = (1, 2)\n",
      "device     = cuda()\n",
      "params     = {'axes': (1, 2), 'shape': (2, 3, 3, 4)}\n",
      "shape      = (2, 3, 3, 4)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:145: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:16: in backward_check\n",
      "    \u001b[0mout = f(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572...62826 ]\n",
      "   [ 0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]]),)\n",
      "        eps        = 0.001\n",
      "        f          = <function flip at 0x7fe5968b4e00>\n",
      "        kwargs     = {'axes': (1, 2)}\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:454: in flip\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Flip(axes)(a)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572 ...7262826 ]\n",
      "   [ 0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]])\n",
      "        axes       = (1, 2)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572...62826 ]\n",
      "   [ 0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]]),)\n",
      "        self       = <needle.ops.ops_mathematic.Flip object at 0x7fe4d9455210>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572...62826 ]\n",
      "   [ 0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]]),)\n",
      "        op         = <needle.ops.ops_mathematic.Flip object at 0x7fe4d9455210>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fe4d9456490>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fe4d9456490>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Flip object at 0x7fe4d9455210>\n",
      "a = NDArray([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572 ]\n",
      "   [...0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]], device=cuda())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "a          = NDArray([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572 ]\n",
      "   [...0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]], device=cuda())\n",
      "self       = <needle.ops.ops_mathematic.Flip object at 0x7fe4d9455210>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:444: NotImplementedError\n",
      "\u001b[31m\u001b[1m____ test_flip_backward[params6-needle.backend_ndarray.ndarray_backend_cpu] ____\u001b[0m\n",
      "\n",
      "params = {'axes': (1, 2), 'shape': (3, 3, 6, 4)}, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, flip_backward_params)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flip_backward\u001b[39;49;00m(params, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        shape, axes = params[\u001b[33m'\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      ">       backward_check(ndl.flip, ndl.Tensor(np.random.randn(*shape), device=device), axes=axes)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "axes       = (1, 2)\n",
      "device     = cpu()\n",
      "params     = {'axes': (1, 2), 'shape': (3, 3, 6, 4)}\n",
      "shape      = (3, 3, 6, 4)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:145: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:16: in backward_check\n",
      "    \u001b[0mout = f(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572...80309 ]\n",
      "   [ 0.2799246  -0.09815039  0.9101789   0.3172182 ]\n",
      "   [ 0.78632796 -0.4664191  -0.94444627 -0.4100497 ]]]]),)\n",
      "        eps        = 0.001\n",
      "        f          = <function flip at 0x7fe5968b4e00>\n",
      "        kwargs     = {'axes': (1, 2)}\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:454: in flip\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Flip(axes)(a)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572 ...7380309 ]\n",
      "   [ 0.2799246  -0.09815039  0.9101789   0.3172182 ]\n",
      "   [ 0.78632796 -0.4664191  -0.94444627 -0.4100497 ]]]])\n",
      "        axes       = (1, 2)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572...80309 ]\n",
      "   [ 0.2799246  -0.09815039  0.9101789   0.3172182 ]\n",
      "   [ 0.78632796 -0.4664191  -0.94444627 -0.4100497 ]]]]),)\n",
      "        self       = <needle.ops.ops_mathematic.Flip object at 0x7fe2b6c03050>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572...80309 ]\n",
      "   [ 0.2799246  -0.09815039  0.9101789   0.3172182 ]\n",
      "   [ 0.78632796 -0.4664191  -0.94444627 -0.4100497 ]]]]),)\n",
      "        op         = <needle.ops.ops_mathematic.Flip object at 0x7fe2b6c03050>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fe2b6c00e10>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fe2b6c00e10>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Flip object at 0x7fe2b6c03050>\n",
      "a = NDArray([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572 ]\n",
      "   [... 0.2799246  -0.09815039  0.9101789   0.3172182 ]\n",
      "   [ 0.78632796 -0.4664191  -0.94444627 -0.4100497 ]]]], device=cpu())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "a          = NDArray([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572 ]\n",
      "   [... 0.2799246  -0.09815039  0.9101789   0.3172182 ]\n",
      "   [ 0.78632796 -0.4664191  -0.94444627 -0.4100497 ]]]], device=cpu())\n",
      "self       = <needle.ops.ops_mathematic.Flip object at 0x7fe2b6c03050>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:444: NotImplementedError\n",
      "\u001b[31m\u001b[1m___ test_flip_backward[params6-needle.backend_ndarray.ndarray_backend_cuda] ____\u001b[0m\n",
      "\n",
      "params = {'axes': (1, 2), 'shape': (3, 3, 6, 4)}, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, flip_backward_params)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flip_backward\u001b[39;49;00m(params, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        shape, axes = params[\u001b[33m'\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      ">       backward_check(ndl.flip, ndl.Tensor(np.random.randn(*shape), device=device), axes=axes)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "axes       = (1, 2)\n",
      "device     = cuda()\n",
      "params     = {'axes': (1, 2), 'shape': (3, 3, 6, 4)}\n",
      "shape      = (3, 3, 6, 4)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:145: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:16: in backward_check\n",
      "    \u001b[0mout = f(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572...80309 ]\n",
      "   [ 0.2799246  -0.09815039  0.9101789   0.3172182 ]\n",
      "   [ 0.78632796 -0.4664191  -0.94444627 -0.4100497 ]]]]),)\n",
      "        eps        = 0.001\n",
      "        f          = <function flip at 0x7fe5968b4e00>\n",
      "        kwargs     = {'axes': (1, 2)}\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:454: in flip\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Flip(axes)(a)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572 ...7380309 ]\n",
      "   [ 0.2799246  -0.09815039  0.9101789   0.3172182 ]\n",
      "   [ 0.78632796 -0.4664191  -0.94444627 -0.4100497 ]]]])\n",
      "        axes       = (1, 2)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572...80309 ]\n",
      "   [ 0.2799246  -0.09815039  0.9101789   0.3172182 ]\n",
      "   [ 0.78632796 -0.4664191  -0.94444627 -0.4100497 ]]]]),)\n",
      "        self       = <needle.ops.ops_mathematic.Flip object at 0x7fe4d93c52d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572...80309 ]\n",
      "   [ 0.2799246  -0.09815039  0.9101789   0.3172182 ]\n",
      "   [ 0.78632796 -0.4664191  -0.94444627 -0.4100497 ]]]]),)\n",
      "        op         = <needle.ops.ops_mathematic.Flip object at 0x7fe4d93c52d0>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fe4d93c4d50>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fe4d93c4d50>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Flip object at 0x7fe4d93c52d0>\n",
      "a = NDArray([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572 ]\n",
      "   [...0.2799246  -0.09815039  0.9101789   0.3172182 ]\n",
      "   [ 0.78632796 -0.4664191  -0.94444627 -0.4100497 ]]]], device=cuda())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "a          = NDArray([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572 ]\n",
      "   [...0.2799246  -0.09815039  0.9101789   0.3172182 ]\n",
      "   [ 0.78632796 -0.4664191  -0.94444627 -0.4100497 ]]]], device=cuda())\n",
      "self       = <needle.ops.ops_mathematic.Flip object at 0x7fe4d93c52d0>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:444: NotImplementedError\n",
      "\u001b[31m\u001b[1m____ test_flip_backward[params7-needle.backend_ndarray.ndarray_backend_cpu] ____\u001b[0m\n",
      "\n",
      "params = {'axes': (2, 3), 'shape': (2, 3, 3, 4)}, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, flip_backward_params)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flip_backward\u001b[39;49;00m(params, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        shape, axes = params[\u001b[33m'\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      ">       backward_check(ndl.flip, ndl.Tensor(np.random.randn(*shape), device=device), axes=axes)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "axes       = (2, 3)\n",
      "device     = cpu()\n",
      "params     = {'axes': (2, 3), 'shape': (2, 3, 3, 4)}\n",
      "shape      = (2, 3, 3, 4)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:145: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:16: in backward_check\n",
      "    \u001b[0mout = f(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572...62826 ]\n",
      "   [ 0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]]),)\n",
      "        eps        = 0.001\n",
      "        f          = <function flip at 0x7fe5968b4e00>\n",
      "        kwargs     = {'axes': (2, 3)}\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:454: in flip\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Flip(axes)(a)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572 ...7262826 ]\n",
      "   [ 0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]])\n",
      "        axes       = (2, 3)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572...62826 ]\n",
      "   [ 0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]]),)\n",
      "        self       = <needle.ops.ops_mathematic.Flip object at 0x7fe2b6bfc4d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572...62826 ]\n",
      "   [ 0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]]),)\n",
      "        op         = <needle.ops.ops_mathematic.Flip object at 0x7fe2b6bfc4d0>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fe2b6bfc590>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fe2b6bfc590>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Flip object at 0x7fe2b6bfc4d0>\n",
      "a = NDArray([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572 ]\n",
      "   [... 0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]], device=cpu())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "a          = NDArray([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572 ]\n",
      "   [... 0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]], device=cpu())\n",
      "self       = <needle.ops.ops_mathematic.Flip object at 0x7fe2b6bfc4d0>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:444: NotImplementedError\n",
      "\u001b[31m\u001b[1m___ test_flip_backward[params7-needle.backend_ndarray.ndarray_backend_cuda] ____\u001b[0m\n",
      "\n",
      "params = {'axes': (2, 3), 'shape': (2, 3, 3, 4)}, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, flip_backward_params)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flip_backward\u001b[39;49;00m(params, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        shape, axes = params[\u001b[33m'\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      ">       backward_check(ndl.flip, ndl.Tensor(np.random.randn(*shape), device=device), axes=axes)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "axes       = (2, 3)\n",
      "device     = cuda()\n",
      "params     = {'axes': (2, 3), 'shape': (2, 3, 3, 4)}\n",
      "shape      = (2, 3, 3, 4)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:145: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:16: in backward_check\n",
      "    \u001b[0mout = f(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572...62826 ]\n",
      "   [ 0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]]),)\n",
      "        eps        = 0.001\n",
      "        f          = <function flip at 0x7fe5968b4e00>\n",
      "        kwargs     = {'axes': (2, 3)}\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:454: in flip\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Flip(axes)(a)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572 ...7262826 ]\n",
      "   [ 0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]])\n",
      "        axes       = (2, 3)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572...62826 ]\n",
      "   [ 0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]]),)\n",
      "        self       = <needle.ops.ops_mathematic.Flip object at 0x7fe4d9491650>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572...62826 ]\n",
      "   [ 0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]]),)\n",
      "        op         = <needle.ops.ops_mathematic.Flip object at 0x7fe4d9491650>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fe4d9491990>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fe4d9491990>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Flip object at 0x7fe4d9491650>\n",
      "a = NDArray([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572 ]\n",
      "   [...0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]], device=cuda())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "a          = NDArray([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572 ]\n",
      "   [...0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]], device=cuda())\n",
      "self       = <needle.ops.ops_mathematic.Flip object at 0x7fe4d9491650>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:444: NotImplementedError\n",
      "\u001b[31m\u001b[1m____ test_flip_backward[params8-needle.backend_ndarray.ndarray_backend_cpu] ____\u001b[0m\n",
      "\n",
      "params = {'axes': (2, 3), 'shape': (3, 3, 6, 4)}, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, flip_backward_params)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flip_backward\u001b[39;49;00m(params, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        shape, axes = params[\u001b[33m'\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      ">       backward_check(ndl.flip, ndl.Tensor(np.random.randn(*shape), device=device), axes=axes)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "axes       = (2, 3)\n",
      "device     = cpu()\n",
      "params     = {'axes': (2, 3), 'shape': (3, 3, 6, 4)}\n",
      "shape      = (3, 3, 6, 4)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:145: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:16: in backward_check\n",
      "    \u001b[0mout = f(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572...80309 ]\n",
      "   [ 0.2799246  -0.09815039  0.9101789   0.3172182 ]\n",
      "   [ 0.78632796 -0.4664191  -0.94444627 -0.4100497 ]]]]),)\n",
      "        eps        = 0.001\n",
      "        f          = <function flip at 0x7fe5968b4e00>\n",
      "        kwargs     = {'axes': (2, 3)}\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:454: in flip\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Flip(axes)(a)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572 ...7380309 ]\n",
      "   [ 0.2799246  -0.09815039  0.9101789   0.3172182 ]\n",
      "   [ 0.78632796 -0.4664191  -0.94444627 -0.4100497 ]]]])\n",
      "        axes       = (2, 3)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572...80309 ]\n",
      "   [ 0.2799246  -0.09815039  0.9101789   0.3172182 ]\n",
      "   [ 0.78632796 -0.4664191  -0.94444627 -0.4100497 ]]]]),)\n",
      "        self       = <needle.ops.ops_mathematic.Flip object at 0x7fe4d9363fd0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572...80309 ]\n",
      "   [ 0.2799246  -0.09815039  0.9101789   0.3172182 ]\n",
      "   [ 0.78632796 -0.4664191  -0.94444627 -0.4100497 ]]]]),)\n",
      "        op         = <needle.ops.ops_mathematic.Flip object at 0x7fe4d9363fd0>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fe4d9360b10>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fe4d9360b10>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Flip object at 0x7fe4d9363fd0>\n",
      "a = NDArray([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572 ]\n",
      "   [... 0.2799246  -0.09815039  0.9101789   0.3172182 ]\n",
      "   [ 0.78632796 -0.4664191  -0.94444627 -0.4100497 ]]]], device=cpu())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "a          = NDArray([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572 ]\n",
      "   [... 0.2799246  -0.09815039  0.9101789   0.3172182 ]\n",
      "   [ 0.78632796 -0.4664191  -0.94444627 -0.4100497 ]]]], device=cpu())\n",
      "self       = <needle.ops.ops_mathematic.Flip object at 0x7fe4d9363fd0>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:444: NotImplementedError\n",
      "\u001b[31m\u001b[1m___ test_flip_backward[params8-needle.backend_ndarray.ndarray_backend_cuda] ____\u001b[0m\n",
      "\n",
      "params = {'axes': (2, 3), 'shape': (3, 3, 6, 4)}, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, flip_backward_params)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flip_backward\u001b[39;49;00m(params, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        shape, axes = params[\u001b[33m'\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      ">       backward_check(ndl.flip, ndl.Tensor(np.random.randn(*shape), device=device), axes=axes)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "axes       = (2, 3)\n",
      "device     = cuda()\n",
      "params     = {'axes': (2, 3), 'shape': (3, 3, 6, 4)}\n",
      "shape      = (3, 3, 6, 4)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:145: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:16: in backward_check\n",
      "    \u001b[0mout = f(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572...80309 ]\n",
      "   [ 0.2799246  -0.09815039  0.9101789   0.3172182 ]\n",
      "   [ 0.78632796 -0.4664191  -0.94444627 -0.4100497 ]]]]),)\n",
      "        eps        = 0.001\n",
      "        f          = <function flip at 0x7fe5968b4e00>\n",
      "        kwargs     = {'axes': (2, 3)}\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:454: in flip\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Flip(axes)(a)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572 ...7380309 ]\n",
      "   [ 0.2799246  -0.09815039  0.9101789   0.3172182 ]\n",
      "   [ 0.78632796 -0.4664191  -0.94444627 -0.4100497 ]]]])\n",
      "        axes       = (2, 3)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572...80309 ]\n",
      "   [ 0.2799246  -0.09815039  0.9101789   0.3172182 ]\n",
      "   [ 0.78632796 -0.4664191  -0.94444627 -0.4100497 ]]]]),)\n",
      "        self       = <needle.ops.ops_mathematic.Flip object at 0x7fe4d9516a50>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572...80309 ]\n",
      "   [ 0.2799246  -0.09815039  0.9101789   0.3172182 ]\n",
      "   [ 0.78632796 -0.4664191  -0.94444627 -0.4100497 ]]]]),)\n",
      "        op         = <needle.ops.ops_mathematic.Flip object at 0x7fe4d9516a50>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fe4d9517250>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fe4d9517250>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Flip object at 0x7fe4d9516a50>\n",
      "a = NDArray([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572 ]\n",
      "   [...0.2799246  -0.09815039  0.9101789   0.3172182 ]\n",
      "   [ 0.78632796 -0.4664191  -0.94444627 -0.4100497 ]]]], device=cuda())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "a          = NDArray([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572 ]\n",
      "   [...0.2799246  -0.09815039  0.9101789   0.3172182 ]\n",
      "   [ 0.78632796 -0.4664191  -0.94444627 -0.4100497 ]]]], device=cuda())\n",
      "self       = <needle.ops.ops_mathematic.Flip object at 0x7fe4d9516a50>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:444: NotImplementedError\n",
      "\u001b[31m\u001b[1m____ test_flip_backward[params9-needle.backend_ndarray.ndarray_backend_cpu] ____\u001b[0m\n",
      "\n",
      "params = {'axes': (0, 1, 2, 3), 'shape': (2, 3, 3, 4)}, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, flip_backward_params)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flip_backward\u001b[39;49;00m(params, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        shape, axes = params[\u001b[33m'\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      ">       backward_check(ndl.flip, ndl.Tensor(np.random.randn(*shape), device=device), axes=axes)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "axes       = (0, 1, 2, 3)\n",
      "device     = cpu()\n",
      "params     = {'axes': (0, 1, 2, 3), 'shape': (2, 3, 3, 4)}\n",
      "shape      = (2, 3, 3, 4)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:145: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:16: in backward_check\n",
      "    \u001b[0mout = f(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572...62826 ]\n",
      "   [ 0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]]),)\n",
      "        eps        = 0.001\n",
      "        f          = <function flip at 0x7fe5968b4e00>\n",
      "        kwargs     = {'axes': (0, 1, 2, 3)}\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:454: in flip\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Flip(axes)(a)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572 ...7262826 ]\n",
      "   [ 0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]])\n",
      "        axes       = (0, 1, 2, 3)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572...62826 ]\n",
      "   [ 0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]]),)\n",
      "        self       = <needle.ops.ops_mathematic.Flip object at 0x7fe2b6c1eb50>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572...62826 ]\n",
      "   [ 0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]]),)\n",
      "        op         = <needle.ops.ops_mathematic.Flip object at 0x7fe2b6c1eb50>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fe2b6c1e3d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fe2b6c1e3d0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Flip object at 0x7fe2b6c1eb50>\n",
      "a = NDArray([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572 ]\n",
      "   [... 0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]], device=cpu())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "a          = NDArray([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572 ]\n",
      "   [... 0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]], device=cpu())\n",
      "self       = <needle.ops.ops_mathematic.Flip object at 0x7fe2b6c1eb50>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:444: NotImplementedError\n",
      "\u001b[31m\u001b[1m___ test_flip_backward[params9-needle.backend_ndarray.ndarray_backend_cuda] ____\u001b[0m\n",
      "\n",
      "params = {'axes': (0, 1, 2, 3), 'shape': (2, 3, 3, 4)}, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, flip_backward_params)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flip_backward\u001b[39;49;00m(params, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        shape, axes = params[\u001b[33m'\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      ">       backward_check(ndl.flip, ndl.Tensor(np.random.randn(*shape), device=device), axes=axes)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "axes       = (0, 1, 2, 3)\n",
      "device     = cuda()\n",
      "params     = {'axes': (0, 1, 2, 3), 'shape': (2, 3, 3, 4)}\n",
      "shape      = (2, 3, 3, 4)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:145: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:16: in backward_check\n",
      "    \u001b[0mout = f(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572...62826 ]\n",
      "   [ 0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]]),)\n",
      "        eps        = 0.001\n",
      "        f          = <function flip at 0x7fe5968b4e00>\n",
      "        kwargs     = {'axes': (0, 1, 2, 3)}\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:454: in flip\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Flip(axes)(a)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572 ...7262826 ]\n",
      "   [ 0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]])\n",
      "        axes       = (0, 1, 2, 3)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572...62826 ]\n",
      "   [ 0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]]),)\n",
      "        self       = <needle.ops.ops_mathematic.Flip object at 0x7fe4d9376850>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572...62826 ]\n",
      "   [ 0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]]),)\n",
      "        op         = <needle.ops.ops_mathematic.Flip object at 0x7fe4d9376850>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fe4d9375050>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fe4d9375050>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Flip object at 0x7fe4d9376850>\n",
      "a = NDArray([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572 ]\n",
      "   [...0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]], device=cuda())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "a          = NDArray([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572 ]\n",
      "   [...0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]], device=cuda())\n",
      "self       = <needle.ops.ops_mathematic.Flip object at 0x7fe4d9376850>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:444: NotImplementedError\n",
      "\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_flip_forward[params0-needle.backend_ndarray.ndarray_backend_cpu]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_flip_forward[params0-needle.backend_ndarray.ndarray_backend_cuda]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_flip_forward[params1-needle.backend_ndarray.ndarray_backend_cpu]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_flip_forward[params1-needle.backend_ndarray.ndarray_backend_cuda]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_flip_forward[params2-needle.backend_ndarray.ndarray_backend_cpu]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_flip_forward[params2-needle.backend_ndarray.ndarray_backend_cuda]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_flip_forward[params3-needle.backend_ndarray.ndarray_backend_cpu]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_flip_forward[params3-needle.backend_ndarray.ndarray_backend_cuda]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_flip_forward[params4-needle.backend_ndarray.ndarray_backend_cpu]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_flip_forward[params4-needle.backend_ndarray.ndarray_backend_cuda]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_flip_forward[params5-needle.backend_ndarray.ndarray_backend_cpu]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_flip_forward[params5-needle.backend_ndarray.ndarray_backend_cuda]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_flip_forward[params6-needle.backend_ndarray.ndarray_backend_cpu]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_flip_forward[params6-needle.backend_ndarray.ndarray_backend_cuda]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_flip_forward[params7-needle.backend_ndarray.ndarray_backend_cpu]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_flip_forward[params7-needle.backend_ndarray.ndarray_backend_cuda]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_flip_forward[params8-needle.backend_ndarray.ndarray_backend_cpu]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_flip_forward[params8-needle.backend_ndarray.ndarray_backend_cuda]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_flip_forward[params9-needle.backend_ndarray.ndarray_backend_cpu]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_flip_forward[params9-needle.backend_ndarray.ndarray_backend_cuda]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_flip_backward[params0-needle.backend_ndarray.ndarray_backend_cpu]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_flip_backward[params0-needle.backend_ndarray.ndarray_backend_cuda]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_flip_backward[params1-needle.backend_ndarray.ndarray_backend_cpu]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_flip_backward[params1-needle.backend_ndarray.ndarray_backend_cuda]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_flip_backward[params2-needle.backend_ndarray.ndarray_backend_cpu]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_flip_backward[params2-needle.backend_ndarray.ndarray_backend_cuda]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_flip_backward[params3-needle.backend_ndarray.ndarray_backend_cpu]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_flip_backward[params3-needle.backend_ndarray.ndarray_backend_cuda]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_flip_backward[params4-needle.backend_ndarray.ndarray_backend_cpu]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_flip_backward[params4-needle.backend_ndarray.ndarray_backend_cuda]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_flip_backward[params5-needle.backend_ndarray.ndarray_backend_cpu]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_flip_backward[params5-needle.backend_ndarray.ndarray_backend_cuda]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_flip_backward[params6-needle.backend_ndarray.ndarray_backend_cpu]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_flip_backward[params6-needle.backend_ndarray.ndarray_backend_cuda]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_flip_backward[params7-needle.backend_ndarray.ndarray_backend_cpu]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_flip_backward[params7-needle.backend_ndarray.ndarray_backend_cuda]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_flip_backward[params8-needle.backend_ndarray.ndarray_backend_cpu]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_flip_backward[params8-needle.backend_ndarray.ndarray_backend_cuda]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_flip_backward[params9-needle.backend_ndarray.ndarray_backend_cpu]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_flip_backward[params9-needle.backend_ndarray.ndarray_backend_cuda]\u001b[0m - NotImplementedError\n",
      "\u001b[31m===================== \u001b[31m\u001b[1m40 failed\u001b[0m, \u001b[33m1763 deselected\u001b[0m\u001b[31m in 4.52s\u001b[0m\u001b[31m ======================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m pytest -l -v -k \"flip\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dilation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dilation operator puts zeros between elements of an ndarray. We will need it for computing the backward pass of convolution when the stride of the convolution is greater than 1. As an example, dilation should do the following to a 2x2 matrix when dilated by 1 on both axes:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 & 2 \\\\\n",
    "3 & 4\n",
    "\\end{bmatrix}\n",
    "\\Longrightarrow\n",
    "\\begin{bmatrix}\n",
    "1 & 0 & 2 & 0 \\\\\n",
    "0 & 0 & 0 & 0 \\\\\n",
    "3 & 0 & 4 & 0 \\\\\n",
    "0 & 0 & 0 & 0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "To get some intuition for why we need dilation for the backward pass of strided convolution, consider a  `stride=2`, `padding=\"same\"`, `input_channels=output_channels=8` convolution applied to an input of size (10, 32, 32, 8). The resulting output will be of size (10, 16, 16, 8) due to the stride, and thus `out_grad` will have shape (10, 16, 16, 8). Yet, the gradient of the input needs to, of course, have shape (10, 32, 32, 8) -- so we must need to increase the size of `out_grad` in some way. Consider also that you could implement strided convolution as `Conv(x)[:, ::2, ::2, :]`, i.e., only keeping every other pixel in the spatial dimension.\n",
    "\n",
    "\n",
    "Implement `Dilate` and `UnDilate` in `ops_mathematic.py`. Each operator takes two additional parameters (in attrs): the `dilation` amount and the `axes` to dilate. You must also implement the corresponding op `UnDilate`, whose forward pass will be used to implement the gradient of `Dilate`. (This is so we do not have to implement `GetItem` and `SetItem` ops, which can be highly inefficient to backprop through without additional optimizations.)\n",
    "\n",
    "**Note**: The dilation amount is additive, not multiplicative. In the example above, a dilation of `1` implies adding one row/column of zeros between each element along each dilated axis (and one removed row/column for each undilated axis). A dilation of `0` means no change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.11.10, pytest-8.3.4, pluggy-1.5.0 -- /opt/conda/envs/train/bin/python\n",
      "cachedir: .pytest_cache\n",
      "hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase(PosixPath('/mnt/zhumingzhu/work/00test/hw4/.hypothesis/examples'))\n",
      "rootdir: /mnt/zhumingzhu/work/00test/hw4\n",
      "plugins: hypothesis-6.115.5\n",
      "collected 1803 items / 1777 deselected / 26 selected                           \u001b[0m\u001b[1m\n",
      "\n",
      "tests/hw4/test_conv.py::test_dilate_forward[needle.backend_ndarray.ndarray_backend_cpu] \u001b[31mFAILED\u001b[0m\u001b[31m [  3%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_dilate_forward[needle.backend_ndarray.ndarray_backend_cuda] \u001b[31mFAILED\u001b[0m\u001b[31m [  7%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_dilate_backward[params0-needle.backend_ndarray.ndarray_backend_cpu] \u001b[31mFAILED\u001b[0m\u001b[31m [ 11%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_dilate_backward[params0-needle.backend_ndarray.ndarray_backend_cuda] \u001b[31mFAILED\u001b[0m\u001b[31m [ 15%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_dilate_backward[params1-needle.backend_ndarray.ndarray_backend_cpu] \u001b[31mFAILED\u001b[0m\u001b[31m [ 19%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_dilate_backward[params1-needle.backend_ndarray.ndarray_backend_cuda] \u001b[31mFAILED\u001b[0m\u001b[31m [ 23%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_dilate_backward[params2-needle.backend_ndarray.ndarray_backend_cpu] \u001b[31mFAILED\u001b[0m\u001b[31m [ 26%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_dilate_backward[params2-needle.backend_ndarray.ndarray_backend_cuda] \u001b[31mFAILED\u001b[0m\u001b[31m [ 30%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_dilate_backward[params3-needle.backend_ndarray.ndarray_backend_cpu] \u001b[31mFAILED\u001b[0m\u001b[31m [ 34%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_dilate_backward[params3-needle.backend_ndarray.ndarray_backend_cuda] \u001b[31mFAILED\u001b[0m\u001b[31m [ 38%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_dilate_backward[params4-needle.backend_ndarray.ndarray_backend_cpu] \u001b[31mFAILED\u001b[0m\u001b[31m [ 42%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_dilate_backward[params4-needle.backend_ndarray.ndarray_backend_cuda] \u001b[31mFAILED\u001b[0m\u001b[31m [ 46%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_dilate_backward[params5-needle.backend_ndarray.ndarray_backend_cpu] \u001b[31mFAILED\u001b[0m\u001b[31m [ 50%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_dilate_backward[params5-needle.backend_ndarray.ndarray_backend_cuda] \u001b[31mFAILED\u001b[0m\u001b[31m [ 53%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_dilate_backward[params6-needle.backend_ndarray.ndarray_backend_cpu] \u001b[31mFAILED\u001b[0m\u001b[31m [ 57%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_dilate_backward[params6-needle.backend_ndarray.ndarray_backend_cuda] \u001b[31mFAILED\u001b[0m\u001b[31m [ 61%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_dilate_backward[params7-needle.backend_ndarray.ndarray_backend_cpu] \u001b[31mFAILED\u001b[0m\u001b[31m [ 65%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_dilate_backward[params7-needle.backend_ndarray.ndarray_backend_cuda] \u001b[31mFAILED\u001b[0m\u001b[31m [ 69%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_dilate_backward[params8-needle.backend_ndarray.ndarray_backend_cpu] \u001b[31mFAILED\u001b[0m\u001b[31m [ 73%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_dilate_backward[params8-needle.backend_ndarray.ndarray_backend_cuda] \u001b[31mFAILED\u001b[0m\u001b[31m [ 76%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_dilate_backward[params9-needle.backend_ndarray.ndarray_backend_cpu] \u001b[31mFAILED\u001b[0m\u001b[31m [ 80%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_dilate_backward[params9-needle.backend_ndarray.ndarray_backend_cuda] \u001b[31mFAILED\u001b[0m\u001b[31m [ 84%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_dilate_backward[params10-needle.backend_ndarray.ndarray_backend_cpu] \u001b[31mFAILED\u001b[0m\u001b[31m [ 88%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_dilate_backward[params10-needle.backend_ndarray.ndarray_backend_cuda] \u001b[31mFAILED\u001b[0m\u001b[31m [ 92%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_dilate_backward[params11-needle.backend_ndarray.ndarray_backend_cpu] \u001b[31mFAILED\u001b[0m\u001b[31m [ 96%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_dilate_backward[params11-needle.backend_ndarray.ndarray_backend_cuda] \u001b[31mFAILED\u001b[0m\u001b[31m [100%]\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m_______ test_dilate_forward[needle.backend_ndarray.ndarray_backend_cpu] ________\u001b[0m\n",
      "\n",
      "device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_dilate_forward\u001b[39;49;00m(device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        device = ndl.cpu()\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randint(\u001b[94m1\u001b[39;49;00m, \u001b[94m10\u001b[39;49;00m, size=(\u001b[94m2\u001b[39;49;00m, \u001b[94m5\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "        A = ndl.Tensor(_A, device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94massert\u001b[39;49;00m np.linalg.norm(ndl.dilate(A, dilation=\u001b[94m0\u001b[39;49;00m, axes=(\u001b[94m0\u001b[39;49;00m,)).numpy() - np.array([[\u001b[94m6.\u001b[39;49;00m, \u001b[94m1.\u001b[39;49;00m, \u001b[94m4.\u001b[39;49;00m, \u001b[94m4.\u001b[39;49;00m, \u001b[94m8.\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "           [\u001b[94m4.\u001b[39;49;00m, \u001b[94m6.\u001b[39;49;00m, \u001b[94m3.\u001b[39;49;00m, \u001b[94m5.\u001b[39;49;00m, \u001b[94m8.\u001b[39;49;00m]])) < \u001b[94m1e-5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "A          = needle.Tensor([[6. 1. 4. 4. 8.]\n",
      " [4. 6. 3. 5. 8.]])\n",
      "_A         = array([[6, 1, 4, 4, 8],\n",
      "       [4, 6, 3, 5, 8]])\n",
      "device     = cpu()\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:202: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:474: in dilate\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Dilate(axes, dilation)(a)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[6. 1. 4. 4. 8.]\n",
      " [4. 6. 3. 5. 8.]])\n",
      "        axes       = (0,)\n",
      "        dilation   = 0\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[6. 1. 4. 4. 8.]\n",
      " [4. 6. 3. 5. 8.]]),)\n",
      "        self       = <needle.ops.ops_mathematic.Dilate object at 0x7fd1e0762610>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[6. 1. 4. 4. 8.]\n",
      " [4. 6. 3. 5. 8.]]),)\n",
      "        op         = <needle.ops.ops_mathematic.Dilate object at 0x7fd1e0762610>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fd1e0951b90>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fd1e0951b90>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Dilate object at 0x7fd1e0762610>\n",
      "a = NDArray([[6. 1. 4. 4. 8.]\n",
      " [4. 6. 3. 5. 8.]], device=cpu())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "a          = NDArray([[6. 1. 4. 4. 8.]\n",
      " [4. 6. 3. 5. 8.]], device=cpu())\n",
      "self       = <needle.ops.ops_mathematic.Dilate object at 0x7fd1e0762610>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:464: NotImplementedError\n",
      "\u001b[31m\u001b[1m_______ test_dilate_forward[needle.backend_ndarray.ndarray_backend_cuda] _______\u001b[0m\n",
      "\n",
      "device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_dilate_forward\u001b[39;49;00m(device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        device = ndl.cpu()\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randint(\u001b[94m1\u001b[39;49;00m, \u001b[94m10\u001b[39;49;00m, size=(\u001b[94m2\u001b[39;49;00m, \u001b[94m5\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "        A = ndl.Tensor(_A, device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94massert\u001b[39;49;00m np.linalg.norm(ndl.dilate(A, dilation=\u001b[94m0\u001b[39;49;00m, axes=(\u001b[94m0\u001b[39;49;00m,)).numpy() - np.array([[\u001b[94m6.\u001b[39;49;00m, \u001b[94m1.\u001b[39;49;00m, \u001b[94m4.\u001b[39;49;00m, \u001b[94m4.\u001b[39;49;00m, \u001b[94m8.\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "           [\u001b[94m4.\u001b[39;49;00m, \u001b[94m6.\u001b[39;49;00m, \u001b[94m3.\u001b[39;49;00m, \u001b[94m5.\u001b[39;49;00m, \u001b[94m8.\u001b[39;49;00m]])) < \u001b[94m1e-5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "A          = needle.Tensor([[6. 1. 4. 4. 8.]\n",
      " [4. 6. 3. 5. 8.]])\n",
      "_A         = array([[6, 1, 4, 4, 8],\n",
      "       [4, 6, 3, 5, 8]])\n",
      "device     = cpu()\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:202: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:474: in dilate\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Dilate(axes, dilation)(a)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[6. 1. 4. 4. 8.]\n",
      " [4. 6. 3. 5. 8.]])\n",
      "        axes       = (0,)\n",
      "        dilation   = 0\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[6. 1. 4. 4. 8.]\n",
      " [4. 6. 3. 5. 8.]]),)\n",
      "        self       = <needle.ops.ops_mathematic.Dilate object at 0x7fd1df8b8b90>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[6. 1. 4. 4. 8.]\n",
      " [4. 6. 3. 5. 8.]]),)\n",
      "        op         = <needle.ops.ops_mathematic.Dilate object at 0x7fd1df8b8b90>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fd1df8b8ad0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fd1df8b8ad0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Dilate object at 0x7fd1df8b8b90>\n",
      "a = NDArray([[6. 1. 4. 4. 8.]\n",
      " [4. 6. 3. 5. 8.]], device=cpu())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "a          = NDArray([[6. 1. 4. 4. 8.]\n",
      " [4. 6. 3. 5. 8.]], device=cpu())\n",
      "self       = <needle.ops.ops_mathematic.Dilate object at 0x7fd1df8b8b90>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:464: NotImplementedError\n",
      "\u001b[31m\u001b[1m___ test_dilate_backward[params0-needle.backend_ndarray.ndarray_backend_cpu] ___\u001b[0m\n",
      "\n",
      "params = {'axes': (0,), 'd': 1, 'shape': (2, 5)}, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, dilate_backward_params)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_dilate_backward\u001b[39;49;00m(params, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        shape, d, axes = params[\u001b[33m'\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      ">       backward_check(ndl.dilate, ndl.Tensor(np.random.randn(*shape), device=device), dilation=d, axes=axes)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "axes       = (0,)\n",
      "d          = 1\n",
      "device     = cpu()\n",
      "params     = {'axes': (0,), 'd': 1, 'shape': (2, 5)}\n",
      "shape      = (2, 5)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:296: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:16: in backward_check\n",
      "    \u001b[0mout = f(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0.10321885  0.41059852]]),)\n",
      "        eps        = 0.001\n",
      "        f          = <function dilate at 0x7fd29ce4d080>\n",
      "        kwargs     = {'axes': (0,), 'dilation': 1}\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:474: in dilate\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Dilate(axes, dilation)(a)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0.10321885  0.41059852]])\n",
      "        axes       = (0,)\n",
      "        dilation   = 1\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0.10321885  0.41059852]]),)\n",
      "        self       = <needle.ops.ops_mathematic.Dilate object at 0x7fd1dfc36190>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0.10321885  0.41059852]]),)\n",
      "        op         = <needle.ops.ops_mathematic.Dilate object at 0x7fd1dfc36190>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fd1dfc35dd0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fd1dfc35dd0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Dilate object at 0x7fd1dfc36190>\n",
      "a = NDArray([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0.10321885  0.41059852]], device=cpu())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "a          = NDArray([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0.10321885  0.41059852]], device=cpu())\n",
      "self       = <needle.ops.ops_mathematic.Dilate object at 0x7fd1dfc36190>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:464: NotImplementedError\n",
      "\u001b[31m\u001b[1m__ test_dilate_backward[params0-needle.backend_ndarray.ndarray_backend_cuda] ___\u001b[0m\n",
      "\n",
      "params = {'axes': (0,), 'd': 1, 'shape': (2, 5)}, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, dilate_backward_params)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_dilate_backward\u001b[39;49;00m(params, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        shape, d, axes = params[\u001b[33m'\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      ">       backward_check(ndl.dilate, ndl.Tensor(np.random.randn(*shape), device=device), dilation=d, axes=axes)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "axes       = (0,)\n",
      "d          = 1\n",
      "device     = cuda()\n",
      "params     = {'axes': (0,), 'd': 1, 'shape': (2, 5)}\n",
      "shape      = (2, 5)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:296: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:16: in backward_check\n",
      "    \u001b[0mout = f(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0.10321885  0.41059852]]),)\n",
      "        eps        = 0.001\n",
      "        f          = <function dilate at 0x7fd29ce4d080>\n",
      "        kwargs     = {'axes': (0,), 'dilation': 1}\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:474: in dilate\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Dilate(axes, dilation)(a)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0.10321885  0.41059852]])\n",
      "        axes       = (0,)\n",
      "        dilation   = 1\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0.10321885  0.41059852]]),)\n",
      "        self       = <needle.ops.ops_mathematic.Dilate object at 0x7fd1dfc9ce50>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0.10321885  0.41059852]]),)\n",
      "        op         = <needle.ops.ops_mathematic.Dilate object at 0x7fd1dfc9ce50>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fd1dfc9ce10>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fd1dfc9ce10>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Dilate object at 0x7fd1dfc9ce50>\n",
      "a = NDArray([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0.10321885  0.41059852]], device=cuda())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "a          = NDArray([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0.10321885  0.41059852]], device=cuda())\n",
      "self       = <needle.ops.ops_mathematic.Dilate object at 0x7fd1dfc9ce50>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:464: NotImplementedError\n",
      "\u001b[31m\u001b[1m___ test_dilate_backward[params1-needle.backend_ndarray.ndarray_backend_cpu] ___\u001b[0m\n",
      "\n",
      "params = {'axes': (1,), 'd': 2, 'shape': (2, 5)}, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, dilate_backward_params)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_dilate_backward\u001b[39;49;00m(params, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        shape, d, axes = params[\u001b[33m'\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      ">       backward_check(ndl.dilate, ndl.Tensor(np.random.randn(*shape), device=device), dilation=d, axes=axes)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "axes       = (1,)\n",
      "d          = 2\n",
      "device     = cpu()\n",
      "params     = {'axes': (1,), 'd': 2, 'shape': (2, 5)}\n",
      "shape      = (2, 5)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:296: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:16: in backward_check\n",
      "    \u001b[0mout = f(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0.10321885  0.41059852]]),)\n",
      "        eps        = 0.001\n",
      "        f          = <function dilate at 0x7fd29ce4d080>\n",
      "        kwargs     = {'axes': (1,), 'dilation': 2}\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:474: in dilate\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Dilate(axes, dilation)(a)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0.10321885  0.41059852]])\n",
      "        axes       = (1,)\n",
      "        dilation   = 2\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0.10321885  0.41059852]]),)\n",
      "        self       = <needle.ops.ops_mathematic.Dilate object at 0x7fd1dfb04450>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0.10321885  0.41059852]]),)\n",
      "        op         = <needle.ops.ops_mathematic.Dilate object at 0x7fd1dfb04450>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fd1dfb044d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fd1dfb044d0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Dilate object at 0x7fd1dfb04450>\n",
      "a = NDArray([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0.10321885  0.41059852]], device=cpu())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "a          = NDArray([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0.10321885  0.41059852]], device=cpu())\n",
      "self       = <needle.ops.ops_mathematic.Dilate object at 0x7fd1dfb04450>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:464: NotImplementedError\n",
      "\u001b[31m\u001b[1m__ test_dilate_backward[params1-needle.backend_ndarray.ndarray_backend_cuda] ___\u001b[0m\n",
      "\n",
      "params = {'axes': (1,), 'd': 2, 'shape': (2, 5)}, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, dilate_backward_params)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_dilate_backward\u001b[39;49;00m(params, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        shape, d, axes = params[\u001b[33m'\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      ">       backward_check(ndl.dilate, ndl.Tensor(np.random.randn(*shape), device=device), dilation=d, axes=axes)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "axes       = (1,)\n",
      "d          = 2\n",
      "device     = cuda()\n",
      "params     = {'axes': (1,), 'd': 2, 'shape': (2, 5)}\n",
      "shape      = (2, 5)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:296: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:16: in backward_check\n",
      "    \u001b[0mout = f(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0.10321885  0.41059852]]),)\n",
      "        eps        = 0.001\n",
      "        f          = <function dilate at 0x7fd29ce4d080>\n",
      "        kwargs     = {'axes': (1,), 'dilation': 2}\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:474: in dilate\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Dilate(axes, dilation)(a)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0.10321885  0.41059852]])\n",
      "        axes       = (1,)\n",
      "        dilation   = 2\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0.10321885  0.41059852]]),)\n",
      "        self       = <needle.ops.ops_mathematic.Dilate object at 0x7fd1dfa4d410>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0.10321885  0.41059852]]),)\n",
      "        op         = <needle.ops.ops_mathematic.Dilate object at 0x7fd1dfa4d410>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fd1dfa4c8d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fd1dfa4c8d0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Dilate object at 0x7fd1dfa4d410>\n",
      "a = NDArray([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0.10321885  0.41059852]], device=cuda())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "a          = NDArray([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0.10321885  0.41059852]], device=cuda())\n",
      "self       = <needle.ops.ops_mathematic.Dilate object at 0x7fd1dfa4d410>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:464: NotImplementedError\n",
      "\u001b[31m\u001b[1m___ test_dilate_backward[params2-needle.backend_ndarray.ndarray_backend_cpu] ___\u001b[0m\n",
      "\n",
      "params = {'axes': (0, 1), 'd': 1, 'shape': (2, 5)}, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, dilate_backward_params)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_dilate_backward\u001b[39;49;00m(params, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        shape, d, axes = params[\u001b[33m'\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      ">       backward_check(ndl.dilate, ndl.Tensor(np.random.randn(*shape), device=device), dilation=d, axes=axes)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "axes       = (0, 1)\n",
      "d          = 1\n",
      "device     = cpu()\n",
      "params     = {'axes': (0, 1), 'd': 1, 'shape': (2, 5)}\n",
      "shape      = (2, 5)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:296: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:16: in backward_check\n",
      "    \u001b[0mout = f(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0.10321885  0.41059852]]),)\n",
      "        eps        = 0.001\n",
      "        f          = <function dilate at 0x7fd29ce4d080>\n",
      "        kwargs     = {'axes': (0, 1), 'dilation': 1}\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:474: in dilate\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Dilate(axes, dilation)(a)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0.10321885  0.41059852]])\n",
      "        axes       = (0, 1)\n",
      "        dilation   = 1\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0.10321885  0.41059852]]),)\n",
      "        self       = <needle.ops.ops_mathematic.Dilate object at 0x7fd6e0034390>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0.10321885  0.41059852]]),)\n",
      "        op         = <needle.ops.ops_mathematic.Dilate object at 0x7fd6e0034390>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fd6e0037790>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fd6e0037790>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Dilate object at 0x7fd6e0034390>\n",
      "a = NDArray([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0.10321885  0.41059852]], device=cpu())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "a          = NDArray([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0.10321885  0.41059852]], device=cpu())\n",
      "self       = <needle.ops.ops_mathematic.Dilate object at 0x7fd6e0034390>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:464: NotImplementedError\n",
      "\u001b[31m\u001b[1m__ test_dilate_backward[params2-needle.backend_ndarray.ndarray_backend_cuda] ___\u001b[0m\n",
      "\n",
      "params = {'axes': (0, 1), 'd': 1, 'shape': (2, 5)}, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, dilate_backward_params)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_dilate_backward\u001b[39;49;00m(params, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        shape, d, axes = params[\u001b[33m'\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      ">       backward_check(ndl.dilate, ndl.Tensor(np.random.randn(*shape), device=device), dilation=d, axes=axes)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "axes       = (0, 1)\n",
      "d          = 1\n",
      "device     = cuda()\n",
      "params     = {'axes': (0, 1), 'd': 1, 'shape': (2, 5)}\n",
      "shape      = (2, 5)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:296: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:16: in backward_check\n",
      "    \u001b[0mout = f(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0.10321885  0.41059852]]),)\n",
      "        eps        = 0.001\n",
      "        f          = <function dilate at 0x7fd29ce4d080>\n",
      "        kwargs     = {'axes': (0, 1), 'dilation': 1}\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:474: in dilate\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Dilate(axes, dilation)(a)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0.10321885  0.41059852]])\n",
      "        axes       = (0, 1)\n",
      "        dilation   = 1\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0.10321885  0.41059852]]),)\n",
      "        self       = <needle.ops.ops_mathematic.Dilate object at 0x7fd1dfa64ed0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0.10321885  0.41059852]]),)\n",
      "        op         = <needle.ops.ops_mathematic.Dilate object at 0x7fd1dfa64ed0>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fd1dfa64bd0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fd1dfa64bd0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Dilate object at 0x7fd1dfa64ed0>\n",
      "a = NDArray([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0.10321885  0.41059852]], device=cuda())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "a          = NDArray([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0.10321885  0.41059852]], device=cuda())\n",
      "self       = <needle.ops.ops_mathematic.Dilate object at 0x7fd1dfa64ed0>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:464: NotImplementedError\n",
      "\u001b[31m\u001b[1m___ test_dilate_backward[params3-needle.backend_ndarray.ndarray_backend_cpu] ___\u001b[0m\n",
      "\n",
      "params = {'axes': (0, 1), 'd': 0, 'shape': (2, 5)}, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, dilate_backward_params)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_dilate_backward\u001b[39;49;00m(params, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        shape, d, axes = params[\u001b[33m'\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      ">       backward_check(ndl.dilate, ndl.Tensor(np.random.randn(*shape), device=device), dilation=d, axes=axes)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "axes       = (0, 1)\n",
      "d          = 0\n",
      "device     = cpu()\n",
      "params     = {'axes': (0, 1), 'd': 0, 'shape': (2, 5)}\n",
      "shape      = (2, 5)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:296: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:16: in backward_check\n",
      "    \u001b[0mout = f(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0.10321885  0.41059852]]),)\n",
      "        eps        = 0.001\n",
      "        f          = <function dilate at 0x7fd29ce4d080>\n",
      "        kwargs     = {'axes': (0, 1), 'dilation': 0}\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:474: in dilate\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Dilate(axes, dilation)(a)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0.10321885  0.41059852]])\n",
      "        axes       = (0, 1)\n",
      "        dilation   = 0\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0.10321885  0.41059852]]),)\n",
      "        self       = <needle.ops.ops_mathematic.Dilate object at 0x7fd1df988490>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0.10321885  0.41059852]]),)\n",
      "        op         = <needle.ops.ops_mathematic.Dilate object at 0x7fd1df988490>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fd1df988690>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fd1df988690>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Dilate object at 0x7fd1df988490>\n",
      "a = NDArray([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0.10321885  0.41059852]], device=cpu())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "a          = NDArray([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0.10321885  0.41059852]], device=cpu())\n",
      "self       = <needle.ops.ops_mathematic.Dilate object at 0x7fd1df988490>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:464: NotImplementedError\n",
      "\u001b[31m\u001b[1m__ test_dilate_backward[params3-needle.backend_ndarray.ndarray_backend_cuda] ___\u001b[0m\n",
      "\n",
      "params = {'axes': (0, 1), 'd': 0, 'shape': (2, 5)}, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, dilate_backward_params)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_dilate_backward\u001b[39;49;00m(params, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        shape, d, axes = params[\u001b[33m'\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      ">       backward_check(ndl.dilate, ndl.Tensor(np.random.randn(*shape), device=device), dilation=d, axes=axes)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "axes       = (0, 1)\n",
      "d          = 0\n",
      "device     = cuda()\n",
      "params     = {'axes': (0, 1), 'd': 0, 'shape': (2, 5)}\n",
      "shape      = (2, 5)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:296: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:16: in backward_check\n",
      "    \u001b[0mout = f(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0.10321885  0.41059852]]),)\n",
      "        eps        = 0.001\n",
      "        f          = <function dilate at 0x7fd29ce4d080>\n",
      "        kwargs     = {'axes': (0, 1), 'dilation': 0}\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:474: in dilate\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Dilate(axes, dilation)(a)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0.10321885  0.41059852]])\n",
      "        axes       = (0, 1)\n",
      "        dilation   = 0\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0.10321885  0.41059852]]),)\n",
      "        self       = <needle.ops.ops_mathematic.Dilate object at 0x7fd1df960350>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0.10321885  0.41059852]]),)\n",
      "        op         = <needle.ops.ops_mathematic.Dilate object at 0x7fd1df960350>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fd1df960290>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fd1df960290>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Dilate object at 0x7fd1df960350>\n",
      "a = NDArray([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0.10321885  0.41059852]], device=cuda())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "a          = NDArray([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0.10321885  0.41059852]], device=cuda())\n",
      "self       = <needle.ops.ops_mathematic.Dilate object at 0x7fd1df960350>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:464: NotImplementedError\n",
      "\u001b[31m\u001b[1m___ test_dilate_backward[params4-needle.backend_ndarray.ndarray_backend_cpu] ___\u001b[0m\n",
      "\n",
      "params = {'axes': (0, 1), 'd': 2, 'shape': (2, 3, 3, 4)}, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, dilate_backward_params)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_dilate_backward\u001b[39;49;00m(params, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        shape, d, axes = params[\u001b[33m'\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      ">       backward_check(ndl.dilate, ndl.Tensor(np.random.randn(*shape), device=device), dilation=d, axes=axes)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "axes       = (0, 1)\n",
      "d          = 2\n",
      "device     = cpu()\n",
      "params     = {'axes': (0, 1), 'd': 2, 'shape': (2, 3, 3, 4)}\n",
      "shape      = (2, 3, 3, 4)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:296: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:16: in backward_check\n",
      "    \u001b[0mout = f(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572...62826 ]\n",
      "   [ 0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]]),)\n",
      "        eps        = 0.001\n",
      "        f          = <function dilate at 0x7fd29ce4d080>\n",
      "        kwargs     = {'axes': (0, 1), 'dilation': 2}\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:474: in dilate\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Dilate(axes, dilation)(a)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572 ...7262826 ]\n",
      "   [ 0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]])\n",
      "        axes       = (0, 1)\n",
      "        dilation   = 2\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572...62826 ]\n",
      "   [ 0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]]),)\n",
      "        self       = <needle.ops.ops_mathematic.Dilate object at 0x7fd1e0744bd0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572...62826 ]\n",
      "   [ 0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]]),)\n",
      "        op         = <needle.ops.ops_mathematic.Dilate object at 0x7fd1e0744bd0>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fd1dfb3f190>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fd1dfb3f190>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Dilate object at 0x7fd1e0744bd0>\n",
      "a = NDArray([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572 ]\n",
      "   [... 0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]], device=cpu())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "a          = NDArray([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572 ]\n",
      "   [... 0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]], device=cpu())\n",
      "self       = <needle.ops.ops_mathematic.Dilate object at 0x7fd1e0744bd0>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:464: NotImplementedError\n",
      "\u001b[31m\u001b[1m__ test_dilate_backward[params4-needle.backend_ndarray.ndarray_backend_cuda] ___\u001b[0m\n",
      "\n",
      "params = {'axes': (0, 1), 'd': 2, 'shape': (2, 3, 3, 4)}, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, dilate_backward_params)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_dilate_backward\u001b[39;49;00m(params, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        shape, d, axes = params[\u001b[33m'\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      ">       backward_check(ndl.dilate, ndl.Tensor(np.random.randn(*shape), device=device), dilation=d, axes=axes)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "axes       = (0, 1)\n",
      "d          = 2\n",
      "device     = cuda()\n",
      "params     = {'axes': (0, 1), 'd': 2, 'shape': (2, 3, 3, 4)}\n",
      "shape      = (2, 3, 3, 4)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:296: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:16: in backward_check\n",
      "    \u001b[0mout = f(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572...62826 ]\n",
      "   [ 0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]]),)\n",
      "        eps        = 0.001\n",
      "        f          = <function dilate at 0x7fd29ce4d080>\n",
      "        kwargs     = {'axes': (0, 1), 'dilation': 2}\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:474: in dilate\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Dilate(axes, dilation)(a)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572 ...7262826 ]\n",
      "   [ 0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]])\n",
      "        axes       = (0, 1)\n",
      "        dilation   = 2\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572...62826 ]\n",
      "   [ 0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]]),)\n",
      "        self       = <needle.ops.ops_mathematic.Dilate object at 0x7fd1df8c1490>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572...62826 ]\n",
      "   [ 0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]]),)\n",
      "        op         = <needle.ops.ops_mathematic.Dilate object at 0x7fd1df8c1490>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fd1df8c2990>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fd1df8c2990>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Dilate object at 0x7fd1df8c1490>\n",
      "a = NDArray([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572 ]\n",
      "   [...0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]], device=cuda())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "a          = NDArray([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572 ]\n",
      "   [...0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]], device=cuda())\n",
      "self       = <needle.ops.ops_mathematic.Dilate object at 0x7fd1df8c1490>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:464: NotImplementedError\n",
      "\u001b[31m\u001b[1m___ test_dilate_backward[params5-needle.backend_ndarray.ndarray_backend_cpu] ___\u001b[0m\n",
      "\n",
      "params = {'axes': (0, 1), 'd': 3, 'shape': (3, 3, 6, 4)}, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, dilate_backward_params)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_dilate_backward\u001b[39;49;00m(params, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        shape, d, axes = params[\u001b[33m'\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      ">       backward_check(ndl.dilate, ndl.Tensor(np.random.randn(*shape), device=device), dilation=d, axes=axes)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "axes       = (0, 1)\n",
      "d          = 3\n",
      "device     = cpu()\n",
      "params     = {'axes': (0, 1), 'd': 3, 'shape': (3, 3, 6, 4)}\n",
      "shape      = (3, 3, 6, 4)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:296: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:16: in backward_check\n",
      "    \u001b[0mout = f(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572...80309 ]\n",
      "   [ 0.2799246  -0.09815039  0.9101789   0.3172182 ]\n",
      "   [ 0.78632796 -0.4664191  -0.94444627 -0.4100497 ]]]]),)\n",
      "        eps        = 0.001\n",
      "        f          = <function dilate at 0x7fd29ce4d080>\n",
      "        kwargs     = {'axes': (0, 1), 'dilation': 3}\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:474: in dilate\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Dilate(axes, dilation)(a)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572 ...7380309 ]\n",
      "   [ 0.2799246  -0.09815039  0.9101789   0.3172182 ]\n",
      "   [ 0.78632796 -0.4664191  -0.94444627 -0.4100497 ]]]])\n",
      "        axes       = (0, 1)\n",
      "        dilation   = 3\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572...80309 ]\n",
      "   [ 0.2799246  -0.09815039  0.9101789   0.3172182 ]\n",
      "   [ 0.78632796 -0.4664191  -0.94444627 -0.4100497 ]]]]),)\n",
      "        self       = <needle.ops.ops_mathematic.Dilate object at 0x7fd1dfb10810>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572...80309 ]\n",
      "   [ 0.2799246  -0.09815039  0.9101789   0.3172182 ]\n",
      "   [ 0.78632796 -0.4664191  -0.94444627 -0.4100497 ]]]]),)\n",
      "        op         = <needle.ops.ops_mathematic.Dilate object at 0x7fd1dfb10810>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fd1dfb10950>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fd1dfb10950>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Dilate object at 0x7fd1dfb10810>\n",
      "a = NDArray([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572 ]\n",
      "   [... 0.2799246  -0.09815039  0.9101789   0.3172182 ]\n",
      "   [ 0.78632796 -0.4664191  -0.94444627 -0.4100497 ]]]], device=cpu())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "a          = NDArray([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572 ]\n",
      "   [... 0.2799246  -0.09815039  0.9101789   0.3172182 ]\n",
      "   [ 0.78632796 -0.4664191  -0.94444627 -0.4100497 ]]]], device=cpu())\n",
      "self       = <needle.ops.ops_mathematic.Dilate object at 0x7fd1dfb10810>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:464: NotImplementedError\n",
      "\u001b[31m\u001b[1m__ test_dilate_backward[params5-needle.backend_ndarray.ndarray_backend_cuda] ___\u001b[0m\n",
      "\n",
      "params = {'axes': (0, 1), 'd': 3, 'shape': (3, 3, 6, 4)}, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, dilate_backward_params)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_dilate_backward\u001b[39;49;00m(params, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        shape, d, axes = params[\u001b[33m'\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      ">       backward_check(ndl.dilate, ndl.Tensor(np.random.randn(*shape), device=device), dilation=d, axes=axes)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "axes       = (0, 1)\n",
      "d          = 3\n",
      "device     = cuda()\n",
      "params     = {'axes': (0, 1), 'd': 3, 'shape': (3, 3, 6, 4)}\n",
      "shape      = (3, 3, 6, 4)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:296: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:16: in backward_check\n",
      "    \u001b[0mout = f(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572...80309 ]\n",
      "   [ 0.2799246  -0.09815039  0.9101789   0.3172182 ]\n",
      "   [ 0.78632796 -0.4664191  -0.94444627 -0.4100497 ]]]]),)\n",
      "        eps        = 0.001\n",
      "        f          = <function dilate at 0x7fd29ce4d080>\n",
      "        kwargs     = {'axes': (0, 1), 'dilation': 3}\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:474: in dilate\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Dilate(axes, dilation)(a)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572 ...7380309 ]\n",
      "   [ 0.2799246  -0.09815039  0.9101789   0.3172182 ]\n",
      "   [ 0.78632796 -0.4664191  -0.94444627 -0.4100497 ]]]])\n",
      "        axes       = (0, 1)\n",
      "        dilation   = 3\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572...80309 ]\n",
      "   [ 0.2799246  -0.09815039  0.9101789   0.3172182 ]\n",
      "   [ 0.78632796 -0.4664191  -0.94444627 -0.4100497 ]]]]),)\n",
      "        self       = <needle.ops.ops_mathematic.Dilate object at 0x7fd1dfb95510>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572...80309 ]\n",
      "   [ 0.2799246  -0.09815039  0.9101789   0.3172182 ]\n",
      "   [ 0.78632796 -0.4664191  -0.94444627 -0.4100497 ]]]]),)\n",
      "        op         = <needle.ops.ops_mathematic.Dilate object at 0x7fd1dfb95510>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fd1dfb95d50>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fd1dfb95d50>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Dilate object at 0x7fd1dfb95510>\n",
      "a = NDArray([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572 ]\n",
      "   [...0.2799246  -0.09815039  0.9101789   0.3172182 ]\n",
      "   [ 0.78632796 -0.4664191  -0.94444627 -0.4100497 ]]]], device=cuda())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "a          = NDArray([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572 ]\n",
      "   [...0.2799246  -0.09815039  0.9101789   0.3172182 ]\n",
      "   [ 0.78632796 -0.4664191  -0.94444627 -0.4100497 ]]]], device=cuda())\n",
      "self       = <needle.ops.ops_mathematic.Dilate object at 0x7fd1dfb95510>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:464: NotImplementedError\n",
      "\u001b[31m\u001b[1m___ test_dilate_backward[params6-needle.backend_ndarray.ndarray_backend_cpu] ___\u001b[0m\n",
      "\n",
      "params = {'axes': (1, 2), 'd': 0, 'shape': (2, 3, 3, 4)}, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, dilate_backward_params)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_dilate_backward\u001b[39;49;00m(params, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        shape, d, axes = params[\u001b[33m'\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      ">       backward_check(ndl.dilate, ndl.Tensor(np.random.randn(*shape), device=device), dilation=d, axes=axes)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "axes       = (1, 2)\n",
      "d          = 0\n",
      "device     = cpu()\n",
      "params     = {'axes': (1, 2), 'd': 0, 'shape': (2, 3, 3, 4)}\n",
      "shape      = (2, 3, 3, 4)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:296: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:16: in backward_check\n",
      "    \u001b[0mout = f(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572...62826 ]\n",
      "   [ 0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]]),)\n",
      "        eps        = 0.001\n",
      "        f          = <function dilate at 0x7fd29ce4d080>\n",
      "        kwargs     = {'axes': (1, 2), 'dilation': 0}\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:474: in dilate\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Dilate(axes, dilation)(a)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572 ...7262826 ]\n",
      "   [ 0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]])\n",
      "        axes       = (1, 2)\n",
      "        dilation   = 0\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572...62826 ]\n",
      "   [ 0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]]),)\n",
      "        self       = <needle.ops.ops_mathematic.Dilate object at 0x7fd1df8de7d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572...62826 ]\n",
      "   [ 0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]]),)\n",
      "        op         = <needle.ops.ops_mathematic.Dilate object at 0x7fd1df8de7d0>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fd1df8de0d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fd1df8de0d0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Dilate object at 0x7fd1df8de7d0>\n",
      "a = NDArray([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572 ]\n",
      "   [... 0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]], device=cpu())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "a          = NDArray([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572 ]\n",
      "   [... 0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]], device=cpu())\n",
      "self       = <needle.ops.ops_mathematic.Dilate object at 0x7fd1df8de7d0>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:464: NotImplementedError\n",
      "\u001b[31m\u001b[1m__ test_dilate_backward[params6-needle.backend_ndarray.ndarray_backend_cuda] ___\u001b[0m\n",
      "\n",
      "params = {'axes': (1, 2), 'd': 0, 'shape': (2, 3, 3, 4)}, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, dilate_backward_params)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_dilate_backward\u001b[39;49;00m(params, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        shape, d, axes = params[\u001b[33m'\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      ">       backward_check(ndl.dilate, ndl.Tensor(np.random.randn(*shape), device=device), dilation=d, axes=axes)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "axes       = (1, 2)\n",
      "d          = 0\n",
      "device     = cuda()\n",
      "params     = {'axes': (1, 2), 'd': 0, 'shape': (2, 3, 3, 4)}\n",
      "shape      = (2, 3, 3, 4)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:296: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:16: in backward_check\n",
      "    \u001b[0mout = f(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572...62826 ]\n",
      "   [ 0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]]),)\n",
      "        eps        = 0.001\n",
      "        f          = <function dilate at 0x7fd29ce4d080>\n",
      "        kwargs     = {'axes': (1, 2), 'dilation': 0}\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:474: in dilate\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Dilate(axes, dilation)(a)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572 ...7262826 ]\n",
      "   [ 0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]])\n",
      "        axes       = (1, 2)\n",
      "        dilation   = 0\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572...62826 ]\n",
      "   [ 0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]]),)\n",
      "        self       = <needle.ops.ops_mathematic.Dilate object at 0x7fd1df9db190>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572...62826 ]\n",
      "   [ 0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]]),)\n",
      "        op         = <needle.ops.ops_mathematic.Dilate object at 0x7fd1df9db190>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fd1df9db850>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fd1df9db850>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Dilate object at 0x7fd1df9db190>\n",
      "a = NDArray([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572 ]\n",
      "   [...0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]], device=cuda())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "a          = NDArray([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572 ]\n",
      "   [...0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]], device=cuda())\n",
      "self       = <needle.ops.ops_mathematic.Dilate object at 0x7fd1df9db190>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:464: NotImplementedError\n",
      "\u001b[31m\u001b[1m___ test_dilate_backward[params7-needle.backend_ndarray.ndarray_backend_cpu] ___\u001b[0m\n",
      "\n",
      "params = {'axes': (1, 2), 'd': 1, 'shape': (2, 3, 3, 4)}, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, dilate_backward_params)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_dilate_backward\u001b[39;49;00m(params, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        shape, d, axes = params[\u001b[33m'\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      ">       backward_check(ndl.dilate, ndl.Tensor(np.random.randn(*shape), device=device), dilation=d, axes=axes)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "axes       = (1, 2)\n",
      "d          = 1\n",
      "device     = cpu()\n",
      "params     = {'axes': (1, 2), 'd': 1, 'shape': (2, 3, 3, 4)}\n",
      "shape      = (2, 3, 3, 4)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:296: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:16: in backward_check\n",
      "    \u001b[0mout = f(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572...62826 ]\n",
      "   [ 0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]]),)\n",
      "        eps        = 0.001\n",
      "        f          = <function dilate at 0x7fd29ce4d080>\n",
      "        kwargs     = {'axes': (1, 2), 'dilation': 1}\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:474: in dilate\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Dilate(axes, dilation)(a)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572 ...7262826 ]\n",
      "   [ 0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]])\n",
      "        axes       = (1, 2)\n",
      "        dilation   = 1\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572...62826 ]\n",
      "   [ 0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]]),)\n",
      "        self       = <needle.ops.ops_mathematic.Dilate object at 0x7fd1df9eb150>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572...62826 ]\n",
      "   [ 0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]]),)\n",
      "        op         = <needle.ops.ops_mathematic.Dilate object at 0x7fd1df9eb150>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fd1df9e8e50>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fd1df9e8e50>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Dilate object at 0x7fd1df9eb150>\n",
      "a = NDArray([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572 ]\n",
      "   [... 0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]], device=cpu())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "a          = NDArray([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572 ]\n",
      "   [... 0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]], device=cpu())\n",
      "self       = <needle.ops.ops_mathematic.Dilate object at 0x7fd1df9eb150>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:464: NotImplementedError\n",
      "\u001b[31m\u001b[1m__ test_dilate_backward[params7-needle.backend_ndarray.ndarray_backend_cuda] ___\u001b[0m\n",
      "\n",
      "params = {'axes': (1, 2), 'd': 1, 'shape': (2, 3, 3, 4)}, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, dilate_backward_params)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_dilate_backward\u001b[39;49;00m(params, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        shape, d, axes = params[\u001b[33m'\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      ">       backward_check(ndl.dilate, ndl.Tensor(np.random.randn(*shape), device=device), dilation=d, axes=axes)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "axes       = (1, 2)\n",
      "d          = 1\n",
      "device     = cuda()\n",
      "params     = {'axes': (1, 2), 'd': 1, 'shape': (2, 3, 3, 4)}\n",
      "shape      = (2, 3, 3, 4)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:296: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:16: in backward_check\n",
      "    \u001b[0mout = f(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572...62826 ]\n",
      "   [ 0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]]),)\n",
      "        eps        = 0.001\n",
      "        f          = <function dilate at 0x7fd29ce4d080>\n",
      "        kwargs     = {'axes': (1, 2), 'dilation': 1}\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:474: in dilate\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Dilate(axes, dilation)(a)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572 ...7262826 ]\n",
      "   [ 0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]])\n",
      "        axes       = (1, 2)\n",
      "        dilation   = 1\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572...62826 ]\n",
      "   [ 0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]]),)\n",
      "        self       = <needle.ops.ops_mathematic.Dilate object at 0x7fd1df9c46d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572...62826 ]\n",
      "   [ 0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]]),)\n",
      "        op         = <needle.ops.ops_mathematic.Dilate object at 0x7fd1df9c46d0>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fd1df9c49d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fd1df9c49d0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Dilate object at 0x7fd1df9c46d0>\n",
      "a = NDArray([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572 ]\n",
      "   [...0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]], device=cuda())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "a          = NDArray([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572 ]\n",
      "   [...0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]], device=cuda())\n",
      "self       = <needle.ops.ops_mathematic.Dilate object at 0x7fd1df9c46d0>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:464: NotImplementedError\n",
      "\u001b[31m\u001b[1m___ test_dilate_backward[params8-needle.backend_ndarray.ndarray_backend_cpu] ___\u001b[0m\n",
      "\n",
      "params = {'axes': (1, 2), 'd': 1, 'shape': (3, 3, 6, 4)}, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, dilate_backward_params)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_dilate_backward\u001b[39;49;00m(params, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        shape, d, axes = params[\u001b[33m'\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      ">       backward_check(ndl.dilate, ndl.Tensor(np.random.randn(*shape), device=device), dilation=d, axes=axes)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "axes       = (1, 2)\n",
      "d          = 1\n",
      "device     = cpu()\n",
      "params     = {'axes': (1, 2), 'd': 1, 'shape': (3, 3, 6, 4)}\n",
      "shape      = (3, 3, 6, 4)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:296: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:16: in backward_check\n",
      "    \u001b[0mout = f(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572...80309 ]\n",
      "   [ 0.2799246  -0.09815039  0.9101789   0.3172182 ]\n",
      "   [ 0.78632796 -0.4664191  -0.94444627 -0.4100497 ]]]]),)\n",
      "        eps        = 0.001\n",
      "        f          = <function dilate at 0x7fd29ce4d080>\n",
      "        kwargs     = {'axes': (1, 2), 'dilation': 1}\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:474: in dilate\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Dilate(axes, dilation)(a)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572 ...7380309 ]\n",
      "   [ 0.2799246  -0.09815039  0.9101789   0.3172182 ]\n",
      "   [ 0.78632796 -0.4664191  -0.94444627 -0.4100497 ]]]])\n",
      "        axes       = (1, 2)\n",
      "        dilation   = 1\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572...80309 ]\n",
      "   [ 0.2799246  -0.09815039  0.9101789   0.3172182 ]\n",
      "   [ 0.78632796 -0.4664191  -0.94444627 -0.4100497 ]]]]),)\n",
      "        self       = <needle.ops.ops_mathematic.Dilate object at 0x7fd1dfa2e810>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572...80309 ]\n",
      "   [ 0.2799246  -0.09815039  0.9101789   0.3172182 ]\n",
      "   [ 0.78632796 -0.4664191  -0.94444627 -0.4100497 ]]]]),)\n",
      "        op         = <needle.ops.ops_mathematic.Dilate object at 0x7fd1dfa2e810>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fd1dfa2dfd0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fd1dfa2dfd0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Dilate object at 0x7fd1dfa2e810>\n",
      "a = NDArray([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572 ]\n",
      "   [... 0.2799246  -0.09815039  0.9101789   0.3172182 ]\n",
      "   [ 0.78632796 -0.4664191  -0.94444627 -0.4100497 ]]]], device=cpu())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "a          = NDArray([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572 ]\n",
      "   [... 0.2799246  -0.09815039  0.9101789   0.3172182 ]\n",
      "   [ 0.78632796 -0.4664191  -0.94444627 -0.4100497 ]]]], device=cpu())\n",
      "self       = <needle.ops.ops_mathematic.Dilate object at 0x7fd1dfa2e810>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:464: NotImplementedError\n",
      "\u001b[31m\u001b[1m__ test_dilate_backward[params8-needle.backend_ndarray.ndarray_backend_cuda] ___\u001b[0m\n",
      "\n",
      "params = {'axes': (1, 2), 'd': 1, 'shape': (3, 3, 6, 4)}, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, dilate_backward_params)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_dilate_backward\u001b[39;49;00m(params, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        shape, d, axes = params[\u001b[33m'\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      ">       backward_check(ndl.dilate, ndl.Tensor(np.random.randn(*shape), device=device), dilation=d, axes=axes)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "axes       = (1, 2)\n",
      "d          = 1\n",
      "device     = cuda()\n",
      "params     = {'axes': (1, 2), 'd': 1, 'shape': (3, 3, 6, 4)}\n",
      "shape      = (3, 3, 6, 4)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:296: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:16: in backward_check\n",
      "    \u001b[0mout = f(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572...80309 ]\n",
      "   [ 0.2799246  -0.09815039  0.9101789   0.3172182 ]\n",
      "   [ 0.78632796 -0.4664191  -0.94444627 -0.4100497 ]]]]),)\n",
      "        eps        = 0.001\n",
      "        f          = <function dilate at 0x7fd29ce4d080>\n",
      "        kwargs     = {'axes': (1, 2), 'dilation': 1}\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:474: in dilate\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Dilate(axes, dilation)(a)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572 ...7380309 ]\n",
      "   [ 0.2799246  -0.09815039  0.9101789   0.3172182 ]\n",
      "   [ 0.78632796 -0.4664191  -0.94444627 -0.4100497 ]]]])\n",
      "        axes       = (1, 2)\n",
      "        dilation   = 1\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572...80309 ]\n",
      "   [ 0.2799246  -0.09815039  0.9101789   0.3172182 ]\n",
      "   [ 0.78632796 -0.4664191  -0.94444627 -0.4100497 ]]]]),)\n",
      "        self       = <needle.ops.ops_mathematic.Dilate object at 0x7fd1dfabb990>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572...80309 ]\n",
      "   [ 0.2799246  -0.09815039  0.9101789   0.3172182 ]\n",
      "   [ 0.78632796 -0.4664191  -0.94444627 -0.4100497 ]]]]),)\n",
      "        op         = <needle.ops.ops_mathematic.Dilate object at 0x7fd1dfabb990>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fd1dfab8690>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fd1dfab8690>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Dilate object at 0x7fd1dfabb990>\n",
      "a = NDArray([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572 ]\n",
      "   [...0.2799246  -0.09815039  0.9101789   0.3172182 ]\n",
      "   [ 0.78632796 -0.4664191  -0.94444627 -0.4100497 ]]]], device=cuda())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "a          = NDArray([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572 ]\n",
      "   [...0.2799246  -0.09815039  0.9101789   0.3172182 ]\n",
      "   [ 0.78632796 -0.4664191  -0.94444627 -0.4100497 ]]]], device=cuda())\n",
      "self       = <needle.ops.ops_mathematic.Dilate object at 0x7fd1dfabb990>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:464: NotImplementedError\n",
      "\u001b[31m\u001b[1m___ test_dilate_backward[params9-needle.backend_ndarray.ndarray_backend_cpu] ___\u001b[0m\n",
      "\n",
      "params = {'axes': (2, 3), 'd': 1, 'shape': (2, 3, 3, 4)}, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, dilate_backward_params)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_dilate_backward\u001b[39;49;00m(params, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        shape, d, axes = params[\u001b[33m'\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      ">       backward_check(ndl.dilate, ndl.Tensor(np.random.randn(*shape), device=device), dilation=d, axes=axes)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "axes       = (2, 3)\n",
      "d          = 1\n",
      "device     = cpu()\n",
      "params     = {'axes': (2, 3), 'd': 1, 'shape': (2, 3, 3, 4)}\n",
      "shape      = (2, 3, 3, 4)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:296: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:16: in backward_check\n",
      "    \u001b[0mout = f(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572...62826 ]\n",
      "   [ 0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]]),)\n",
      "        eps        = 0.001\n",
      "        f          = <function dilate at 0x7fd29ce4d080>\n",
      "        kwargs     = {'axes': (2, 3), 'dilation': 1}\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:474: in dilate\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Dilate(axes, dilation)(a)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572 ...7262826 ]\n",
      "   [ 0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]])\n",
      "        axes       = (2, 3)\n",
      "        dilation   = 1\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572...62826 ]\n",
      "   [ 0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]]),)\n",
      "        self       = <needle.ops.ops_mathematic.Dilate object at 0x7fd1dfb04750>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572...62826 ]\n",
      "   [ 0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]]),)\n",
      "        op         = <needle.ops.ops_mathematic.Dilate object at 0x7fd1dfb04750>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fd1dfb06690>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fd1dfb06690>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Dilate object at 0x7fd1dfb04750>\n",
      "a = NDArray([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572 ]\n",
      "   [... 0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]], device=cpu())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "a          = NDArray([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572 ]\n",
      "   [... 0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]], device=cpu())\n",
      "self       = <needle.ops.ops_mathematic.Dilate object at 0x7fd1dfb04750>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:464: NotImplementedError\n",
      "\u001b[31m\u001b[1m__ test_dilate_backward[params9-needle.backend_ndarray.ndarray_backend_cuda] ___\u001b[0m\n",
      "\n",
      "params = {'axes': (2, 3), 'd': 1, 'shape': (2, 3, 3, 4)}, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, dilate_backward_params)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_dilate_backward\u001b[39;49;00m(params, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        shape, d, axes = params[\u001b[33m'\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      ">       backward_check(ndl.dilate, ndl.Tensor(np.random.randn(*shape), device=device), dilation=d, axes=axes)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "axes       = (2, 3)\n",
      "d          = 1\n",
      "device     = cuda()\n",
      "params     = {'axes': (2, 3), 'd': 1, 'shape': (2, 3, 3, 4)}\n",
      "shape      = (2, 3, 3, 4)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:296: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:16: in backward_check\n",
      "    \u001b[0mout = f(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572...62826 ]\n",
      "   [ 0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]]),)\n",
      "        eps        = 0.001\n",
      "        f          = <function dilate at 0x7fd29ce4d080>\n",
      "        kwargs     = {'axes': (2, 3), 'dilation': 1}\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:474: in dilate\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Dilate(axes, dilation)(a)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572 ...7262826 ]\n",
      "   [ 0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]])\n",
      "        axes       = (2, 3)\n",
      "        dilation   = 1\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572...62826 ]\n",
      "   [ 0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]]),)\n",
      "        self       = <needle.ops.ops_mathematic.Dilate object at 0x7fd1dfbaaa10>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572...62826 ]\n",
      "   [ 0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]]),)\n",
      "        op         = <needle.ops.ops_mathematic.Dilate object at 0x7fd1dfbaaa10>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fd1dfbaab10>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fd1dfbaab10>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Dilate object at 0x7fd1dfbaaa10>\n",
      "a = NDArray([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572 ]\n",
      "   [...0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]], device=cuda())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "a          = NDArray([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572 ]\n",
      "   [...0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]], device=cuda())\n",
      "self       = <needle.ops.ops_mathematic.Dilate object at 0x7fd1dfbaaa10>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:464: NotImplementedError\n",
      "\u001b[31m\u001b[1m__ test_dilate_backward[params10-needle.backend_ndarray.ndarray_backend_cpu] ___\u001b[0m\n",
      "\n",
      "params = {'axes': (2, 3), 'd': 1, 'shape': (3, 3, 6, 4)}, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, dilate_backward_params)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_dilate_backward\u001b[39;49;00m(params, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        shape, d, axes = params[\u001b[33m'\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      ">       backward_check(ndl.dilate, ndl.Tensor(np.random.randn(*shape), device=device), dilation=d, axes=axes)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "axes       = (2, 3)\n",
      "d          = 1\n",
      "device     = cpu()\n",
      "params     = {'axes': (2, 3), 'd': 1, 'shape': (3, 3, 6, 4)}\n",
      "shape      = (3, 3, 6, 4)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:296: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:16: in backward_check\n",
      "    \u001b[0mout = f(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572...80309 ]\n",
      "   [ 0.2799246  -0.09815039  0.9101789   0.3172182 ]\n",
      "   [ 0.78632796 -0.4664191  -0.94444627 -0.4100497 ]]]]),)\n",
      "        eps        = 0.001\n",
      "        f          = <function dilate at 0x7fd29ce4d080>\n",
      "        kwargs     = {'axes': (2, 3), 'dilation': 1}\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:474: in dilate\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Dilate(axes, dilation)(a)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572 ...7380309 ]\n",
      "   [ 0.2799246  -0.09815039  0.9101789   0.3172182 ]\n",
      "   [ 0.78632796 -0.4664191  -0.94444627 -0.4100497 ]]]])\n",
      "        axes       = (2, 3)\n",
      "        dilation   = 1\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572...80309 ]\n",
      "   [ 0.2799246  -0.09815039  0.9101789   0.3172182 ]\n",
      "   [ 0.78632796 -0.4664191  -0.94444627 -0.4100497 ]]]]),)\n",
      "        self       = <needle.ops.ops_mathematic.Dilate object at 0x7fd1dfa894d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572...80309 ]\n",
      "   [ 0.2799246  -0.09815039  0.9101789   0.3172182 ]\n",
      "   [ 0.78632796 -0.4664191  -0.94444627 -0.4100497 ]]]]),)\n",
      "        op         = <needle.ops.ops_mathematic.Dilate object at 0x7fd1dfa894d0>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fd1dfa88610>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fd1dfa88610>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Dilate object at 0x7fd1dfa894d0>\n",
      "a = NDArray([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572 ]\n",
      "   [... 0.2799246  -0.09815039  0.9101789   0.3172182 ]\n",
      "   [ 0.78632796 -0.4664191  -0.94444627 -0.4100497 ]]]], device=cpu())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "a          = NDArray([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572 ]\n",
      "   [... 0.2799246  -0.09815039  0.9101789   0.3172182 ]\n",
      "   [ 0.78632796 -0.4664191  -0.94444627 -0.4100497 ]]]], device=cpu())\n",
      "self       = <needle.ops.ops_mathematic.Dilate object at 0x7fd1dfa894d0>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:464: NotImplementedError\n",
      "\u001b[31m\u001b[1m__ test_dilate_backward[params10-needle.backend_ndarray.ndarray_backend_cuda] __\u001b[0m\n",
      "\n",
      "params = {'axes': (2, 3), 'd': 1, 'shape': (3, 3, 6, 4)}, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, dilate_backward_params)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_dilate_backward\u001b[39;49;00m(params, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        shape, d, axes = params[\u001b[33m'\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      ">       backward_check(ndl.dilate, ndl.Tensor(np.random.randn(*shape), device=device), dilation=d, axes=axes)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "axes       = (2, 3)\n",
      "d          = 1\n",
      "device     = cuda()\n",
      "params     = {'axes': (2, 3), 'd': 1, 'shape': (3, 3, 6, 4)}\n",
      "shape      = (3, 3, 6, 4)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:296: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:16: in backward_check\n",
      "    \u001b[0mout = f(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572...80309 ]\n",
      "   [ 0.2799246  -0.09815039  0.9101789   0.3172182 ]\n",
      "   [ 0.78632796 -0.4664191  -0.94444627 -0.4100497 ]]]]),)\n",
      "        eps        = 0.001\n",
      "        f          = <function dilate at 0x7fd29ce4d080>\n",
      "        kwargs     = {'axes': (2, 3), 'dilation': 1}\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:474: in dilate\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Dilate(axes, dilation)(a)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572 ...7380309 ]\n",
      "   [ 0.2799246  -0.09815039  0.9101789   0.3172182 ]\n",
      "   [ 0.78632796 -0.4664191  -0.94444627 -0.4100497 ]]]])\n",
      "        axes       = (2, 3)\n",
      "        dilation   = 1\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572...80309 ]\n",
      "   [ 0.2799246  -0.09815039  0.9101789   0.3172182 ]\n",
      "   [ 0.78632796 -0.4664191  -0.94444627 -0.4100497 ]]]]),)\n",
      "        self       = <needle.ops.ops_mathematic.Dilate object at 0x7fd1dfb95450>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572...80309 ]\n",
      "   [ 0.2799246  -0.09815039  0.9101789   0.3172182 ]\n",
      "   [ 0.78632796 -0.4664191  -0.94444627 -0.4100497 ]]]]),)\n",
      "        op         = <needle.ops.ops_mathematic.Dilate object at 0x7fd1dfb95450>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fd1dfb95b10>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fd1dfb95b10>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Dilate object at 0x7fd1dfb95450>\n",
      "a = NDArray([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572 ]\n",
      "   [...0.2799246  -0.09815039  0.9101789   0.3172182 ]\n",
      "   [ 0.78632796 -0.4664191  -0.94444627 -0.4100497 ]]]], device=cuda())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "a          = NDArray([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572 ]\n",
      "   [...0.2799246  -0.09815039  0.9101789   0.3172182 ]\n",
      "   [ 0.78632796 -0.4664191  -0.94444627 -0.4100497 ]]]], device=cuda())\n",
      "self       = <needle.ops.ops_mathematic.Dilate object at 0x7fd1dfb95450>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:464: NotImplementedError\n",
      "\u001b[31m\u001b[1m__ test_dilate_backward[params11-needle.backend_ndarray.ndarray_backend_cpu] ___\u001b[0m\n",
      "\n",
      "params = {'axes': (0, 1, 2, 3), 'd': 1, 'shape': (2, 3, 3, 4)}, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, dilate_backward_params)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_dilate_backward\u001b[39;49;00m(params, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        shape, d, axes = params[\u001b[33m'\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      ">       backward_check(ndl.dilate, ndl.Tensor(np.random.randn(*shape), device=device), dilation=d, axes=axes)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "axes       = (0, 1, 2, 3)\n",
      "d          = 1\n",
      "device     = cpu()\n",
      "params     = {'axes': (0, 1, 2, 3), 'd': 1, 'shape': (2, 3, 3, 4)}\n",
      "shape      = (2, 3, 3, 4)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:296: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:16: in backward_check\n",
      "    \u001b[0mout = f(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572...62826 ]\n",
      "   [ 0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]]),)\n",
      "        eps        = 0.001\n",
      "        f          = <function dilate at 0x7fd29ce4d080>\n",
      "        kwargs     = {'axes': (0, 1, 2, 3), 'dilation': 1}\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:474: in dilate\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Dilate(axes, dilation)(a)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572 ...7262826 ]\n",
      "   [ 0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]])\n",
      "        axes       = (0, 1, 2, 3)\n",
      "        dilation   = 1\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572...62826 ]\n",
      "   [ 0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]]),)\n",
      "        self       = <needle.ops.ops_mathematic.Dilate object at 0x7fd1df8bbe90>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572...62826 ]\n",
      "   [ 0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]]),)\n",
      "        op         = <needle.ops.ops_mathematic.Dilate object at 0x7fd1df8bbe90>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fd1df8ba750>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fd1df8ba750>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Dilate object at 0x7fd1df8bbe90>\n",
      "a = NDArray([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572 ]\n",
      "   [... 0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]], device=cpu())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "a          = NDArray([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572 ]\n",
      "   [... 0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]], device=cpu())\n",
      "self       = <needle.ops.ops_mathematic.Dilate object at 0x7fd1df8bbe90>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:464: NotImplementedError\n",
      "\u001b[31m\u001b[1m__ test_dilate_backward[params11-needle.backend_ndarray.ndarray_backend_cuda] __\u001b[0m\n",
      "\n",
      "params = {'axes': (0, 1, 2, 3), 'd': 1, 'shape': (2, 3, 3, 4)}, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, dilate_backward_params)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_dilate_backward\u001b[39;49;00m(params, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        shape, d, axes = params[\u001b[33m'\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      ">       backward_check(ndl.dilate, ndl.Tensor(np.random.randn(*shape), device=device), dilation=d, axes=axes)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "axes       = (0, 1, 2, 3)\n",
      "d          = 1\n",
      "device     = cuda()\n",
      "params     = {'axes': (0, 1, 2, 3), 'd': 1, 'shape': (2, 3, 3, 4)}\n",
      "shape      = (2, 3, 3, 4)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:296: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:16: in backward_check\n",
      "    \u001b[0mout = f(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572...62826 ]\n",
      "   [ 0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]]),)\n",
      "        eps        = 0.001\n",
      "        f          = <function dilate at 0x7fd29ce4d080>\n",
      "        kwargs     = {'axes': (0, 1, 2, 3), 'dilation': 1}\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:474: in dilate\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Dilate(axes, dilation)(a)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572 ...7262826 ]\n",
      "   [ 0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]])\n",
      "        axes       = (0, 1, 2, 3)\n",
      "        dilation   = 1\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572...62826 ]\n",
      "   [ 0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]]),)\n",
      "        self       = <needle.ops.ops_mathematic.Dilate object at 0x7fd1dfba2850>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572...62826 ]\n",
      "   [ 0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]]),)\n",
      "        op         = <needle.ops.ops_mathematic.Dilate object at 0x7fd1dfba2850>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fd1dfba2f90>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fd1dfba2f90>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Dilate object at 0x7fd1dfba2850>\n",
      "a = NDArray([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572 ]\n",
      "   [...0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]], device=cuda())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "a          = NDArray([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572 ]\n",
      "   [...0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]], device=cuda())\n",
      "self       = <needle.ops.ops_mathematic.Dilate object at 0x7fd1dfba2850>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:464: NotImplementedError\n",
      "\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_dilate_forward[needle.backend_ndarray.ndarray_backend_cpu]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_dilate_forward[needle.backend_ndarray.ndarray_backend_cuda]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_dilate_backward[params0-needle.backend_ndarray.ndarray_backend_cpu]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_dilate_backward[params0-needle.backend_ndarray.ndarray_backend_cuda]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_dilate_backward[params1-needle.backend_ndarray.ndarray_backend_cpu]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_dilate_backward[params1-needle.backend_ndarray.ndarray_backend_cuda]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_dilate_backward[params2-needle.backend_ndarray.ndarray_backend_cpu]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_dilate_backward[params2-needle.backend_ndarray.ndarray_backend_cuda]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_dilate_backward[params3-needle.backend_ndarray.ndarray_backend_cpu]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_dilate_backward[params3-needle.backend_ndarray.ndarray_backend_cuda]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_dilate_backward[params4-needle.backend_ndarray.ndarray_backend_cpu]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_dilate_backward[params4-needle.backend_ndarray.ndarray_backend_cuda]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_dilate_backward[params5-needle.backend_ndarray.ndarray_backend_cpu]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_dilate_backward[params5-needle.backend_ndarray.ndarray_backend_cuda]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_dilate_backward[params6-needle.backend_ndarray.ndarray_backend_cpu]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_dilate_backward[params6-needle.backend_ndarray.ndarray_backend_cuda]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_dilate_backward[params7-needle.backend_ndarray.ndarray_backend_cpu]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_dilate_backward[params7-needle.backend_ndarray.ndarray_backend_cuda]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_dilate_backward[params8-needle.backend_ndarray.ndarray_backend_cpu]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_dilate_backward[params8-needle.backend_ndarray.ndarray_backend_cuda]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_dilate_backward[params9-needle.backend_ndarray.ndarray_backend_cpu]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_dilate_backward[params9-needle.backend_ndarray.ndarray_backend_cuda]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_dilate_backward[params10-needle.backend_ndarray.ndarray_backend_cpu]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_dilate_backward[params10-needle.backend_ndarray.ndarray_backend_cuda]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_dilate_backward[params11-needle.backend_ndarray.ndarray_backend_cpu]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_dilate_backward[params11-needle.backend_ndarray.ndarray_backend_cuda]\u001b[0m - NotImplementedError\n",
      "\u001b[31m===================== \u001b[31m\u001b[1m26 failed\u001b[0m, \u001b[33m1777 deselected\u001b[0m\u001b[31m in 4.08s\u001b[0m\u001b[31m ======================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m pytest -l -v -k \"dilate\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit new ops (flip/dilation) to mugrade [10 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution forward\n",
    "\n",
    "Implement the forward pass of 2D multi-channel convolution in `ops_mathematic.py`. You should probably refer to [this notebook](https://github.com/dlsyscourse/public_notebooks/blob/main/convolution_implementation.ipynb) from lecture, which implements 2D multi-channel convolution using im2col in numpy.\n",
    "\n",
    "**Note:** Your convolution op should accept tensors in the NHWC format, as in the example above, and weights in the format (kernel_size, kernel_size, input_channels, output_channels).\n",
    "\n",
    "However, you will need to add two additional features. Your convolution function should accept arguments for `padding` (default 0) and `stride` (default 1). For `padding`, you should simply apply your padding function to the spatial dimensions (i.e., axes 1 and 2). \n",
    "\n",
    "Implementing strided convolution should consist of a relatively small set of changes to your plain convolution implementation.\n",
    "\n",
    "We recommend working your way up through the full feature set: Implement convolution without stride first, ensuring you pass some of the tests below, and then add in support for stride."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.11.10, pytest-8.3.4, pluggy-1.5.0 -- /opt/conda/envs/train/bin/python\n",
      "cachedir: .pytest_cache\n",
      "hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase(PosixPath('/mnt/zhumingzhu/work/00test/hw4/.hypothesis/examples'))\n",
      "rootdir: /mnt/zhumingzhu/work/00test/hw4\n",
      "plugins: hypothesis-6.115.5\n",
      "collected 1803 items / 1769 deselected / 34 selected                           \u001b[0m\u001b[1m\n",
      "\n",
      "tests/hw4/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape0-W_shape0-1-0] \u001b[31mFAILED\u001b[0m\u001b[31m [  2%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape1-W_shape1-1-1] \u001b[31mFAILED\u001b[0m\u001b[31m [  5%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape2-W_shape2-1-2] \u001b[31mFAILED\u001b[0m\u001b[31m [  8%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape3-W_shape3-1-0] \u001b[31mFAILED\u001b[0m\u001b[31m [ 11%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape4-W_shape4-1-0] \u001b[31mFAILED\u001b[0m\u001b[31m [ 14%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape5-W_shape5-2-0] \u001b[31mFAILED\u001b[0m\u001b[31m [ 17%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape6-W_shape6-2-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 20%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape7-W_shape7-2-2] \u001b[31mFAILED\u001b[0m\u001b[31m [ 23%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape8-W_shape8-2-0] \u001b[31mFAILED\u001b[0m\u001b[31m [ 26%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape9-W_shape9-2-0] \u001b[31mFAILED\u001b[0m\u001b[31m [ 29%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape10-W_shape10-1-0] \u001b[31mFAILED\u001b[0m\u001b[31m [ 32%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape11-W_shape11-1-0] \u001b[31mFAILED\u001b[0m\u001b[31m [ 35%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape12-W_shape12-1-0] \u001b[31mFAILED\u001b[0m\u001b[31m [ 38%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape13-W_shape13-1-0] \u001b[31mFAILED\u001b[0m\u001b[31m [ 41%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape14-W_shape14-1-0] \u001b[31mFAILED\u001b[0m\u001b[31m [ 44%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape15-W_shape15-1-0] \u001b[31mFAILED\u001b[0m\u001b[31m [ 47%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape16-W_shape16-1-0] \u001b[31mFAILED\u001b[0m\u001b[31m [ 50%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape0-W_shape0-1-0] \u001b[31mFAILED\u001b[0m\u001b[31m [ 52%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape1-W_shape1-1-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 55%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape2-W_shape2-1-2] \u001b[31mFAILED\u001b[0m\u001b[31m [ 58%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape3-W_shape3-1-0] \u001b[31mFAILED\u001b[0m\u001b[31m [ 61%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape4-W_shape4-1-0] \u001b[31mFAILED\u001b[0m\u001b[31m [ 64%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape5-W_shape5-2-0] \u001b[31mFAILED\u001b[0m\u001b[31m [ 67%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape6-W_shape6-2-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 70%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape7-W_shape7-2-2] \u001b[31mFAILED\u001b[0m\u001b[31m [ 73%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape8-W_shape8-2-0] \u001b[31mFAILED\u001b[0m\u001b[31m [ 76%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape9-W_shape9-2-0] \u001b[31mFAILED\u001b[0m\u001b[31m [ 79%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape10-W_shape10-1-0] \u001b[31mFAILED\u001b[0m\u001b[31m [ 82%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape11-W_shape11-1-0] \u001b[31mFAILED\u001b[0m\u001b[31m [ 85%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape12-W_shape12-1-0] \u001b[31mFAILED\u001b[0m\u001b[31m [ 88%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape13-W_shape13-1-0] \u001b[31mFAILED\u001b[0m\u001b[31m [ 91%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape14-W_shape14-1-0] \u001b[31mFAILED\u001b[0m\u001b[31m [ 94%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape15-W_shape15-1-0] \u001b[31mFAILED\u001b[0m\u001b[31m [ 97%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape16-W_shape16-1-0] \u001b[31mFAILED\u001b[0m\u001b[31m [100%]\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m_ test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape0-W_shape0-1-0] _\u001b[0m\n",
      "\n",
      "Z_shape = (3, 14, 14, 8), W_shape = (3, 3, 8, 16), stride = 1, padding = 0\n",
      "backward = False, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mimport\u001b[39;49;00m \u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "W          = needle.Tensor([[[[  4.282669     8.133604    -4.6223483  ...   2.0798197\n",
      "      2.1704152   -0.38803983]\n",
      "   [  2.245844...2.618842     0.71487164]\n",
      "   [ -4.09374      3.7291534  -10.551835   ...   0.31741843\n",
      "      0.03223436   2.0512383 ]]]])\n",
      "W_shape    = (3, 3, 8, 16)\n",
      "Z          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e...3e+00]\n",
      "   [ 1.09905624e+01  2.23166728e+00  4.62055349e+00 ... -6.23090088e-01\n",
      "    -8.48226726e-01 -2.29984760e+00]]]])\n",
      "Z_shape    = (3, 14, 14, 8)\n",
      "_W         = array([[[[  4.282669  ,   8.133604  ,  -4.6223483 , ...,   2.0798197 ,\n",
      "            2.1704152 ,  -0.38803983],\n",
      "        ... [ -4.09374   ,   3.7291534 , -10.551835  , ...,   0.31741843,\n",
      "            0.03223436,   2.0512383 ]]]], dtype=float32)\n",
      "_Z         = array([[[[ 8.82026196e+00,  2.00078607e+00,  4.89369011e+00, ...,\n",
      "          -4.88638926e+00,  4.75044203e+00, -7.56786...166728e+00,  4.62055349e+00, ...,\n",
      "          -6.23090088e-01, -8.48226726e-01, -2.29984760e+00]]]],\n",
      "      dtype=float32)\n",
      "backward   = False\n",
      "device     = cpu()\n",
      "padding    = 0\n",
      "stride     = 1\n",
      "torch      = <module 'torch' from '/opt/conda/envs/train/lib/python3.11/site-packages/torch/__init__.py'>\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:429: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:514: in conv\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Conv(stride, padding)(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e...3e+00]\n",
      "   [ 1.09905624e+01  2.23166728e+00  4.62055349e+00 ... -6.23090088e-01\n",
      "    -8.48226726e-01 -2.29984760e+00]]]])\n",
      "        b          = needle.Tensor([[[[  4.282669     8.133604    -4.6223483  ...   2.0798197\n",
      "      2.1704152   -0.38803983]\n",
      "   [  2.245844...2.618842     0.71487164]\n",
      "   [ -4.09374      3.7291534  -10.551835   ...   0.31741843\n",
      "      0.03223436   2.0512383 ]]]])\n",
      "        padding    = 0\n",
      "        stride     = 1\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048....618842     0.71487164]\n",
      "   [ -4.09374      3.7291534  -10.551835   ...   0.31741843\n",
      "      0.03223436   2.0512383 ]]]]))\n",
      "        self       = <needle.ops.ops_mathematic.Conv object at 0x7fb2dbd69450>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048....618842     0.71487164]\n",
      "   [ -4.09374      3.7291534  -10.551835   ...   0.31741843\n",
      "      0.03223436   2.0512383 ]]]]))\n",
      "        op         = <needle.ops.ops_mathematic.Conv object at 0x7fb2dbd69450>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fb7db445110>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fb7db445110>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Conv object at 0x7fb2dbd69450>\n",
      "A = NDArray([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e-01]\n",
      " ...09905624e+01  2.23166728e+00  4.62055349e+00 ... -6.23090088e-01\n",
      "    -8.48226726e-01 -2.29984760e+00]]]], device=cpu())\n",
      "B = NDArray([[[[  4.282669     8.133604    -4.6223483  ...   2.0798197\n",
      "      2.1704152   -0.38803983]\n",
      "   [  2.2458448   -0....71487164]\n",
      "   [ -4.09374      3.7291534  -10.551835   ...   0.31741843\n",
      "      0.03223436   2.0512383 ]]]], device=cpu())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, A, B):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "A          = NDArray([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e-01]\n",
      " ...09905624e+01  2.23166728e+00  4.62055349e+00 ... -6.23090088e-01\n",
      "    -8.48226726e-01 -2.29984760e+00]]]], device=cpu())\n",
      "B          = NDArray([[[[  4.282669     8.133604    -4.6223483  ...   2.0798197\n",
      "      2.1704152   -0.38803983]\n",
      "   [  2.2458448   -0....71487164]\n",
      "   [ -4.09374      3.7291534  -10.551835   ...   0.31741843\n",
      "      0.03223436   2.0512383 ]]]], device=cpu())\n",
      "self       = <needle.ops.ops_mathematic.Conv object at 0x7fb2dbd69450>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:504: NotImplementedError\n",
      "\u001b[31m\u001b[1m_ test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape1-W_shape1-1-1] _\u001b[0m\n",
      "\n",
      "Z_shape = (3, 14, 14, 8), W_shape = (3, 3, 8, 16), stride = 1, padding = 1\n",
      "backward = False, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mimport\u001b[39;49;00m \u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "W          = needle.Tensor([[[[  4.282669     8.133604    -4.6223483  ...   2.0798197\n",
      "      2.1704152   -0.38803983]\n",
      "   [  2.245844...2.618842     0.71487164]\n",
      "   [ -4.09374      3.7291534  -10.551835   ...   0.31741843\n",
      "      0.03223436   2.0512383 ]]]])\n",
      "W_shape    = (3, 3, 8, 16)\n",
      "Z          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e...3e+00]\n",
      "   [ 1.09905624e+01  2.23166728e+00  4.62055349e+00 ... -6.23090088e-01\n",
      "    -8.48226726e-01 -2.29984760e+00]]]])\n",
      "Z_shape    = (3, 14, 14, 8)\n",
      "_W         = array([[[[  4.282669  ,   8.133604  ,  -4.6223483 , ...,   2.0798197 ,\n",
      "            2.1704152 ,  -0.38803983],\n",
      "        ... [ -4.09374   ,   3.7291534 , -10.551835  , ...,   0.31741843,\n",
      "            0.03223436,   2.0512383 ]]]], dtype=float32)\n",
      "_Z         = array([[[[ 8.82026196e+00,  2.00078607e+00,  4.89369011e+00, ...,\n",
      "          -4.88638926e+00,  4.75044203e+00, -7.56786...166728e+00,  4.62055349e+00, ...,\n",
      "          -6.23090088e-01, -8.48226726e-01, -2.29984760e+00]]]],\n",
      "      dtype=float32)\n",
      "backward   = False\n",
      "device     = cpu()\n",
      "padding    = 1\n",
      "stride     = 1\n",
      "torch      = <module 'torch' from '/opt/conda/envs/train/lib/python3.11/site-packages/torch/__init__.py'>\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:429: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:514: in conv\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Conv(stride, padding)(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e...3e+00]\n",
      "   [ 1.09905624e+01  2.23166728e+00  4.62055349e+00 ... -6.23090088e-01\n",
      "    -8.48226726e-01 -2.29984760e+00]]]])\n",
      "        b          = needle.Tensor([[[[  4.282669     8.133604    -4.6223483  ...   2.0798197\n",
      "      2.1704152   -0.38803983]\n",
      "   [  2.245844...2.618842     0.71487164]\n",
      "   [ -4.09374      3.7291534  -10.551835   ...   0.31741843\n",
      "      0.03223436   2.0512383 ]]]])\n",
      "        padding    = 1\n",
      "        stride     = 1\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048....618842     0.71487164]\n",
      "   [ -4.09374      3.7291534  -10.551835   ...   0.31741843\n",
      "      0.03223436   2.0512383 ]]]]))\n",
      "        self       = <needle.ops.ops_mathematic.Conv object at 0x7fb2daedcbd0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048....618842     0.71487164]\n",
      "   [ -4.09374      3.7291534  -10.551835   ...   0.31741843\n",
      "      0.03223436   2.0512383 ]]]]))\n",
      "        op         = <needle.ops.ops_mathematic.Conv object at 0x7fb2daedcbd0>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fb2daedc750>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fb2daedc750>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Conv object at 0x7fb2daedcbd0>\n",
      "A = NDArray([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e-01]\n",
      " ...09905624e+01  2.23166728e+00  4.62055349e+00 ... -6.23090088e-01\n",
      "    -8.48226726e-01 -2.29984760e+00]]]], device=cpu())\n",
      "B = NDArray([[[[  4.282669     8.133604    -4.6223483  ...   2.0798197\n",
      "      2.1704152   -0.38803983]\n",
      "   [  2.2458448   -0....71487164]\n",
      "   [ -4.09374      3.7291534  -10.551835   ...   0.31741843\n",
      "      0.03223436   2.0512383 ]]]], device=cpu())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, A, B):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "A          = NDArray([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e-01]\n",
      " ...09905624e+01  2.23166728e+00  4.62055349e+00 ... -6.23090088e-01\n",
      "    -8.48226726e-01 -2.29984760e+00]]]], device=cpu())\n",
      "B          = NDArray([[[[  4.282669     8.133604    -4.6223483  ...   2.0798197\n",
      "      2.1704152   -0.38803983]\n",
      "   [  2.2458448   -0....71487164]\n",
      "   [ -4.09374      3.7291534  -10.551835   ...   0.31741843\n",
      "      0.03223436   2.0512383 ]]]], device=cpu())\n",
      "self       = <needle.ops.ops_mathematic.Conv object at 0x7fb2daedcbd0>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:504: NotImplementedError\n",
      "\u001b[31m\u001b[1m_ test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape2-W_shape2-1-2] _\u001b[0m\n",
      "\n",
      "Z_shape = (3, 16, 16, 8), W_shape = (3, 3, 8, 16), stride = 1, padding = 2\n",
      "backward = False, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mimport\u001b[39;49;00m \u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "W          = needle.Tensor([[[[  0.40910086  -0.64982927  11.320294   ...  -0.12077556\n",
      "      1.428692    -5.5845156 ]\n",
      "   [ -6.89178...-1.5437248  -10.136281  ]\n",
      "   [  1.1511405    0.7441038   16.463472   ...  -6.8931007\n",
      "     -0.5833115   -2.504636  ]]]])\n",
      "W_shape    = (3, 3, 8, 16)\n",
      "Z          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e...5e+00]\n",
      "   [ 6.10367346e+00 -2.05434513e+00 -4.41384506e+00 ... -6.15245628e+00\n",
      "     3.33514667e+00 -1.35630560e+00]]]])\n",
      "Z_shape    = (3, 16, 16, 8)\n",
      "_W         = array([[[[  0.40910086,  -0.64982927,  11.320294  , ...,  -0.12077556,\n",
      "            1.428692  ,  -5.5845156 ],\n",
      "        ... [  1.1511405 ,   0.7441038 ,  16.463472  , ...,  -6.8931007 ,\n",
      "           -0.5833115 ,  -2.504636  ]]]], dtype=float32)\n",
      "_Z         = array([[[[ 8.82026196e+00,  2.00078607e+00,  4.89369011e+00, ...,\n",
      "          -4.88638926e+00,  4.75044203e+00, -7.56786...434513e+00, -4.41384506e+00, ...,\n",
      "          -6.15245628e+00,  3.33514667e+00, -1.35630560e+00]]]],\n",
      "      dtype=float32)\n",
      "backward   = False\n",
      "device     = cpu()\n",
      "padding    = 2\n",
      "stride     = 1\n",
      "torch      = <module 'torch' from '/opt/conda/envs/train/lib/python3.11/site-packages/torch/__init__.py'>\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:429: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:514: in conv\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Conv(stride, padding)(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e...5e+00]\n",
      "   [ 6.10367346e+00 -2.05434513e+00 -4.41384506e+00 ... -6.15245628e+00\n",
      "     3.33514667e+00 -1.35630560e+00]]]])\n",
      "        b          = needle.Tensor([[[[  0.40910086  -0.64982927  11.320294   ...  -0.12077556\n",
      "      1.428692    -5.5845156 ]\n",
      "   [ -6.89178...-1.5437248  -10.136281  ]\n",
      "   [  1.1511405    0.7441038   16.463472   ...  -6.8931007\n",
      "     -0.5833115   -2.504636  ]]]])\n",
      "        padding    = 2\n",
      "        stride     = 1\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048...1.5437248  -10.136281  ]\n",
      "   [  1.1511405    0.7441038   16.463472   ...  -6.8931007\n",
      "     -0.5833115   -2.504636  ]]]]))\n",
      "        self       = <needle.ops.ops_mathematic.Conv object at 0x7fb2db0139d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048...1.5437248  -10.136281  ]\n",
      "   [  1.1511405    0.7441038   16.463472   ...  -6.8931007\n",
      "     -0.5833115   -2.504636  ]]]]))\n",
      "        op         = <needle.ops.ops_mathematic.Conv object at 0x7fb2db0139d0>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fb2db0136d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fb2db0136d0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Conv object at 0x7fb2db0139d0>\n",
      "A = NDArray([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e-01]\n",
      " ...10367346e+00 -2.05434513e+00 -4.41384506e+00 ... -6.15245628e+00\n",
      "     3.33514667e+00 -1.35630560e+00]]]], device=cpu())\n",
      "B = NDArray([[[[  0.40910086  -0.64982927  11.320294   ...  -0.12077556\n",
      "      1.428692    -5.5845156 ]\n",
      "   [ -6.891782    -...0.136281  ]\n",
      "   [  1.1511405    0.7441038   16.463472   ...  -6.8931007\n",
      "     -0.5833115   -2.504636  ]]]], device=cpu())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, A, B):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "A          = NDArray([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e-01]\n",
      " ...10367346e+00 -2.05434513e+00 -4.41384506e+00 ... -6.15245628e+00\n",
      "     3.33514667e+00 -1.35630560e+00]]]], device=cpu())\n",
      "B          = NDArray([[[[  0.40910086  -0.64982927  11.320294   ...  -0.12077556\n",
      "      1.428692    -5.5845156 ]\n",
      "   [ -6.891782    -...0.136281  ]\n",
      "   [  1.1511405    0.7441038   16.463472   ...  -6.8931007\n",
      "     -0.5833115   -2.504636  ]]]], device=cpu())\n",
      "self       = <needle.ops.ops_mathematic.Conv object at 0x7fb2db0139d0>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:504: NotImplementedError\n",
      "\u001b[31m\u001b[1m_ test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape3-W_shape3-1-0] _\u001b[0m\n",
      "\n",
      "Z_shape = (3, 16, 16, 8), W_shape = (3, 3, 8, 14), stride = 1, padding = 0\n",
      "backward = False, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mimport\u001b[39;49;00m \u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "W          = needle.Tensor([[[[ 4.09100860e-01 -6.49829268e-01  1.13202944e+01 ... -5.56089449e+00\n",
      "    -4.84385538e+00 -1.20775558e...5e+00]\n",
      "   [-1.10714078e+00  2.54448557e+00 -7.73867989e+00 ... -1.13336074e+00\n",
      "    -1.21658230e+00 -4.79330921e+00]]]])\n",
      "W_shape    = (3, 3, 8, 14)\n",
      "Z          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e...5e+00]\n",
      "   [ 6.10367346e+00 -2.05434513e+00 -4.41384506e+00 ... -6.15245628e+00\n",
      "     3.33514667e+00 -1.35630560e+00]]]])\n",
      "Z_shape    = (3, 16, 16, 8)\n",
      "_W         = array([[[[ 4.09100860e-01, -6.49829268e-01,  1.13202944e+01, ...,\n",
      "          -5.56089449e+00, -4.84385538e+00, -1.20775...448557e+00, -7.73867989e+00, ...,\n",
      "          -1.13336074e+00, -1.21658230e+00, -4.79330921e+00]]]],\n",
      "      dtype=float32)\n",
      "_Z         = array([[[[ 8.82026196e+00,  2.00078607e+00,  4.89369011e+00, ...,\n",
      "          -4.88638926e+00,  4.75044203e+00, -7.56786...434513e+00, -4.41384506e+00, ...,\n",
      "          -6.15245628e+00,  3.33514667e+00, -1.35630560e+00]]]],\n",
      "      dtype=float32)\n",
      "backward   = False\n",
      "device     = cpu()\n",
      "padding    = 0\n",
      "stride     = 1\n",
      "torch      = <module 'torch' from '/opt/conda/envs/train/lib/python3.11/site-packages/torch/__init__.py'>\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:429: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:514: in conv\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Conv(stride, padding)(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e...5e+00]\n",
      "   [ 6.10367346e+00 -2.05434513e+00 -4.41384506e+00 ... -6.15245628e+00\n",
      "     3.33514667e+00 -1.35630560e+00]]]])\n",
      "        b          = needle.Tensor([[[[ 4.09100860e-01 -6.49829268e-01  1.13202944e+01 ... -5.56089449e+00\n",
      "    -4.84385538e+00 -1.20775558e...5e+00]\n",
      "   [-1.10714078e+00  2.54448557e+00 -7.73867989e+00 ... -1.13336074e+00\n",
      "    -1.21658230e+00 -4.79330921e+00]]]])\n",
      "        padding    = 0\n",
      "        stride     = 1\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048...e+00]\n",
      "   [-1.10714078e+00  2.54448557e+00 -7.73867989e+00 ... -1.13336074e+00\n",
      "    -1.21658230e+00 -4.79330921e+00]]]]))\n",
      "        self       = <needle.ops.ops_mathematic.Conv object at 0x7fb2daec4610>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048...e+00]\n",
      "   [-1.10714078e+00  2.54448557e+00 -7.73867989e+00 ... -1.13336074e+00\n",
      "    -1.21658230e+00 -4.79330921e+00]]]]))\n",
      "        op         = <needle.ops.ops_mathematic.Conv object at 0x7fb2daec4610>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fb2daec54d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fb2daec54d0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Conv object at 0x7fb2daec4610>\n",
      "A = NDArray([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e-01]\n",
      " ...10367346e+00 -2.05434513e+00 -4.41384506e+00 ... -6.15245628e+00\n",
      "     3.33514667e+00 -1.35630560e+00]]]], device=cpu())\n",
      "B = NDArray([[[[ 4.09100860e-01 -6.49829268e-01  1.13202944e+01 ... -5.56089449e+00\n",
      "    -4.84385538e+00 -1.20775558e-01]\n",
      " ...10714078e+00  2.54448557e+00 -7.73867989e+00 ... -1.13336074e+00\n",
      "    -1.21658230e+00 -4.79330921e+00]]]], device=cpu())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, A, B):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "A          = NDArray([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e-01]\n",
      " ...10367346e+00 -2.05434513e+00 -4.41384506e+00 ... -6.15245628e+00\n",
      "     3.33514667e+00 -1.35630560e+00]]]], device=cpu())\n",
      "B          = NDArray([[[[ 4.09100860e-01 -6.49829268e-01  1.13202944e+01 ... -5.56089449e+00\n",
      "    -4.84385538e+00 -1.20775558e-01]\n",
      " ...10714078e+00  2.54448557e+00 -7.73867989e+00 ... -1.13336074e+00\n",
      "    -1.21658230e+00 -4.79330921e+00]]]], device=cpu())\n",
      "self       = <needle.ops.ops_mathematic.Conv object at 0x7fb2daec4610>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:504: NotImplementedError\n",
      "\u001b[31m\u001b[1m_ test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape4-W_shape4-1-0] _\u001b[0m\n",
      "\n",
      "Z_shape = (3, 16, 16, 2), W_shape = (3, 3, 2, 14), stride = 1, padding = 0\n",
      "backward = False, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mimport\u001b[39;49;00m \u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "W          = needle.Tensor([[[[ -8.0968      -2.5552022    8.703147    -1.4674252    4.5861077\n",
      "     -0.28521433   4.383634    -9.13...727333   0.22582927   9.296732    -8.13161     -0.67411226\n",
      "     -2.9204676    1.675528   -12.187821     5.5746226 ]]]])\n",
      "W_shape    = (3, 3, 2, 14)\n",
      "Z          = needle.Tensor([[[[  8.820262     2.000786  ]\n",
      "   [  4.89369     11.204466  ]\n",
      "   [  9.33779     -4.8863893 ]\n",
      "   ...\n",
      "   [...   -5.112822  ]\n",
      "   ...\n",
      "   [ -7.922368     4.2222714 ]\n",
      "   [ -6.064339     1.4188478 ]\n",
      "   [ -1.4109794   -5.791016  ]]]])\n",
      "Z_shape    = (3, 16, 16, 2)\n",
      "_W         = array([[[[ -8.0968    ,  -2.5552022 ,   8.703147  ,  -1.4674252 ,\n",
      "            4.5861077 ,  -0.28521433,   4.383634  , ...        -8.13161   ,  -0.67411226,  -2.9204676 ,   1.675528  ,\n",
      "          -12.187821  ,   5.5746226 ]]]], dtype=float32)\n",
      "_Z         = array([[[[  8.820262  ,   2.000786  ],\n",
      "         [  4.89369   ,  11.204466  ],\n",
      "         [  9.33779   ,  -4.8863893 ],\n",
      " ...22368  ,   4.2222714 ],\n",
      "         [ -6.064339  ,   1.4188478 ],\n",
      "         [ -1.4109794 ,  -5.791016  ]]]], dtype=float32)\n",
      "backward   = False\n",
      "device     = cpu()\n",
      "padding    = 0\n",
      "stride     = 1\n",
      "torch      = <module 'torch' from '/opt/conda/envs/train/lib/python3.11/site-packages/torch/__init__.py'>\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:429: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:514: in conv\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Conv(stride, padding)(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[[  8.820262     2.000786  ]\n",
      "   [  4.89369     11.204466  ]\n",
      "   [  9.33779     -4.8863893 ]\n",
      "   ...\n",
      "   [...   -5.112822  ]\n",
      "   ...\n",
      "   [ -7.922368     4.2222714 ]\n",
      "   [ -6.064339     1.4188478 ]\n",
      "   [ -1.4109794   -5.791016  ]]]])\n",
      "        b          = needle.Tensor([[[[ -8.0968      -2.5552022    8.703147    -1.4674252    4.5861077\n",
      "     -0.28521433   4.383634    -9.13...727333   0.22582927   9.296732    -8.13161     -0.67411226\n",
      "     -2.9204676    1.675528   -12.187821     5.5746226 ]]]])\n",
      "        padding    = 0\n",
      "        stride     = 1\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[  8.820262     2.000786  ]\n",
      "   [  4.89369     11.204466  ]\n",
      "   [  9.33779     -4.8863893 ]\n",
      "   ...\n",
      "   ...27333   0.22582927   9.296732    -8.13161     -0.67411226\n",
      "     -2.9204676    1.675528   -12.187821     5.5746226 ]]]]))\n",
      "        self       = <needle.ops.ops_mathematic.Conv object at 0x7fb2daf09510>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[[  8.820262     2.000786  ]\n",
      "   [  4.89369     11.204466  ]\n",
      "   [  9.33779     -4.8863893 ]\n",
      "   ...\n",
      "   ...27333   0.22582927   9.296732    -8.13161     -0.67411226\n",
      "     -2.9204676    1.675528   -12.187821     5.5746226 ]]]]))\n",
      "        op         = <needle.ops.ops_mathematic.Conv object at 0x7fb2daf09510>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fb2daf0b950>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fb2daf0b950>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Conv object at 0x7fb2daf09510>\n",
      "A = NDArray([[[[  8.820262     2.000786  ]\n",
      "   [  4.89369     11.204466  ]\n",
      "   [  9.33779     -4.8863893 ]\n",
      "   ...\n",
      "   [  0.22...]\n",
      "   ...\n",
      "   [ -7.922368     4.2222714 ]\n",
      "   [ -6.064339     1.4188478 ]\n",
      "   [ -1.4109794   -5.791016  ]]]], device=cpu())\n",
      "B = NDArray([[[[ -8.0968      -2.5552022    8.703147    -1.4674252    4.5861077\n",
      "     -0.28521433   4.383634    -9.134557  ...82927   9.296732    -8.13161     -0.67411226\n",
      "     -2.9204676    1.675528   -12.187821     5.5746226 ]]]], device=cpu())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, A, B):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "A          = NDArray([[[[  8.820262     2.000786  ]\n",
      "   [  4.89369     11.204466  ]\n",
      "   [  9.33779     -4.8863893 ]\n",
      "   ...\n",
      "   [  0.22...]\n",
      "   ...\n",
      "   [ -7.922368     4.2222714 ]\n",
      "   [ -6.064339     1.4188478 ]\n",
      "   [ -1.4109794   -5.791016  ]]]], device=cpu())\n",
      "B          = NDArray([[[[ -8.0968      -2.5552022    8.703147    -1.4674252    4.5861077\n",
      "     -0.28521433   4.383634    -9.134557  ...82927   9.296732    -8.13161     -0.67411226\n",
      "     -2.9204676    1.675528   -12.187821     5.5746226 ]]]], device=cpu())\n",
      "self       = <needle.ops.ops_mathematic.Conv object at 0x7fb2daf09510>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:504: NotImplementedError\n",
      "\u001b[31m\u001b[1m_ test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape5-W_shape5-2-0] _\u001b[0m\n",
      "\n",
      "Z_shape = (3, 14, 14, 8), W_shape = (3, 3, 8, 16), stride = 2, padding = 0\n",
      "backward = False, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mimport\u001b[39;49;00m \u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "W          = needle.Tensor([[[[  4.282669     8.133604    -4.6223483  ...   2.0798197\n",
      "      2.1704152   -0.38803983]\n",
      "   [  2.245844...2.618842     0.71487164]\n",
      "   [ -4.09374      3.7291534  -10.551835   ...   0.31741843\n",
      "      0.03223436   2.0512383 ]]]])\n",
      "W_shape    = (3, 3, 8, 16)\n",
      "Z          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e...3e+00]\n",
      "   [ 1.09905624e+01  2.23166728e+00  4.62055349e+00 ... -6.23090088e-01\n",
      "    -8.48226726e-01 -2.29984760e+00]]]])\n",
      "Z_shape    = (3, 14, 14, 8)\n",
      "_W         = array([[[[  4.282669  ,   8.133604  ,  -4.6223483 , ...,   2.0798197 ,\n",
      "            2.1704152 ,  -0.38803983],\n",
      "        ... [ -4.09374   ,   3.7291534 , -10.551835  , ...,   0.31741843,\n",
      "            0.03223436,   2.0512383 ]]]], dtype=float32)\n",
      "_Z         = array([[[[ 8.82026196e+00,  2.00078607e+00,  4.89369011e+00, ...,\n",
      "          -4.88638926e+00,  4.75044203e+00, -7.56786...166728e+00,  4.62055349e+00, ...,\n",
      "          -6.23090088e-01, -8.48226726e-01, -2.29984760e+00]]]],\n",
      "      dtype=float32)\n",
      "backward   = False\n",
      "device     = cpu()\n",
      "padding    = 0\n",
      "stride     = 2\n",
      "torch      = <module 'torch' from '/opt/conda/envs/train/lib/python3.11/site-packages/torch/__init__.py'>\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:429: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:514: in conv\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Conv(stride, padding)(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e...3e+00]\n",
      "   [ 1.09905624e+01  2.23166728e+00  4.62055349e+00 ... -6.23090088e-01\n",
      "    -8.48226726e-01 -2.29984760e+00]]]])\n",
      "        b          = needle.Tensor([[[[  4.282669     8.133604    -4.6223483  ...   2.0798197\n",
      "      2.1704152   -0.38803983]\n",
      "   [  2.245844...2.618842     0.71487164]\n",
      "   [ -4.09374      3.7291534  -10.551835   ...   0.31741843\n",
      "      0.03223436   2.0512383 ]]]])\n",
      "        padding    = 0\n",
      "        stride     = 2\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048....618842     0.71487164]\n",
      "   [ -4.09374      3.7291534  -10.551835   ...   0.31741843\n",
      "      0.03223436   2.0512383 ]]]]))\n",
      "        self       = <needle.ops.ops_mathematic.Conv object at 0x7fb2db0f06d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048....618842     0.71487164]\n",
      "   [ -4.09374      3.7291534  -10.551835   ...   0.31741843\n",
      "      0.03223436   2.0512383 ]]]]))\n",
      "        op         = <needle.ops.ops_mathematic.Conv object at 0x7fb2db0f06d0>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fb2db0f3390>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fb2db0f3390>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Conv object at 0x7fb2db0f06d0>\n",
      "A = NDArray([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e-01]\n",
      " ...09905624e+01  2.23166728e+00  4.62055349e+00 ... -6.23090088e-01\n",
      "    -8.48226726e-01 -2.29984760e+00]]]], device=cpu())\n",
      "B = NDArray([[[[  4.282669     8.133604    -4.6223483  ...   2.0798197\n",
      "      2.1704152   -0.38803983]\n",
      "   [  2.2458448   -0....71487164]\n",
      "   [ -4.09374      3.7291534  -10.551835   ...   0.31741843\n",
      "      0.03223436   2.0512383 ]]]], device=cpu())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, A, B):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "A          = NDArray([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e-01]\n",
      " ...09905624e+01  2.23166728e+00  4.62055349e+00 ... -6.23090088e-01\n",
      "    -8.48226726e-01 -2.29984760e+00]]]], device=cpu())\n",
      "B          = NDArray([[[[  4.282669     8.133604    -4.6223483  ...   2.0798197\n",
      "      2.1704152   -0.38803983]\n",
      "   [  2.2458448   -0....71487164]\n",
      "   [ -4.09374      3.7291534  -10.551835   ...   0.31741843\n",
      "      0.03223436   2.0512383 ]]]], device=cpu())\n",
      "self       = <needle.ops.ops_mathematic.Conv object at 0x7fb2db0f06d0>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:504: NotImplementedError\n",
      "\u001b[31m\u001b[1m_ test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape6-W_shape6-2-1] _\u001b[0m\n",
      "\n",
      "Z_shape = (3, 14, 14, 8), W_shape = (3, 3, 8, 16), stride = 2, padding = 1\n",
      "backward = False, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mimport\u001b[39;49;00m \u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "W          = needle.Tensor([[[[  4.282669     8.133604    -4.6223483  ...   2.0798197\n",
      "      2.1704152   -0.38803983]\n",
      "   [  2.245844...2.618842     0.71487164]\n",
      "   [ -4.09374      3.7291534  -10.551835   ...   0.31741843\n",
      "      0.03223436   2.0512383 ]]]])\n",
      "W_shape    = (3, 3, 8, 16)\n",
      "Z          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e...3e+00]\n",
      "   [ 1.09905624e+01  2.23166728e+00  4.62055349e+00 ... -6.23090088e-01\n",
      "    -8.48226726e-01 -2.29984760e+00]]]])\n",
      "Z_shape    = (3, 14, 14, 8)\n",
      "_W         = array([[[[  4.282669  ,   8.133604  ,  -4.6223483 , ...,   2.0798197 ,\n",
      "            2.1704152 ,  -0.38803983],\n",
      "        ... [ -4.09374   ,   3.7291534 , -10.551835  , ...,   0.31741843,\n",
      "            0.03223436,   2.0512383 ]]]], dtype=float32)\n",
      "_Z         = array([[[[ 8.82026196e+00,  2.00078607e+00,  4.89369011e+00, ...,\n",
      "          -4.88638926e+00,  4.75044203e+00, -7.56786...166728e+00,  4.62055349e+00, ...,\n",
      "          -6.23090088e-01, -8.48226726e-01, -2.29984760e+00]]]],\n",
      "      dtype=float32)\n",
      "backward   = False\n",
      "device     = cpu()\n",
      "padding    = 1\n",
      "stride     = 2\n",
      "torch      = <module 'torch' from '/opt/conda/envs/train/lib/python3.11/site-packages/torch/__init__.py'>\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:429: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:514: in conv\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Conv(stride, padding)(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e...3e+00]\n",
      "   [ 1.09905624e+01  2.23166728e+00  4.62055349e+00 ... -6.23090088e-01\n",
      "    -8.48226726e-01 -2.29984760e+00]]]])\n",
      "        b          = needle.Tensor([[[[  4.282669     8.133604    -4.6223483  ...   2.0798197\n",
      "      2.1704152   -0.38803983]\n",
      "   [  2.245844...2.618842     0.71487164]\n",
      "   [ -4.09374      3.7291534  -10.551835   ...   0.31741843\n",
      "      0.03223436   2.0512383 ]]]])\n",
      "        padding    = 1\n",
      "        stride     = 2\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048....618842     0.71487164]\n",
      "   [ -4.09374      3.7291534  -10.551835   ...   0.31741843\n",
      "      0.03223436   2.0512383 ]]]]))\n",
      "        self       = <needle.ops.ops_mathematic.Conv object at 0x7fb2db0c2bd0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048....618842     0.71487164]\n",
      "   [ -4.09374      3.7291534  -10.551835   ...   0.31741843\n",
      "      0.03223436   2.0512383 ]]]]))\n",
      "        op         = <needle.ops.ops_mathematic.Conv object at 0x7fb2db0c2bd0>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fb2db0c2f50>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fb2db0c2f50>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Conv object at 0x7fb2db0c2bd0>\n",
      "A = NDArray([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e-01]\n",
      " ...09905624e+01  2.23166728e+00  4.62055349e+00 ... -6.23090088e-01\n",
      "    -8.48226726e-01 -2.29984760e+00]]]], device=cpu())\n",
      "B = NDArray([[[[  4.282669     8.133604    -4.6223483  ...   2.0798197\n",
      "      2.1704152   -0.38803983]\n",
      "   [  2.2458448   -0....71487164]\n",
      "   [ -4.09374      3.7291534  -10.551835   ...   0.31741843\n",
      "      0.03223436   2.0512383 ]]]], device=cpu())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, A, B):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "A          = NDArray([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e-01]\n",
      " ...09905624e+01  2.23166728e+00  4.62055349e+00 ... -6.23090088e-01\n",
      "    -8.48226726e-01 -2.29984760e+00]]]], device=cpu())\n",
      "B          = NDArray([[[[  4.282669     8.133604    -4.6223483  ...   2.0798197\n",
      "      2.1704152   -0.38803983]\n",
      "   [  2.2458448   -0....71487164]\n",
      "   [ -4.09374      3.7291534  -10.551835   ...   0.31741843\n",
      "      0.03223436   2.0512383 ]]]], device=cpu())\n",
      "self       = <needle.ops.ops_mathematic.Conv object at 0x7fb2db0c2bd0>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:504: NotImplementedError\n",
      "\u001b[31m\u001b[1m_ test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape7-W_shape7-2-2] _\u001b[0m\n",
      "\n",
      "Z_shape = (3, 16, 16, 8), W_shape = (3, 3, 8, 16), stride = 2, padding = 2\n",
      "backward = False, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mimport\u001b[39;49;00m \u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "W          = needle.Tensor([[[[  0.40910086  -0.64982927  11.320294   ...  -0.12077556\n",
      "      1.428692    -5.5845156 ]\n",
      "   [ -6.89178...-1.5437248  -10.136281  ]\n",
      "   [  1.1511405    0.7441038   16.463472   ...  -6.8931007\n",
      "     -0.5833115   -2.504636  ]]]])\n",
      "W_shape    = (3, 3, 8, 16)\n",
      "Z          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e...5e+00]\n",
      "   [ 6.10367346e+00 -2.05434513e+00 -4.41384506e+00 ... -6.15245628e+00\n",
      "     3.33514667e+00 -1.35630560e+00]]]])\n",
      "Z_shape    = (3, 16, 16, 8)\n",
      "_W         = array([[[[  0.40910086,  -0.64982927,  11.320294  , ...,  -0.12077556,\n",
      "            1.428692  ,  -5.5845156 ],\n",
      "        ... [  1.1511405 ,   0.7441038 ,  16.463472  , ...,  -6.8931007 ,\n",
      "           -0.5833115 ,  -2.504636  ]]]], dtype=float32)\n",
      "_Z         = array([[[[ 8.82026196e+00,  2.00078607e+00,  4.89369011e+00, ...,\n",
      "          -4.88638926e+00,  4.75044203e+00, -7.56786...434513e+00, -4.41384506e+00, ...,\n",
      "          -6.15245628e+00,  3.33514667e+00, -1.35630560e+00]]]],\n",
      "      dtype=float32)\n",
      "backward   = False\n",
      "device     = cpu()\n",
      "padding    = 2\n",
      "stride     = 2\n",
      "torch      = <module 'torch' from '/opt/conda/envs/train/lib/python3.11/site-packages/torch/__init__.py'>\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:429: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:514: in conv\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Conv(stride, padding)(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e...5e+00]\n",
      "   [ 6.10367346e+00 -2.05434513e+00 -4.41384506e+00 ... -6.15245628e+00\n",
      "     3.33514667e+00 -1.35630560e+00]]]])\n",
      "        b          = needle.Tensor([[[[  0.40910086  -0.64982927  11.320294   ...  -0.12077556\n",
      "      1.428692    -5.5845156 ]\n",
      "   [ -6.89178...-1.5437248  -10.136281  ]\n",
      "   [  1.1511405    0.7441038   16.463472   ...  -6.8931007\n",
      "     -0.5833115   -2.504636  ]]]])\n",
      "        padding    = 2\n",
      "        stride     = 2\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048...1.5437248  -10.136281  ]\n",
      "   [  1.1511405    0.7441038   16.463472   ...  -6.8931007\n",
      "     -0.5833115   -2.504636  ]]]]))\n",
      "        self       = <needle.ops.ops_mathematic.Conv object at 0x7fb2daf14c10>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048...1.5437248  -10.136281  ]\n",
      "   [  1.1511405    0.7441038   16.463472   ...  -6.8931007\n",
      "     -0.5833115   -2.504636  ]]]]))\n",
      "        op         = <needle.ops.ops_mathematic.Conv object at 0x7fb2daf14c10>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fb2daf159d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fb2daf159d0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Conv object at 0x7fb2daf14c10>\n",
      "A = NDArray([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e-01]\n",
      " ...10367346e+00 -2.05434513e+00 -4.41384506e+00 ... -6.15245628e+00\n",
      "     3.33514667e+00 -1.35630560e+00]]]], device=cpu())\n",
      "B = NDArray([[[[  0.40910086  -0.64982927  11.320294   ...  -0.12077556\n",
      "      1.428692    -5.5845156 ]\n",
      "   [ -6.891782    -...0.136281  ]\n",
      "   [  1.1511405    0.7441038   16.463472   ...  -6.8931007\n",
      "     -0.5833115   -2.504636  ]]]], device=cpu())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, A, B):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "A          = NDArray([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e-01]\n",
      " ...10367346e+00 -2.05434513e+00 -4.41384506e+00 ... -6.15245628e+00\n",
      "     3.33514667e+00 -1.35630560e+00]]]], device=cpu())\n",
      "B          = NDArray([[[[  0.40910086  -0.64982927  11.320294   ...  -0.12077556\n",
      "      1.428692    -5.5845156 ]\n",
      "   [ -6.891782    -...0.136281  ]\n",
      "   [  1.1511405    0.7441038   16.463472   ...  -6.8931007\n",
      "     -0.5833115   -2.504636  ]]]], device=cpu())\n",
      "self       = <needle.ops.ops_mathematic.Conv object at 0x7fb2daf14c10>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:504: NotImplementedError\n",
      "\u001b[31m\u001b[1m_ test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape8-W_shape8-2-0] _\u001b[0m\n",
      "\n",
      "Z_shape = (3, 16, 16, 8), W_shape = (3, 3, 8, 14), stride = 2, padding = 0\n",
      "backward = False, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mimport\u001b[39;49;00m \u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "W          = needle.Tensor([[[[ 4.09100860e-01 -6.49829268e-01  1.13202944e+01 ... -5.56089449e+00\n",
      "    -4.84385538e+00 -1.20775558e...5e+00]\n",
      "   [-1.10714078e+00  2.54448557e+00 -7.73867989e+00 ... -1.13336074e+00\n",
      "    -1.21658230e+00 -4.79330921e+00]]]])\n",
      "W_shape    = (3, 3, 8, 14)\n",
      "Z          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e...5e+00]\n",
      "   [ 6.10367346e+00 -2.05434513e+00 -4.41384506e+00 ... -6.15245628e+00\n",
      "     3.33514667e+00 -1.35630560e+00]]]])\n",
      "Z_shape    = (3, 16, 16, 8)\n",
      "_W         = array([[[[ 4.09100860e-01, -6.49829268e-01,  1.13202944e+01, ...,\n",
      "          -5.56089449e+00, -4.84385538e+00, -1.20775...448557e+00, -7.73867989e+00, ...,\n",
      "          -1.13336074e+00, -1.21658230e+00, -4.79330921e+00]]]],\n",
      "      dtype=float32)\n",
      "_Z         = array([[[[ 8.82026196e+00,  2.00078607e+00,  4.89369011e+00, ...,\n",
      "          -4.88638926e+00,  4.75044203e+00, -7.56786...434513e+00, -4.41384506e+00, ...,\n",
      "          -6.15245628e+00,  3.33514667e+00, -1.35630560e+00]]]],\n",
      "      dtype=float32)\n",
      "backward   = False\n",
      "device     = cpu()\n",
      "padding    = 0\n",
      "stride     = 2\n",
      "torch      = <module 'torch' from '/opt/conda/envs/train/lib/python3.11/site-packages/torch/__init__.py'>\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:429: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:514: in conv\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Conv(stride, padding)(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e...5e+00]\n",
      "   [ 6.10367346e+00 -2.05434513e+00 -4.41384506e+00 ... -6.15245628e+00\n",
      "     3.33514667e+00 -1.35630560e+00]]]])\n",
      "        b          = needle.Tensor([[[[ 4.09100860e-01 -6.49829268e-01  1.13202944e+01 ... -5.56089449e+00\n",
      "    -4.84385538e+00 -1.20775558e...5e+00]\n",
      "   [-1.10714078e+00  2.54448557e+00 -7.73867989e+00 ... -1.13336074e+00\n",
      "    -1.21658230e+00 -4.79330921e+00]]]])\n",
      "        padding    = 0\n",
      "        stride     = 2\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048...e+00]\n",
      "   [-1.10714078e+00  2.54448557e+00 -7.73867989e+00 ... -1.13336074e+00\n",
      "    -1.21658230e+00 -4.79330921e+00]]]]))\n",
      "        self       = <needle.ops.ops_mathematic.Conv object at 0x7fb2daecac50>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048...e+00]\n",
      "   [-1.10714078e+00  2.54448557e+00 -7.73867989e+00 ... -1.13336074e+00\n",
      "    -1.21658230e+00 -4.79330921e+00]]]]))\n",
      "        op         = <needle.ops.ops_mathematic.Conv object at 0x7fb2daecac50>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fb2daecb550>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fb2daecb550>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Conv object at 0x7fb2daecac50>\n",
      "A = NDArray([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e-01]\n",
      " ...10367346e+00 -2.05434513e+00 -4.41384506e+00 ... -6.15245628e+00\n",
      "     3.33514667e+00 -1.35630560e+00]]]], device=cpu())\n",
      "B = NDArray([[[[ 4.09100860e-01 -6.49829268e-01  1.13202944e+01 ... -5.56089449e+00\n",
      "    -4.84385538e+00 -1.20775558e-01]\n",
      " ...10714078e+00  2.54448557e+00 -7.73867989e+00 ... -1.13336074e+00\n",
      "    -1.21658230e+00 -4.79330921e+00]]]], device=cpu())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, A, B):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "A          = NDArray([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e-01]\n",
      " ...10367346e+00 -2.05434513e+00 -4.41384506e+00 ... -6.15245628e+00\n",
      "     3.33514667e+00 -1.35630560e+00]]]], device=cpu())\n",
      "B          = NDArray([[[[ 4.09100860e-01 -6.49829268e-01  1.13202944e+01 ... -5.56089449e+00\n",
      "    -4.84385538e+00 -1.20775558e-01]\n",
      " ...10714078e+00  2.54448557e+00 -7.73867989e+00 ... -1.13336074e+00\n",
      "    -1.21658230e+00 -4.79330921e+00]]]], device=cpu())\n",
      "self       = <needle.ops.ops_mathematic.Conv object at 0x7fb2daecac50>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:504: NotImplementedError\n",
      "\u001b[31m\u001b[1m_ test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape9-W_shape9-2-0] _\u001b[0m\n",
      "\n",
      "Z_shape = (3, 16, 16, 2), W_shape = (3, 3, 2, 14), stride = 2, padding = 0\n",
      "backward = False, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mimport\u001b[39;49;00m \u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "W          = needle.Tensor([[[[ -8.0968      -2.5552022    8.703147    -1.4674252    4.5861077\n",
      "     -0.28521433   4.383634    -9.13...727333   0.22582927   9.296732    -8.13161     -0.67411226\n",
      "     -2.9204676    1.675528   -12.187821     5.5746226 ]]]])\n",
      "W_shape    = (3, 3, 2, 14)\n",
      "Z          = needle.Tensor([[[[  8.820262     2.000786  ]\n",
      "   [  4.89369     11.204466  ]\n",
      "   [  9.33779     -4.8863893 ]\n",
      "   ...\n",
      "   [...   -5.112822  ]\n",
      "   ...\n",
      "   [ -7.922368     4.2222714 ]\n",
      "   [ -6.064339     1.4188478 ]\n",
      "   [ -1.4109794   -5.791016  ]]]])\n",
      "Z_shape    = (3, 16, 16, 2)\n",
      "_W         = array([[[[ -8.0968    ,  -2.5552022 ,   8.703147  ,  -1.4674252 ,\n",
      "            4.5861077 ,  -0.28521433,   4.383634  , ...        -8.13161   ,  -0.67411226,  -2.9204676 ,   1.675528  ,\n",
      "          -12.187821  ,   5.5746226 ]]]], dtype=float32)\n",
      "_Z         = array([[[[  8.820262  ,   2.000786  ],\n",
      "         [  4.89369   ,  11.204466  ],\n",
      "         [  9.33779   ,  -4.8863893 ],\n",
      " ...22368  ,   4.2222714 ],\n",
      "         [ -6.064339  ,   1.4188478 ],\n",
      "         [ -1.4109794 ,  -5.791016  ]]]], dtype=float32)\n",
      "backward   = False\n",
      "device     = cpu()\n",
      "padding    = 0\n",
      "stride     = 2\n",
      "torch      = <module 'torch' from '/opt/conda/envs/train/lib/python3.11/site-packages/torch/__init__.py'>\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:429: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:514: in conv\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Conv(stride, padding)(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[[  8.820262     2.000786  ]\n",
      "   [  4.89369     11.204466  ]\n",
      "   [  9.33779     -4.8863893 ]\n",
      "   ...\n",
      "   [...   -5.112822  ]\n",
      "   ...\n",
      "   [ -7.922368     4.2222714 ]\n",
      "   [ -6.064339     1.4188478 ]\n",
      "   [ -1.4109794   -5.791016  ]]]])\n",
      "        b          = needle.Tensor([[[[ -8.0968      -2.5552022    8.703147    -1.4674252    4.5861077\n",
      "     -0.28521433   4.383634    -9.13...727333   0.22582927   9.296732    -8.13161     -0.67411226\n",
      "     -2.9204676    1.675528   -12.187821     5.5746226 ]]]])\n",
      "        padding    = 0\n",
      "        stride     = 2\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[  8.820262     2.000786  ]\n",
      "   [  4.89369     11.204466  ]\n",
      "   [  9.33779     -4.8863893 ]\n",
      "   ...\n",
      "   ...27333   0.22582927   9.296732    -8.13161     -0.67411226\n",
      "     -2.9204676    1.675528   -12.187821     5.5746226 ]]]]))\n",
      "        self       = <needle.ops.ops_mathematic.Conv object at 0x7fb2db068790>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[[  8.820262     2.000786  ]\n",
      "   [  4.89369     11.204466  ]\n",
      "   [  9.33779     -4.8863893 ]\n",
      "   ...\n",
      "   ...27333   0.22582927   9.296732    -8.13161     -0.67411226\n",
      "     -2.9204676    1.675528   -12.187821     5.5746226 ]]]]))\n",
      "        op         = <needle.ops.ops_mathematic.Conv object at 0x7fb2db068790>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fb2db068350>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fb2db068350>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Conv object at 0x7fb2db068790>\n",
      "A = NDArray([[[[  8.820262     2.000786  ]\n",
      "   [  4.89369     11.204466  ]\n",
      "   [  9.33779     -4.8863893 ]\n",
      "   ...\n",
      "   [  0.22...]\n",
      "   ...\n",
      "   [ -7.922368     4.2222714 ]\n",
      "   [ -6.064339     1.4188478 ]\n",
      "   [ -1.4109794   -5.791016  ]]]], device=cpu())\n",
      "B = NDArray([[[[ -8.0968      -2.5552022    8.703147    -1.4674252    4.5861077\n",
      "     -0.28521433   4.383634    -9.134557  ...82927   9.296732    -8.13161     -0.67411226\n",
      "     -2.9204676    1.675528   -12.187821     5.5746226 ]]]], device=cpu())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, A, B):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "A          = NDArray([[[[  8.820262     2.000786  ]\n",
      "   [  4.89369     11.204466  ]\n",
      "   [  9.33779     -4.8863893 ]\n",
      "   ...\n",
      "   [  0.22...]\n",
      "   ...\n",
      "   [ -7.922368     4.2222714 ]\n",
      "   [ -6.064339     1.4188478 ]\n",
      "   [ -1.4109794   -5.791016  ]]]], device=cpu())\n",
      "B          = NDArray([[[[ -8.0968      -2.5552022    8.703147    -1.4674252    4.5861077\n",
      "     -0.28521433   4.383634    -9.134557  ...82927   9.296732    -8.13161     -0.67411226\n",
      "     -2.9204676    1.675528   -12.187821     5.5746226 ]]]], device=cpu())\n",
      "self       = <needle.ops.ops_mathematic.Conv object at 0x7fb2db068790>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:504: NotImplementedError\n",
      "\u001b[31m\u001b[1m_ test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape10-W_shape10-1-0] _\u001b[0m\n",
      "\n",
      "Z_shape = (3, 16, 16, 24), W_shape = (3, 3, 24, 14), stride = 1, padding = 0\n",
      "backward = False, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mimport\u001b[39;49;00m \u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "W          = needle.Tensor([[[[ -1.3245817    1.0111231   -1.9236585  ...  -2.5907016\n",
      "     -0.36175704   4.6234684 ]\n",
      "   [  5.279258... -4.82095      4.98517   ]\n",
      "   [  3.2300415   -2.4161792   -3.420825   ...  -9.444658\n",
      "      8.537833     2.3818388 ]]]])\n",
      "W_shape    = (3, 3, 24, 14)\n",
      "Z          = needle.Tensor([[[[  8.820262     2.000786     4.89369    ...   3.2680929\n",
      "      4.322181    -3.7108252 ]\n",
      "   [ 11.348773...4.207389     8.935435  ]\n",
      "   [  2.5084004    3.1399443   -0.12958507 ...   0.13740699\n",
      "     -1.400853     0.07475674]]]])\n",
      "Z_shape    = (3, 16, 16, 24)\n",
      "_W         = array([[[[ -1.3245817 ,   1.0111231 ,  -1.9236585 , ...,  -2.5907016 ,\n",
      "           -0.36175704,   4.6234684 ],\n",
      "        ... [  3.2300415 ,  -2.4161792 ,  -3.420825  , ...,  -9.444658  ,\n",
      "            8.537833  ,   2.3818388 ]]]], dtype=float32)\n",
      "_Z         = array([[[[  8.820262  ,   2.000786  ,   4.89369   , ...,   3.2680929 ,\n",
      "            4.322181  ,  -3.7108252 ],\n",
      "        ... [  2.5084004 ,   3.1399443 ,  -0.12958507, ...,   0.13740699,\n",
      "           -1.400853  ,   0.07475674]]]], dtype=float32)\n",
      "backward   = False\n",
      "device     = cpu()\n",
      "padding    = 0\n",
      "stride     = 1\n",
      "torch      = <module 'torch' from '/opt/conda/envs/train/lib/python3.11/site-packages/torch/__init__.py'>\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:429: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:514: in conv\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Conv(stride, padding)(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[[  8.820262     2.000786     4.89369    ...   3.2680929\n",
      "      4.322181    -3.7108252 ]\n",
      "   [ 11.348773...4.207389     8.935435  ]\n",
      "   [  2.5084004    3.1399443   -0.12958507 ...   0.13740699\n",
      "     -1.400853     0.07475674]]]])\n",
      "        b          = needle.Tensor([[[[ -1.3245817    1.0111231   -1.9236585  ...  -2.5907016\n",
      "     -0.36175704   4.6234684 ]\n",
      "   [  5.279258... -4.82095      4.98517   ]\n",
      "   [  3.2300415   -2.4161792   -3.420825   ...  -9.444658\n",
      "      8.537833     2.3818388 ]]]])\n",
      "        padding    = 0\n",
      "        stride     = 1\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[  8.820262     2.000786     4.89369    ...   3.2680929\n",
      "      4.322181    -3.7108252 ]\n",
      "   [ 11.34877...-4.82095      4.98517   ]\n",
      "   [  3.2300415   -2.4161792   -3.420825   ...  -9.444658\n",
      "      8.537833     2.3818388 ]]]]))\n",
      "        self       = <needle.ops.ops_mathematic.Conv object at 0x7fb2db016e10>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[[  8.820262     2.000786     4.89369    ...   3.2680929\n",
      "      4.322181    -3.7108252 ]\n",
      "   [ 11.34877...-4.82095      4.98517   ]\n",
      "   [  3.2300415   -2.4161792   -3.420825   ...  -9.444658\n",
      "      8.537833     2.3818388 ]]]]))\n",
      "        op         = <needle.ops.ops_mathematic.Conv object at 0x7fb2db016e10>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fb2db014510>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fb2db014510>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Conv object at 0x7fb2db016e10>\n",
      "A = NDArray([[[[  8.820262     2.000786     4.89369    ...   3.2680929\n",
      "      4.322181    -3.7108252 ]\n",
      "   [ 11.348773    -7....935435  ]\n",
      "   [  2.5084004    3.1399443   -0.12958507 ...   0.13740699\n",
      "     -1.400853     0.07475674]]]], device=cpu())\n",
      "B = NDArray([[[[ -1.3245817    1.0111231   -1.9236585  ...  -2.5907016\n",
      "     -0.36175704   4.6234684 ]\n",
      "   [  5.2792587   -1... 4.98517   ]\n",
      "   [  3.2300415   -2.4161792   -3.420825   ...  -9.444658\n",
      "      8.537833     2.3818388 ]]]], device=cpu())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, A, B):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "A          = NDArray([[[[  8.820262     2.000786     4.89369    ...   3.2680929\n",
      "      4.322181    -3.7108252 ]\n",
      "   [ 11.348773    -7....935435  ]\n",
      "   [  2.5084004    3.1399443   -0.12958507 ...   0.13740699\n",
      "     -1.400853     0.07475674]]]], device=cpu())\n",
      "B          = NDArray([[[[ -1.3245817    1.0111231   -1.9236585  ...  -2.5907016\n",
      "     -0.36175704   4.6234684 ]\n",
      "   [  5.2792587   -1... 4.98517   ]\n",
      "   [  3.2300415   -2.4161792   -3.420825   ...  -9.444658\n",
      "      8.537833     2.3818388 ]]]], device=cpu())\n",
      "self       = <needle.ops.ops_mathematic.Conv object at 0x7fb2db016e10>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:504: NotImplementedError\n",
      "\u001b[31m\u001b[1m_ test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape11-W_shape11-1-0] _\u001b[0m\n",
      "\n",
      "Z_shape = (3, 14, 14, 8), W_shape = (5, 5, 8, 16), stride = 1, padding = 0\n",
      "backward = False, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mimport\u001b[39;49;00m \u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "W          = needle.Tensor([[[[ 4.28266907e+00  8.13360405e+00 -4.62234831e+00 ...  2.07981968e+00\n",
      "     2.17041516e+00 -3.88039827e...7e+00]\n",
      "   [-7.86027253e-01  4.30947495e+00 -4.97804356e+00 ... -5.69430470e-01\n",
      "     4.27775383e-01 -3.84437203e+00]]]])\n",
      "W_shape    = (5, 5, 8, 16)\n",
      "Z          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e...3e+00]\n",
      "   [ 1.09905624e+01  2.23166728e+00  4.62055349e+00 ... -6.23090088e-01\n",
      "    -8.48226726e-01 -2.29984760e+00]]]])\n",
      "Z_shape    = (3, 14, 14, 8)\n",
      "_W         = array([[[[ 4.28266907e+00,  8.13360405e+00, -4.62234831e+00, ...,\n",
      "           2.07981968e+00,  2.17041516e+00, -3.88039...947495e+00, -4.97804356e+00, ...,\n",
      "          -5.69430470e-01,  4.27775383e-01, -3.84437203e+00]]]],\n",
      "      dtype=float32)\n",
      "_Z         = array([[[[ 8.82026196e+00,  2.00078607e+00,  4.89369011e+00, ...,\n",
      "          -4.88638926e+00,  4.75044203e+00, -7.56786...166728e+00,  4.62055349e+00, ...,\n",
      "          -6.23090088e-01, -8.48226726e-01, -2.29984760e+00]]]],\n",
      "      dtype=float32)\n",
      "backward   = False\n",
      "device     = cpu()\n",
      "padding    = 0\n",
      "stride     = 1\n",
      "torch      = <module 'torch' from '/opt/conda/envs/train/lib/python3.11/site-packages/torch/__init__.py'>\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:429: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:514: in conv\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Conv(stride, padding)(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e...3e+00]\n",
      "   [ 1.09905624e+01  2.23166728e+00  4.62055349e+00 ... -6.23090088e-01\n",
      "    -8.48226726e-01 -2.29984760e+00]]]])\n",
      "        b          = needle.Tensor([[[[ 4.28266907e+00  8.13360405e+00 -4.62234831e+00 ...  2.07981968e+00\n",
      "     2.17041516e+00 -3.88039827e...7e+00]\n",
      "   [-7.86027253e-01  4.30947495e+00 -4.97804356e+00 ... -5.69430470e-01\n",
      "     4.27775383e-01 -3.84437203e+00]]]])\n",
      "        padding    = 0\n",
      "        stride     = 1\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048...e+00]\n",
      "   [-7.86027253e-01  4.30947495e+00 -4.97804356e+00 ... -5.69430470e-01\n",
      "     4.27775383e-01 -3.84437203e+00]]]]))\n",
      "        self       = <needle.ops.ops_mathematic.Conv object at 0x7fb2daff6150>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048...e+00]\n",
      "   [-7.86027253e-01  4.30947495e+00 -4.97804356e+00 ... -5.69430470e-01\n",
      "     4.27775383e-01 -3.84437203e+00]]]]))\n",
      "        op         = <needle.ops.ops_mathematic.Conv object at 0x7fb2daff6150>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fb2daff5050>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fb2daff5050>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Conv object at 0x7fb2daff6150>\n",
      "A = NDArray([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e-01]\n",
      " ...09905624e+01  2.23166728e+00  4.62055349e+00 ... -6.23090088e-01\n",
      "    -8.48226726e-01 -2.29984760e+00]]]], device=cpu())\n",
      "B = NDArray([[[[ 4.28266907e+00  8.13360405e+00 -4.62234831e+00 ...  2.07981968e+00\n",
      "     2.17041516e+00 -3.88039827e-01]\n",
      " ...86027253e-01  4.30947495e+00 -4.97804356e+00 ... -5.69430470e-01\n",
      "     4.27775383e-01 -3.84437203e+00]]]], device=cpu())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, A, B):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "A          = NDArray([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e-01]\n",
      " ...09905624e+01  2.23166728e+00  4.62055349e+00 ... -6.23090088e-01\n",
      "    -8.48226726e-01 -2.29984760e+00]]]], device=cpu())\n",
      "B          = NDArray([[[[ 4.28266907e+00  8.13360405e+00 -4.62234831e+00 ...  2.07981968e+00\n",
      "     2.17041516e+00 -3.88039827e-01]\n",
      " ...86027253e-01  4.30947495e+00 -4.97804356e+00 ... -5.69430470e-01\n",
      "     4.27775383e-01 -3.84437203e+00]]]], device=cpu())\n",
      "self       = <needle.ops.ops_mathematic.Conv object at 0x7fb2daff6150>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:504: NotImplementedError\n",
      "\u001b[31m\u001b[1m_ test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape12-W_shape12-1-0] _\u001b[0m\n",
      "\n",
      "Z_shape = (3, 17, 17, 8), W_shape = (5, 5, 8, 16), stride = 1, padding = 0\n",
      "backward = False, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mimport\u001b[39;49;00m \u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "W          = needle.Tensor([[[[ 5.93117094e+00 -4.61600447e+00 -7.43411541e+00 ...  9.49053860e+00\n",
      "     6.47611380e+00 -3.38525438e...7e+00]\n",
      "   [-3.93575430e+00 -6.98609781e+00  1.63659811e+00 ...  8.22474575e+00\n",
      "     2.56555057e+00  1.63004756e+00]]]])\n",
      "W_shape    = (5, 5, 8, 16)\n",
      "Z          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e...8e+00]\n",
      "   [ 2.80155873e+00  3.11298299e+00 -3.24756050e+00 ... -7.96233535e-01\n",
      "     4.72636795e+00 -5.75466204e+00]]]])\n",
      "Z_shape    = (3, 17, 17, 8)\n",
      "_W         = array([[[[ 5.93117094e+00, -4.61600447e+00, -7.43411541e+00, ...,\n",
      "           9.49053860e+00,  6.47611380e+00, -3.38525...609781e+00,  1.63659811e+00, ...,\n",
      "           8.22474575e+00,  2.56555057e+00,  1.63004756e+00]]]],\n",
      "      dtype=float32)\n",
      "_Z         = array([[[[ 8.82026196e+00,  2.00078607e+00,  4.89369011e+00, ...,\n",
      "          -4.88638926e+00,  4.75044203e+00, -7.56786...298299e+00, -3.24756050e+00, ...,\n",
      "          -7.96233535e-01,  4.72636795e+00, -5.75466204e+00]]]],\n",
      "      dtype=float32)\n",
      "backward   = False\n",
      "device     = cpu()\n",
      "padding    = 0\n",
      "stride     = 1\n",
      "torch      = <module 'torch' from '/opt/conda/envs/train/lib/python3.11/site-packages/torch/__init__.py'>\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:429: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:514: in conv\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Conv(stride, padding)(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e...8e+00]\n",
      "   [ 2.80155873e+00  3.11298299e+00 -3.24756050e+00 ... -7.96233535e-01\n",
      "     4.72636795e+00 -5.75466204e+00]]]])\n",
      "        b          = needle.Tensor([[[[ 5.93117094e+00 -4.61600447e+00 -7.43411541e+00 ...  9.49053860e+00\n",
      "     6.47611380e+00 -3.38525438e...7e+00]\n",
      "   [-3.93575430e+00 -6.98609781e+00  1.63659811e+00 ...  8.22474575e+00\n",
      "     2.56555057e+00  1.63004756e+00]]]])\n",
      "        padding    = 0\n",
      "        stride     = 1\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048...e+00]\n",
      "   [-3.93575430e+00 -6.98609781e+00  1.63659811e+00 ...  8.22474575e+00\n",
      "     2.56555057e+00  1.63004756e+00]]]]))\n",
      "        self       = <needle.ops.ops_mathematic.Conv object at 0x7fb2daf99a10>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048...e+00]\n",
      "   [-3.93575430e+00 -6.98609781e+00  1.63659811e+00 ...  8.22474575e+00\n",
      "     2.56555057e+00  1.63004756e+00]]]]))\n",
      "        op         = <needle.ops.ops_mathematic.Conv object at 0x7fb2daf99a10>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fb2daf9bf90>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fb2daf9bf90>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Conv object at 0x7fb2daf99a10>\n",
      "A = NDArray([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e-01]\n",
      " ...80155873e+00  3.11298299e+00 -3.24756050e+00 ... -7.96233535e-01\n",
      "     4.72636795e+00 -5.75466204e+00]]]], device=cpu())\n",
      "B = NDArray([[[[ 5.93117094e+00 -4.61600447e+00 -7.43411541e+00 ...  9.49053860e+00\n",
      "     6.47611380e+00 -3.38525438e+00]\n",
      " ...93575430e+00 -6.98609781e+00  1.63659811e+00 ...  8.22474575e+00\n",
      "     2.56555057e+00  1.63004756e+00]]]], device=cpu())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, A, B):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "A          = NDArray([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e-01]\n",
      " ...80155873e+00  3.11298299e+00 -3.24756050e+00 ... -7.96233535e-01\n",
      "     4.72636795e+00 -5.75466204e+00]]]], device=cpu())\n",
      "B          = NDArray([[[[ 5.93117094e+00 -4.61600447e+00 -7.43411541e+00 ...  9.49053860e+00\n",
      "     6.47611380e+00 -3.38525438e+00]\n",
      " ...93575430e+00 -6.98609781e+00  1.63659811e+00 ...  8.22474575e+00\n",
      "     2.56555057e+00  1.63004756e+00]]]], device=cpu())\n",
      "self       = <needle.ops.ops_mathematic.Conv object at 0x7fb2daf99a10>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:504: NotImplementedError\n",
      "\u001b[31m\u001b[1m_ test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape13-W_shape13-1-0] _\u001b[0m\n",
      "\n",
      "Z_shape = (3, 17, 17, 1), W_shape = (5, 5, 1, 16), stride = 1, padding = 0\n",
      "backward = False, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mimport\u001b[39;49;00m \u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "W          = needle.Tensor([[[[  1.5437562   -6.8538       4.3282647    5.4068804   -3.15688\n",
      "     -1.206689    -4.3909516    3.4969...448   -2.242325     0.6578698\n",
      "     -7.0278      -1.7489109   10.11736      2.5269346    1.7962458\n",
      "     -7.9124722 ]]]])\n",
      "W_shape    = (5, 5, 1, 16)\n",
      "Z          = needle.Tensor([[[[ 8.82026196e+00]\n",
      "   [ 2.00078607e+00]\n",
      "   [ 4.89369011e+00]\n",
      "   [ 1.12044659e+01]\n",
      "   [ 9.33778954e+00]...22066e-01]\n",
      "   [ 1.05310105e-01]\n",
      "   [ 4.97272283e-01]\n",
      "   [ 1.13696384e+00]\n",
      "   [-5.08369303e+00]\n",
      "   [-5.73876619e-01]]]])\n",
      "Z_shape    = (3, 17, 17, 1)\n",
      "_W         = array([[[[  1.5437562 ,  -6.8538    ,   4.3282647 ,   5.4068804 ,\n",
      "           -3.15688   ,  -1.206689  ,  -4.3909516 , ...  -7.0278    ,  -1.7489109 ,\n",
      "           10.11736   ,   2.5269346 ,   1.7962458 ,  -7.9124722 ]]]],\n",
      "      dtype=float32)\n",
      "_Z         = array([[[[ 8.82026196e+00],\n",
      "         [ 2.00078607e+00],\n",
      "         [ 4.89369011e+00],\n",
      "         [ 1.12044659e+01],\n",
      "      ... 4.97272283e-01],\n",
      "         [ 1.13696384e+00],\n",
      "         [-5.08369303e+00],\n",
      "         [-5.73876619e-01]]]], dtype=float32)\n",
      "backward   = False\n",
      "device     = cpu()\n",
      "padding    = 0\n",
      "stride     = 1\n",
      "torch      = <module 'torch' from '/opt/conda/envs/train/lib/python3.11/site-packages/torch/__init__.py'>\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:429: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:514: in conv\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Conv(stride, padding)(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[[ 8.82026196e+00]\n",
      "   [ 2.00078607e+00]\n",
      "   [ 4.89369011e+00]\n",
      "   [ 1.12044659e+01]\n",
      "   [ 9.33778954e+00]...22066e-01]\n",
      "   [ 1.05310105e-01]\n",
      "   [ 4.97272283e-01]\n",
      "   [ 1.13696384e+00]\n",
      "   [-5.08369303e+00]\n",
      "   [-5.73876619e-01]]]])\n",
      "        b          = needle.Tensor([[[[  1.5437562   -6.8538       4.3282647    5.4068804   -3.15688\n",
      "     -1.206689    -4.3909516    3.4969...448   -2.242325     0.6578698\n",
      "     -7.0278      -1.7489109   10.11736      2.5269346    1.7962458\n",
      "     -7.9124722 ]]]])\n",
      "        padding    = 0\n",
      "        stride     = 1\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[ 8.82026196e+00]\n",
      "   [ 2.00078607e+00]\n",
      "   [ 4.89369011e+00]\n",
      "   [ 1.12044659e+01]\n",
      "   [ 9.33778954e+00...48   -2.242325     0.6578698\n",
      "     -7.0278      -1.7489109   10.11736      2.5269346    1.7962458\n",
      "     -7.9124722 ]]]]))\n",
      "        self       = <needle.ops.ops_mathematic.Conv object at 0x7fb2daff02d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[[ 8.82026196e+00]\n",
      "   [ 2.00078607e+00]\n",
      "   [ 4.89369011e+00]\n",
      "   [ 1.12044659e+01]\n",
      "   [ 9.33778954e+00...48   -2.242325     0.6578698\n",
      "     -7.0278      -1.7489109   10.11736      2.5269346    1.7962458\n",
      "     -7.9124722 ]]]]))\n",
      "        op         = <needle.ops.ops_mathematic.Conv object at 0x7fb2daff02d0>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fb2daff3a90>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fb2daff3a90>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Conv object at 0x7fb2daff02d0>\n",
      "A = NDArray([[[[ 8.82026196e+00]\n",
      "   [ 2.00078607e+00]\n",
      "   [ 4.89369011e+00]\n",
      "   [ 1.12044659e+01]\n",
      "   [ 9.33778954e+00]\n",
      "   [-...[ 1.05310105e-01]\n",
      "   [ 4.97272283e-01]\n",
      "   [ 1.13696384e+00]\n",
      "   [-5.08369303e+00]\n",
      "   [-5.73876619e-01]]]], device=cpu())\n",
      "B = NDArray([[[[  1.5437562   -6.8538       4.3282647    5.4068804   -3.15688\n",
      "     -1.206689    -4.3909516    3.4969025   ...5     0.6578698\n",
      "     -7.0278      -1.7489109   10.11736      2.5269346    1.7962458\n",
      "     -7.9124722 ]]]], device=cpu())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, A, B):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "A          = NDArray([[[[ 8.82026196e+00]\n",
      "   [ 2.00078607e+00]\n",
      "   [ 4.89369011e+00]\n",
      "   [ 1.12044659e+01]\n",
      "   [ 9.33778954e+00]\n",
      "   [-...[ 1.05310105e-01]\n",
      "   [ 4.97272283e-01]\n",
      "   [ 1.13696384e+00]\n",
      "   [-5.08369303e+00]\n",
      "   [-5.73876619e-01]]]], device=cpu())\n",
      "B          = NDArray([[[[  1.5437562   -6.8538       4.3282647    5.4068804   -3.15688\n",
      "     -1.206689    -4.3909516    3.4969025   ...5     0.6578698\n",
      "     -7.0278      -1.7489109   10.11736      2.5269346    1.7962458\n",
      "     -7.9124722 ]]]], device=cpu())\n",
      "self       = <needle.ops.ops_mathematic.Conv object at 0x7fb2daff02d0>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:504: NotImplementedError\n",
      "\u001b[31m\u001b[1m_ test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape14-W_shape14-1-0] _\u001b[0m\n",
      "\n",
      "Z_shape = (3, 17, 17, 16), W_shape = (5, 5, 16, 1), stride = 1, padding = 0\n",
      "backward = False, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mimport\u001b[39;49;00m \u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "W          = needle.Tensor([[[[ 1.55579513e-02]\n",
      "   [-3.04658723e+00]\n",
      "   [ 4.09725952e+00]\n",
      "   [ 1.29906273e+00]\n",
      "   [-7.50546598e+00]...75421e+00]\n",
      "   [ 3.02805209e+00]\n",
      "   [-4.20992136e+00]\n",
      "   [ 4.11707735e+00]\n",
      "   [ 4.99041653e+00]\n",
      "   [ 5.11040497e+00]]]])\n",
      "W_shape    = (5, 5, 16, 1)\n",
      "Z          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ...  6.08375072e-01\n",
      "     2.21931624e+00  1.66837168e...3e+00]\n",
      "   [-1.25365603e+00 -1.71452928e+00  1.35930243e+01 ...  3.13170171e+00\n",
      "    -1.35708165e+00  1.29997072e+01]]]])\n",
      "Z_shape    = (3, 17, 17, 16)\n",
      "_W         = array([[[[ 1.55579513e-02],\n",
      "         [-3.04658723e+00],\n",
      "         [ 4.09725952e+00],\n",
      "         [ 1.29906273e+00],\n",
      "      ...-4.20992136e+00],\n",
      "         [ 4.11707735e+00],\n",
      "         [ 4.99041653e+00],\n",
      "         [ 5.11040497e+00]]]], dtype=float32)\n",
      "_Z         = array([[[[ 8.82026196e+00,  2.00078607e+00,  4.89369011e+00, ...,\n",
      "           6.08375072e-01,  2.21931624e+00,  1.66837...452928e+00,  1.35930243e+01, ...,\n",
      "           3.13170171e+00, -1.35708165e+00,  1.29997072e+01]]]],\n",
      "      dtype=float32)\n",
      "backward   = False\n",
      "device     = cpu()\n",
      "padding    = 0\n",
      "stride     = 1\n",
      "torch      = <module 'torch' from '/opt/conda/envs/train/lib/python3.11/site-packages/torch/__init__.py'>\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:429: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:514: in conv\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Conv(stride, padding)(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ...  6.08375072e-01\n",
      "     2.21931624e+00  1.66837168e...3e+00]\n",
      "   [-1.25365603e+00 -1.71452928e+00  1.35930243e+01 ...  3.13170171e+00\n",
      "    -1.35708165e+00  1.29997072e+01]]]])\n",
      "        b          = needle.Tensor([[[[ 1.55579513e-02]\n",
      "   [-3.04658723e+00]\n",
      "   [ 4.09725952e+00]\n",
      "   [ 1.29906273e+00]\n",
      "   [-7.50546598e+00]...75421e+00]\n",
      "   [ 3.02805209e+00]\n",
      "   [-4.20992136e+00]\n",
      "   [ 4.11707735e+00]\n",
      "   [ 4.99041653e+00]\n",
      "   [ 5.11040497e+00]]]])\n",
      "        padding    = 0\n",
      "        stride     = 1\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ...  6.08375072e-01\n",
      "     2.21931624e+00  1.66837168...5421e+00]\n",
      "   [ 3.02805209e+00]\n",
      "   [-4.20992136e+00]\n",
      "   [ 4.11707735e+00]\n",
      "   [ 4.99041653e+00]\n",
      "   [ 5.11040497e+00]]]]))\n",
      "        self       = <needle.ops.ops_mathematic.Conv object at 0x7fb2daf03410>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ...  6.08375072e-01\n",
      "     2.21931624e+00  1.66837168...5421e+00]\n",
      "   [ 3.02805209e+00]\n",
      "   [-4.20992136e+00]\n",
      "   [ 4.11707735e+00]\n",
      "   [ 4.99041653e+00]\n",
      "   [ 5.11040497e+00]]]]))\n",
      "        op         = <needle.ops.ops_mathematic.Conv object at 0x7fb2daf03410>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fb2daf02610>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fb2daf02610>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Conv object at 0x7fb2daf03410>\n",
      "A = NDArray([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ...  6.08375072e-01\n",
      "     2.21931624e+00  1.66837168e+00]\n",
      " ...25365603e+00 -1.71452928e+00  1.35930243e+01 ...  3.13170171e+00\n",
      "    -1.35708165e+00  1.29997072e+01]]]], device=cpu())\n",
      "B = NDArray([[[[ 1.55579513e-02]\n",
      "   [-3.04658723e+00]\n",
      "   [ 4.09725952e+00]\n",
      "   [ 1.29906273e+00]\n",
      "   [-7.50546598e+00]\n",
      "   [-...[ 3.02805209e+00]\n",
      "   [-4.20992136e+00]\n",
      "   [ 4.11707735e+00]\n",
      "   [ 4.99041653e+00]\n",
      "   [ 5.11040497e+00]]]], device=cpu())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, A, B):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "A          = NDArray([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ...  6.08375072e-01\n",
      "     2.21931624e+00  1.66837168e+00]\n",
      " ...25365603e+00 -1.71452928e+00  1.35930243e+01 ...  3.13170171e+00\n",
      "    -1.35708165e+00  1.29997072e+01]]]], device=cpu())\n",
      "B          = NDArray([[[[ 1.55579513e-02]\n",
      "   [-3.04658723e+00]\n",
      "   [ 4.09725952e+00]\n",
      "   [ 1.29906273e+00]\n",
      "   [-7.50546598e+00]\n",
      "   [-...[ 3.02805209e+00]\n",
      "   [-4.20992136e+00]\n",
      "   [ 4.11707735e+00]\n",
      "   [ 4.99041653e+00]\n",
      "   [ 5.11040497e+00]]]], device=cpu())\n",
      "self       = <needle.ops.ops_mathematic.Conv object at 0x7fb2daf03410>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:504: NotImplementedError\n",
      "\u001b[31m\u001b[1m_ test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape15-W_shape15-1-0] _\u001b[0m\n",
      "\n",
      "Z_shape = (3, 17, 17, 16), W_shape = (1, 1, 16, 1), stride = 1, padding = 0\n",
      "backward = False, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mimport\u001b[39;49;00m \u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "W          = needle.Tensor([[[[ 0.01555795]\n",
      "   [-3.0465872 ]\n",
      "   [ 4.0972595 ]\n",
      "   [ 1.2990627 ]\n",
      "   [-7.505466  ]\n",
      "   [-1.7335238 ]\n",
      "  ...[ 5.6893935 ]\n",
      "   [ 2.267663  ]\n",
      "   [-1.0709513 ]\n",
      "   [-5.0566583 ]\n",
      "   [-0.23777105]\n",
      "   [-9.931785  ]\n",
      "   [-0.44905776]]]])\n",
      "W_shape    = (1, 1, 16, 1)\n",
      "Z          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ...  6.08375072e-01\n",
      "     2.21931624e+00  1.66837168e...3e+00]\n",
      "   [-1.25365603e+00 -1.71452928e+00  1.35930243e+01 ...  3.13170171e+00\n",
      "    -1.35708165e+00  1.29997072e+01]]]])\n",
      "Z_shape    = (3, 17, 17, 16)\n",
      "_W         = array([[[[ 0.01555795],\n",
      "         [-3.0465872 ],\n",
      "         [ 4.0972595 ],\n",
      "         [ 1.2990627 ],\n",
      "         [-7.505466  ]...13 ],\n",
      "         [-5.0566583 ],\n",
      "         [-0.23777105],\n",
      "         [-9.931785  ],\n",
      "         [-0.44905776]]]], dtype=float32)\n",
      "_Z         = array([[[[ 8.82026196e+00,  2.00078607e+00,  4.89369011e+00, ...,\n",
      "           6.08375072e-01,  2.21931624e+00,  1.66837...452928e+00,  1.35930243e+01, ...,\n",
      "           3.13170171e+00, -1.35708165e+00,  1.29997072e+01]]]],\n",
      "      dtype=float32)\n",
      "backward   = False\n",
      "device     = cpu()\n",
      "padding    = 0\n",
      "stride     = 1\n",
      "torch      = <module 'torch' from '/opt/conda/envs/train/lib/python3.11/site-packages/torch/__init__.py'>\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:429: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:514: in conv\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Conv(stride, padding)(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ...  6.08375072e-01\n",
      "     2.21931624e+00  1.66837168e...3e+00]\n",
      "   [-1.25365603e+00 -1.71452928e+00  1.35930243e+01 ...  3.13170171e+00\n",
      "    -1.35708165e+00  1.29997072e+01]]]])\n",
      "        b          = needle.Tensor([[[[ 0.01555795]\n",
      "   [-3.0465872 ]\n",
      "   [ 4.0972595 ]\n",
      "   [ 1.2990627 ]\n",
      "   [-7.505466  ]\n",
      "   [-1.7335238 ]\n",
      "  ...[ 5.6893935 ]\n",
      "   [ 2.267663  ]\n",
      "   [-1.0709513 ]\n",
      "   [-5.0566583 ]\n",
      "   [-0.23777105]\n",
      "   [-9.931785  ]\n",
      "   [-0.44905776]]]])\n",
      "        padding    = 0\n",
      "        stride     = 1\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ...  6.08375072e-01\n",
      "     2.21931624e+00  1.66837168... 5.6893935 ]\n",
      "   [ 2.267663  ]\n",
      "   [-1.0709513 ]\n",
      "   [-5.0566583 ]\n",
      "   [-0.23777105]\n",
      "   [-9.931785  ]\n",
      "   [-0.44905776]]]]))\n",
      "        self       = <needle.ops.ops_mathematic.Conv object at 0x7fb2dafa2190>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ...  6.08375072e-01\n",
      "     2.21931624e+00  1.66837168... 5.6893935 ]\n",
      "   [ 2.267663  ]\n",
      "   [-1.0709513 ]\n",
      "   [-5.0566583 ]\n",
      "   [-0.23777105]\n",
      "   [-9.931785  ]\n",
      "   [-0.44905776]]]]))\n",
      "        op         = <needle.ops.ops_mathematic.Conv object at 0x7fb2dafa2190>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fb2dafa0ad0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fb2dafa0ad0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Conv object at 0x7fb2dafa2190>\n",
      "A = NDArray([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ...  6.08375072e-01\n",
      "     2.21931624e+00  1.66837168e+00]\n",
      " ...25365603e+00 -1.71452928e+00  1.35930243e+01 ...  3.13170171e+00\n",
      "    -1.35708165e+00  1.29997072e+01]]]], device=cpu())\n",
      "B = NDArray([[[[ 0.01555795]\n",
      "   [-3.0465872 ]\n",
      "   [ 4.0972595 ]\n",
      "   [ 1.2990627 ]\n",
      "   [-7.505466  ]\n",
      "   [-1.7335238 ]\n",
      "   [-2.5...   [ 2.267663  ]\n",
      "   [-1.0709513 ]\n",
      "   [-5.0566583 ]\n",
      "   [-0.23777105]\n",
      "   [-9.931785  ]\n",
      "   [-0.44905776]]]], device=cpu())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, A, B):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "A          = NDArray([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ...  6.08375072e-01\n",
      "     2.21931624e+00  1.66837168e+00]\n",
      " ...25365603e+00 -1.71452928e+00  1.35930243e+01 ...  3.13170171e+00\n",
      "    -1.35708165e+00  1.29997072e+01]]]], device=cpu())\n",
      "B          = NDArray([[[[ 0.01555795]\n",
      "   [-3.0465872 ]\n",
      "   [ 4.0972595 ]\n",
      "   [ 1.2990627 ]\n",
      "   [-7.505466  ]\n",
      "   [-1.7335238 ]\n",
      "   [-2.5...   [ 2.267663  ]\n",
      "   [-1.0709513 ]\n",
      "   [-5.0566583 ]\n",
      "   [-0.23777105]\n",
      "   [-9.931785  ]\n",
      "   [-0.44905776]]]], device=cpu())\n",
      "self       = <needle.ops.ops_mathematic.Conv object at 0x7fb2dafa2190>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:504: NotImplementedError\n",
      "\u001b[31m\u001b[1m_ test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape16-W_shape16-1-0] _\u001b[0m\n",
      "\n",
      "Z_shape = (1, 14, 14, 2), W_shape = (3, 3, 2, 2), stride = 1, padding = 0\n",
      "backward = False, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mimport\u001b[39;49;00m \u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "W          = needle.Tensor([[[[ -1.7671587   -8.082371  ]\n",
      "   [ -1.4591868   -3.807461  ]]\n",
      "\n",
      "  [[  4.2896194    5.705509  ]\n",
      "   [  7.3...7526     2.911123  ]\n",
      "   [-10.473016     0.61860955]]\n",
      "\n",
      "  [[ -0.65053475   0.46976614]\n",
      "   [  4.7152305  -13.698386  ]]]])\n",
      "W_shape    = (3, 3, 2, 2)\n",
      "Z          = needle.Tensor([[[[  8.820262     2.000786  ]\n",
      "   [  4.89369     11.204466  ]\n",
      "   [  9.33779     -4.8863893 ]\n",
      "   [  4.750...19315276  -8.283575  ]\n",
      "   [ -4.9275537   -7.359175  ]\n",
      "   [  8.240675     0.8211388 ]\n",
      "   [  2.8364513   -1.1133755 ]]]])\n",
      "Z_shape    = (1, 14, 14, 2)\n",
      "_W         = array([[[[ -1.7671587 ,  -8.082371  ],\n",
      "         [ -1.4591868 ,  -3.807461  ]],\n",
      "\n",
      "        [[  4.2896194 ,   5.705509  ],...016  ,   0.61860955]],\n",
      "\n",
      "        [[ -0.65053475,   0.46976614],\n",
      "         [  4.7152305 , -13.698386  ]]]], dtype=float32)\n",
      "_Z         = array([[[[  8.820262  ,   2.000786  ],\n",
      "         [  4.89369   ,  11.204466  ],\n",
      "         [  9.33779   ,  -4.8863893 ],\n",
      " ...275537 ,  -7.359175  ],\n",
      "         [  8.240675  ,   0.8211388 ],\n",
      "         [  2.8364513 ,  -1.1133755 ]]]], dtype=float32)\n",
      "backward   = False\n",
      "device     = cpu()\n",
      "padding    = 0\n",
      "stride     = 1\n",
      "torch      = <module 'torch' from '/opt/conda/envs/train/lib/python3.11/site-packages/torch/__init__.py'>\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:429: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:514: in conv\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Conv(stride, padding)(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[[  8.820262     2.000786  ]\n",
      "   [  4.89369     11.204466  ]\n",
      "   [  9.33779     -4.8863893 ]\n",
      "   [  4.750...19315276  -8.283575  ]\n",
      "   [ -4.9275537   -7.359175  ]\n",
      "   [  8.240675     0.8211388 ]\n",
      "   [  2.8364513   -1.1133755 ]]]])\n",
      "        b          = needle.Tensor([[[[ -1.7671587   -8.082371  ]\n",
      "   [ -1.4591868   -3.807461  ]]\n",
      "\n",
      "  [[  4.2896194    5.705509  ]\n",
      "   [  7.3...7526     2.911123  ]\n",
      "   [-10.473016     0.61860955]]\n",
      "\n",
      "  [[ -0.65053475   0.46976614]\n",
      "   [  4.7152305  -13.698386  ]]]])\n",
      "        padding    = 0\n",
      "        stride     = 1\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[  8.820262     2.000786  ]\n",
      "   [  4.89369     11.204466  ]\n",
      "   [  9.33779     -4.8863893 ]\n",
      "   [  4.75...526     2.911123  ]\n",
      "   [-10.473016     0.61860955]]\n",
      "\n",
      "  [[ -0.65053475   0.46976614]\n",
      "   [  4.7152305  -13.698386  ]]]]))\n",
      "        self       = <needle.ops.ops_mathematic.Conv object at 0x7fb2daf09e90>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[[  8.820262     2.000786  ]\n",
      "   [  4.89369     11.204466  ]\n",
      "   [  9.33779     -4.8863893 ]\n",
      "   [  4.75...526     2.911123  ]\n",
      "   [-10.473016     0.61860955]]\n",
      "\n",
      "  [[ -0.65053475   0.46976614]\n",
      "   [  4.7152305  -13.698386  ]]]]))\n",
      "        op         = <needle.ops.ops_mathematic.Conv object at 0x7fb2daf09e90>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fb2daf08a50>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fb2daf08a50>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Conv object at 0x7fb2daf09e90>\n",
      "A = NDArray([[[[  8.820262     2.000786  ]\n",
      "   [  4.89369     11.204466  ]\n",
      "   [  9.33779     -4.8863893 ]\n",
      "   [  4.750442   ...83575  ]\n",
      "   [ -4.9275537   -7.359175  ]\n",
      "   [  8.240675     0.8211388 ]\n",
      "   [  2.8364513   -1.1133755 ]]]], device=cpu())\n",
      "B = NDArray([[[[ -1.7671587   -8.082371  ]\n",
      "   [ -1.4591868   -3.807461  ]]\n",
      "\n",
      "  [[  4.2896194    5.705509  ]\n",
      "   [  7.3328934...123  ]\n",
      "   [-10.473016     0.61860955]]\n",
      "\n",
      "  [[ -0.65053475   0.46976614]\n",
      "   [  4.7152305  -13.698386  ]]]], device=cpu())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, A, B):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "A          = NDArray([[[[  8.820262     2.000786  ]\n",
      "   [  4.89369     11.204466  ]\n",
      "   [  9.33779     -4.8863893 ]\n",
      "   [  4.750442   ...83575  ]\n",
      "   [ -4.9275537   -7.359175  ]\n",
      "   [  8.240675     0.8211388 ]\n",
      "   [  2.8364513   -1.1133755 ]]]], device=cpu())\n",
      "B          = NDArray([[[[ -1.7671587   -8.082371  ]\n",
      "   [ -1.4591868   -3.807461  ]]\n",
      "\n",
      "  [[  4.2896194    5.705509  ]\n",
      "   [  7.3328934...123  ]\n",
      "   [-10.473016     0.61860955]]\n",
      "\n",
      "  [[ -0.65053475   0.46976614]\n",
      "   [  4.7152305  -13.698386  ]]]], device=cpu())\n",
      "self       = <needle.ops.ops_mathematic.Conv object at 0x7fb2daf09e90>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:504: NotImplementedError\n",
      "\u001b[31m\u001b[1m_ test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape0-W_shape0-1-0] _\u001b[0m\n",
      "\n",
      "Z_shape = (3, 14, 14, 8), W_shape = (3, 3, 8, 16), stride = 1, padding = 0\n",
      "backward = False, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mimport\u001b[39;49;00m \u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "W          = needle.Tensor([[[[  4.282669     8.133604    -4.6223483  ...   2.0798197\n",
      "      2.1704152   -0.38803983]\n",
      "   [  2.245844...2.618842     0.71487164]\n",
      "   [ -4.09374      3.7291534  -10.551835   ...   0.31741843\n",
      "      0.03223436   2.0512383 ]]]])\n",
      "W_shape    = (3, 3, 8, 16)\n",
      "Z          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e...3e+00]\n",
      "   [ 1.09905624e+01  2.23166728e+00  4.62055349e+00 ... -6.23090088e-01\n",
      "    -8.48226726e-01 -2.29984760e+00]]]])\n",
      "Z_shape    = (3, 14, 14, 8)\n",
      "_W         = array([[[[  4.282669  ,   8.133604  ,  -4.6223483 , ...,   2.0798197 ,\n",
      "            2.1704152 ,  -0.38803983],\n",
      "        ... [ -4.09374   ,   3.7291534 , -10.551835  , ...,   0.31741843,\n",
      "            0.03223436,   2.0512383 ]]]], dtype=float32)\n",
      "_Z         = array([[[[ 8.82026196e+00,  2.00078607e+00,  4.89369011e+00, ...,\n",
      "          -4.88638926e+00,  4.75044203e+00, -7.56786...166728e+00,  4.62055349e+00, ...,\n",
      "          -6.23090088e-01, -8.48226726e-01, -2.29984760e+00]]]],\n",
      "      dtype=float32)\n",
      "backward   = False\n",
      "device     = cuda()\n",
      "padding    = 0\n",
      "stride     = 1\n",
      "torch      = <module 'torch' from '/opt/conda/envs/train/lib/python3.11/site-packages/torch/__init__.py'>\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:429: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:514: in conv\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Conv(stride, padding)(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e...3e+00]\n",
      "   [ 1.09905624e+01  2.23166728e+00  4.62055349e+00 ... -6.23090088e-01\n",
      "    -8.48226726e-01 -2.29984760e+00]]]])\n",
      "        b          = needle.Tensor([[[[  4.282669     8.133604    -4.6223483  ...   2.0798197\n",
      "      2.1704152   -0.38803983]\n",
      "   [  2.245844...2.618842     0.71487164]\n",
      "   [ -4.09374      3.7291534  -10.551835   ...   0.31741843\n",
      "      0.03223436   2.0512383 ]]]])\n",
      "        padding    = 0\n",
      "        stride     = 1\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048....618842     0.71487164]\n",
      "   [ -4.09374      3.7291534  -10.551835   ...   0.31741843\n",
      "      0.03223436   2.0512383 ]]]]))\n",
      "        self       = <needle.ops.ops_mathematic.Conv object at 0x7fb2db241e90>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048....618842     0.71487164]\n",
      "   [ -4.09374      3.7291534  -10.551835   ...   0.31741843\n",
      "      0.03223436   2.0512383 ]]]]))\n",
      "        op         = <needle.ops.ops_mathematic.Conv object at 0x7fb2db241e90>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fb2db243f50>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fb2db243f50>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Conv object at 0x7fb2db241e90>\n",
      "A = NDArray([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e-01]\n",
      " ...9905624e+01  2.23166728e+00  4.62055349e+00 ... -6.23090088e-01\n",
      "    -8.48226726e-01 -2.29984760e+00]]]], device=cuda())\n",
      "B = NDArray([[[[  4.282669     8.133604    -4.6223483  ...   2.0798197\n",
      "      2.1704152   -0.38803983]\n",
      "   [  2.2458448   -0...71487164]\n",
      "   [ -4.09374      3.7291534  -10.551835   ...   0.31741843\n",
      "      0.03223436   2.0512383 ]]]], device=cuda())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, A, B):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "A          = NDArray([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e-01]\n",
      " ...9905624e+01  2.23166728e+00  4.62055349e+00 ... -6.23090088e-01\n",
      "    -8.48226726e-01 -2.29984760e+00]]]], device=cuda())\n",
      "B          = NDArray([[[[  4.282669     8.133604    -4.6223483  ...   2.0798197\n",
      "      2.1704152   -0.38803983]\n",
      "   [  2.2458448   -0...71487164]\n",
      "   [ -4.09374      3.7291534  -10.551835   ...   0.31741843\n",
      "      0.03223436   2.0512383 ]]]], device=cuda())\n",
      "self       = <needle.ops.ops_mathematic.Conv object at 0x7fb2db241e90>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:504: NotImplementedError\n",
      "\u001b[31m\u001b[1m_ test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape1-W_shape1-1-1] _\u001b[0m\n",
      "\n",
      "Z_shape = (3, 14, 14, 8), W_shape = (3, 3, 8, 16), stride = 1, padding = 1\n",
      "backward = False, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mimport\u001b[39;49;00m \u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "W          = needle.Tensor([[[[  4.282669     8.133604    -4.6223483  ...   2.0798197\n",
      "      2.1704152   -0.38803983]\n",
      "   [  2.245844...2.618842     0.71487164]\n",
      "   [ -4.09374      3.7291534  -10.551835   ...   0.31741843\n",
      "      0.03223436   2.0512383 ]]]])\n",
      "W_shape    = (3, 3, 8, 16)\n",
      "Z          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e...3e+00]\n",
      "   [ 1.09905624e+01  2.23166728e+00  4.62055349e+00 ... -6.23090088e-01\n",
      "    -8.48226726e-01 -2.29984760e+00]]]])\n",
      "Z_shape    = (3, 14, 14, 8)\n",
      "_W         = array([[[[  4.282669  ,   8.133604  ,  -4.6223483 , ...,   2.0798197 ,\n",
      "            2.1704152 ,  -0.38803983],\n",
      "        ... [ -4.09374   ,   3.7291534 , -10.551835  , ...,   0.31741843,\n",
      "            0.03223436,   2.0512383 ]]]], dtype=float32)\n",
      "_Z         = array([[[[ 8.82026196e+00,  2.00078607e+00,  4.89369011e+00, ...,\n",
      "          -4.88638926e+00,  4.75044203e+00, -7.56786...166728e+00,  4.62055349e+00, ...,\n",
      "          -6.23090088e-01, -8.48226726e-01, -2.29984760e+00]]]],\n",
      "      dtype=float32)\n",
      "backward   = False\n",
      "device     = cuda()\n",
      "padding    = 1\n",
      "stride     = 1\n",
      "torch      = <module 'torch' from '/opt/conda/envs/train/lib/python3.11/site-packages/torch/__init__.py'>\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:429: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:514: in conv\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Conv(stride, padding)(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e...3e+00]\n",
      "   [ 1.09905624e+01  2.23166728e+00  4.62055349e+00 ... -6.23090088e-01\n",
      "    -8.48226726e-01 -2.29984760e+00]]]])\n",
      "        b          = needle.Tensor([[[[  4.282669     8.133604    -4.6223483  ...   2.0798197\n",
      "      2.1704152   -0.38803983]\n",
      "   [  2.245844...2.618842     0.71487164]\n",
      "   [ -4.09374      3.7291534  -10.551835   ...   0.31741843\n",
      "      0.03223436   2.0512383 ]]]])\n",
      "        padding    = 1\n",
      "        stride     = 1\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048....618842     0.71487164]\n",
      "   [ -4.09374      3.7291534  -10.551835   ...   0.31741843\n",
      "      0.03223436   2.0512383 ]]]]))\n",
      "        self       = <needle.ops.ops_mathematic.Conv object at 0x7fb2db0cee90>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048....618842     0.71487164]\n",
      "   [ -4.09374      3.7291534  -10.551835   ...   0.31741843\n",
      "      0.03223436   2.0512383 ]]]]))\n",
      "        op         = <needle.ops.ops_mathematic.Conv object at 0x7fb2db0cee90>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fb2db0cd090>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fb2db0cd090>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Conv object at 0x7fb2db0cee90>\n",
      "A = NDArray([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e-01]\n",
      " ...9905624e+01  2.23166728e+00  4.62055349e+00 ... -6.23090088e-01\n",
      "    -8.48226726e-01 -2.29984760e+00]]]], device=cuda())\n",
      "B = NDArray([[[[  4.282669     8.133604    -4.6223483  ...   2.0798197\n",
      "      2.1704152   -0.38803983]\n",
      "   [  2.2458448   -0...71487164]\n",
      "   [ -4.09374      3.7291534  -10.551835   ...   0.31741843\n",
      "      0.03223436   2.0512383 ]]]], device=cuda())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, A, B):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "A          = NDArray([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e-01]\n",
      " ...9905624e+01  2.23166728e+00  4.62055349e+00 ... -6.23090088e-01\n",
      "    -8.48226726e-01 -2.29984760e+00]]]], device=cuda())\n",
      "B          = NDArray([[[[  4.282669     8.133604    -4.6223483  ...   2.0798197\n",
      "      2.1704152   -0.38803983]\n",
      "   [  2.2458448   -0...71487164]\n",
      "   [ -4.09374      3.7291534  -10.551835   ...   0.31741843\n",
      "      0.03223436   2.0512383 ]]]], device=cuda())\n",
      "self       = <needle.ops.ops_mathematic.Conv object at 0x7fb2db0cee90>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:504: NotImplementedError\n",
      "\u001b[31m\u001b[1m_ test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape2-W_shape2-1-2] _\u001b[0m\n",
      "\n",
      "Z_shape = (3, 16, 16, 8), W_shape = (3, 3, 8, 16), stride = 1, padding = 2\n",
      "backward = False, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mimport\u001b[39;49;00m \u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "W          = needle.Tensor([[[[  0.40910086  -0.64982927  11.320294   ...  -0.12077556\n",
      "      1.428692    -5.5845156 ]\n",
      "   [ -6.89178...-1.5437248  -10.136281  ]\n",
      "   [  1.1511405    0.7441038   16.463472   ...  -6.8931007\n",
      "     -0.5833115   -2.504636  ]]]])\n",
      "W_shape    = (3, 3, 8, 16)\n",
      "Z          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e...5e+00]\n",
      "   [ 6.10367346e+00 -2.05434513e+00 -4.41384506e+00 ... -6.15245628e+00\n",
      "     3.33514667e+00 -1.35630560e+00]]]])\n",
      "Z_shape    = (3, 16, 16, 8)\n",
      "_W         = array([[[[  0.40910086,  -0.64982927,  11.320294  , ...,  -0.12077556,\n",
      "            1.428692  ,  -5.5845156 ],\n",
      "        ... [  1.1511405 ,   0.7441038 ,  16.463472  , ...,  -6.8931007 ,\n",
      "           -0.5833115 ,  -2.504636  ]]]], dtype=float32)\n",
      "_Z         = array([[[[ 8.82026196e+00,  2.00078607e+00,  4.89369011e+00, ...,\n",
      "          -4.88638926e+00,  4.75044203e+00, -7.56786...434513e+00, -4.41384506e+00, ...,\n",
      "          -6.15245628e+00,  3.33514667e+00, -1.35630560e+00]]]],\n",
      "      dtype=float32)\n",
      "backward   = False\n",
      "device     = cuda()\n",
      "padding    = 2\n",
      "stride     = 1\n",
      "torch      = <module 'torch' from '/opt/conda/envs/train/lib/python3.11/site-packages/torch/__init__.py'>\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:429: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:514: in conv\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Conv(stride, padding)(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e...5e+00]\n",
      "   [ 6.10367346e+00 -2.05434513e+00 -4.41384506e+00 ... -6.15245628e+00\n",
      "     3.33514667e+00 -1.35630560e+00]]]])\n",
      "        b          = needle.Tensor([[[[  0.40910086  -0.64982927  11.320294   ...  -0.12077556\n",
      "      1.428692    -5.5845156 ]\n",
      "   [ -6.89178...-1.5437248  -10.136281  ]\n",
      "   [  1.1511405    0.7441038   16.463472   ...  -6.8931007\n",
      "     -0.5833115   -2.504636  ]]]])\n",
      "        padding    = 2\n",
      "        stride     = 1\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048...1.5437248  -10.136281  ]\n",
      "   [  1.1511405    0.7441038   16.463472   ...  -6.8931007\n",
      "     -0.5833115   -2.504636  ]]]]))\n",
      "        self       = <needle.ops.ops_mathematic.Conv object at 0x7fb2db0700d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048...1.5437248  -10.136281  ]\n",
      "   [  1.1511405    0.7441038   16.463472   ...  -6.8931007\n",
      "     -0.5833115   -2.504636  ]]]]))\n",
      "        op         = <needle.ops.ops_mathematic.Conv object at 0x7fb2db0700d0>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fb2db070790>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fb2db070790>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Conv object at 0x7fb2db0700d0>\n",
      "A = NDArray([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e-01]\n",
      " ...0367346e+00 -2.05434513e+00 -4.41384506e+00 ... -6.15245628e+00\n",
      "     3.33514667e+00 -1.35630560e+00]]]], device=cuda())\n",
      "B = NDArray([[[[  0.40910086  -0.64982927  11.320294   ...  -0.12077556\n",
      "      1.428692    -5.5845156 ]\n",
      "   [ -6.891782    -....136281  ]\n",
      "   [  1.1511405    0.7441038   16.463472   ...  -6.8931007\n",
      "     -0.5833115   -2.504636  ]]]], device=cuda())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, A, B):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "A          = NDArray([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e-01]\n",
      " ...0367346e+00 -2.05434513e+00 -4.41384506e+00 ... -6.15245628e+00\n",
      "     3.33514667e+00 -1.35630560e+00]]]], device=cuda())\n",
      "B          = NDArray([[[[  0.40910086  -0.64982927  11.320294   ...  -0.12077556\n",
      "      1.428692    -5.5845156 ]\n",
      "   [ -6.891782    -....136281  ]\n",
      "   [  1.1511405    0.7441038   16.463472   ...  -6.8931007\n",
      "     -0.5833115   -2.504636  ]]]], device=cuda())\n",
      "self       = <needle.ops.ops_mathematic.Conv object at 0x7fb2db0700d0>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:504: NotImplementedError\n",
      "\u001b[31m\u001b[1m_ test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape3-W_shape3-1-0] _\u001b[0m\n",
      "\n",
      "Z_shape = (3, 16, 16, 8), W_shape = (3, 3, 8, 14), stride = 1, padding = 0\n",
      "backward = False, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mimport\u001b[39;49;00m \u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "W          = needle.Tensor([[[[ 4.09100860e-01 -6.49829268e-01  1.13202944e+01 ... -5.56089449e+00\n",
      "    -4.84385538e+00 -1.20775558e...5e+00]\n",
      "   [-1.10714078e+00  2.54448557e+00 -7.73867989e+00 ... -1.13336074e+00\n",
      "    -1.21658230e+00 -4.79330921e+00]]]])\n",
      "W_shape    = (3, 3, 8, 14)\n",
      "Z          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e...5e+00]\n",
      "   [ 6.10367346e+00 -2.05434513e+00 -4.41384506e+00 ... -6.15245628e+00\n",
      "     3.33514667e+00 -1.35630560e+00]]]])\n",
      "Z_shape    = (3, 16, 16, 8)\n",
      "_W         = array([[[[ 4.09100860e-01, -6.49829268e-01,  1.13202944e+01, ...,\n",
      "          -5.56089449e+00, -4.84385538e+00, -1.20775...448557e+00, -7.73867989e+00, ...,\n",
      "          -1.13336074e+00, -1.21658230e+00, -4.79330921e+00]]]],\n",
      "      dtype=float32)\n",
      "_Z         = array([[[[ 8.82026196e+00,  2.00078607e+00,  4.89369011e+00, ...,\n",
      "          -4.88638926e+00,  4.75044203e+00, -7.56786...434513e+00, -4.41384506e+00, ...,\n",
      "          -6.15245628e+00,  3.33514667e+00, -1.35630560e+00]]]],\n",
      "      dtype=float32)\n",
      "backward   = False\n",
      "device     = cuda()\n",
      "padding    = 0\n",
      "stride     = 1\n",
      "torch      = <module 'torch' from '/opt/conda/envs/train/lib/python3.11/site-packages/torch/__init__.py'>\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:429: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:514: in conv\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Conv(stride, padding)(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e...5e+00]\n",
      "   [ 6.10367346e+00 -2.05434513e+00 -4.41384506e+00 ... -6.15245628e+00\n",
      "     3.33514667e+00 -1.35630560e+00]]]])\n",
      "        b          = needle.Tensor([[[[ 4.09100860e-01 -6.49829268e-01  1.13202944e+01 ... -5.56089449e+00\n",
      "    -4.84385538e+00 -1.20775558e...5e+00]\n",
      "   [-1.10714078e+00  2.54448557e+00 -7.73867989e+00 ... -1.13336074e+00\n",
      "    -1.21658230e+00 -4.79330921e+00]]]])\n",
      "        padding    = 0\n",
      "        stride     = 1\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048...e+00]\n",
      "   [-1.10714078e+00  2.54448557e+00 -7.73867989e+00 ... -1.13336074e+00\n",
      "    -1.21658230e+00 -4.79330921e+00]]]]))\n",
      "        self       = <needle.ops.ops_mathematic.Conv object at 0x7fb2dafda810>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048...e+00]\n",
      "   [-1.10714078e+00  2.54448557e+00 -7.73867989e+00 ... -1.13336074e+00\n",
      "    -1.21658230e+00 -4.79330921e+00]]]]))\n",
      "        op         = <needle.ops.ops_mathematic.Conv object at 0x7fb2dafda810>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fb2dafd90d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fb2dafd90d0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Conv object at 0x7fb2dafda810>\n",
      "A = NDArray([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e-01]\n",
      " ...0367346e+00 -2.05434513e+00 -4.41384506e+00 ... -6.15245628e+00\n",
      "     3.33514667e+00 -1.35630560e+00]]]], device=cuda())\n",
      "B = NDArray([[[[ 4.09100860e-01 -6.49829268e-01  1.13202944e+01 ... -5.56089449e+00\n",
      "    -4.84385538e+00 -1.20775558e-01]\n",
      " ...0714078e+00  2.54448557e+00 -7.73867989e+00 ... -1.13336074e+00\n",
      "    -1.21658230e+00 -4.79330921e+00]]]], device=cuda())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, A, B):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "A          = NDArray([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e-01]\n",
      " ...0367346e+00 -2.05434513e+00 -4.41384506e+00 ... -6.15245628e+00\n",
      "     3.33514667e+00 -1.35630560e+00]]]], device=cuda())\n",
      "B          = NDArray([[[[ 4.09100860e-01 -6.49829268e-01  1.13202944e+01 ... -5.56089449e+00\n",
      "    -4.84385538e+00 -1.20775558e-01]\n",
      " ...0714078e+00  2.54448557e+00 -7.73867989e+00 ... -1.13336074e+00\n",
      "    -1.21658230e+00 -4.79330921e+00]]]], device=cuda())\n",
      "self       = <needle.ops.ops_mathematic.Conv object at 0x7fb2dafda810>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:504: NotImplementedError\n",
      "\u001b[31m\u001b[1m_ test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape4-W_shape4-1-0] _\u001b[0m\n",
      "\n",
      "Z_shape = (3, 16, 16, 2), W_shape = (3, 3, 2, 14), stride = 1, padding = 0\n",
      "backward = False, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mimport\u001b[39;49;00m \u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "W          = needle.Tensor([[[[ -8.0968      -2.5552022    8.703147    -1.4674252    4.5861077\n",
      "     -0.28521433   4.383634    -9.13...727333   0.22582927   9.296732    -8.13161     -0.67411226\n",
      "     -2.9204676    1.675528   -12.187821     5.5746226 ]]]])\n",
      "W_shape    = (3, 3, 2, 14)\n",
      "Z          = needle.Tensor([[[[  8.820262     2.000786  ]\n",
      "   [  4.89369     11.204466  ]\n",
      "   [  9.33779     -4.8863893 ]\n",
      "   ...\n",
      "   [...   -5.112822  ]\n",
      "   ...\n",
      "   [ -7.922368     4.2222714 ]\n",
      "   [ -6.064339     1.4188478 ]\n",
      "   [ -1.4109794   -5.791016  ]]]])\n",
      "Z_shape    = (3, 16, 16, 2)\n",
      "_W         = array([[[[ -8.0968    ,  -2.5552022 ,   8.703147  ,  -1.4674252 ,\n",
      "            4.5861077 ,  -0.28521433,   4.383634  , ...        -8.13161   ,  -0.67411226,  -2.9204676 ,   1.675528  ,\n",
      "          -12.187821  ,   5.5746226 ]]]], dtype=float32)\n",
      "_Z         = array([[[[  8.820262  ,   2.000786  ],\n",
      "         [  4.89369   ,  11.204466  ],\n",
      "         [  9.33779   ,  -4.8863893 ],\n",
      " ...22368  ,   4.2222714 ],\n",
      "         [ -6.064339  ,   1.4188478 ],\n",
      "         [ -1.4109794 ,  -5.791016  ]]]], dtype=float32)\n",
      "backward   = False\n",
      "device     = cuda()\n",
      "padding    = 0\n",
      "stride     = 1\n",
      "torch      = <module 'torch' from '/opt/conda/envs/train/lib/python3.11/site-packages/torch/__init__.py'>\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:429: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:514: in conv\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Conv(stride, padding)(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[[  8.820262     2.000786  ]\n",
      "   [  4.89369     11.204466  ]\n",
      "   [  9.33779     -4.8863893 ]\n",
      "   ...\n",
      "   [...   -5.112822  ]\n",
      "   ...\n",
      "   [ -7.922368     4.2222714 ]\n",
      "   [ -6.064339     1.4188478 ]\n",
      "   [ -1.4109794   -5.791016  ]]]])\n",
      "        b          = needle.Tensor([[[[ -8.0968      -2.5552022    8.703147    -1.4674252    4.5861077\n",
      "     -0.28521433   4.383634    -9.13...727333   0.22582927   9.296732    -8.13161     -0.67411226\n",
      "     -2.9204676    1.675528   -12.187821     5.5746226 ]]]])\n",
      "        padding    = 0\n",
      "        stride     = 1\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[  8.820262     2.000786  ]\n",
      "   [  4.89369     11.204466  ]\n",
      "   [  9.33779     -4.8863893 ]\n",
      "   ...\n",
      "   ...27333   0.22582927   9.296732    -8.13161     -0.67411226\n",
      "     -2.9204676    1.675528   -12.187821     5.5746226 ]]]]))\n",
      "        self       = <needle.ops.ops_mathematic.Conv object at 0x7fb2db118e50>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[[  8.820262     2.000786  ]\n",
      "   [  4.89369     11.204466  ]\n",
      "   [  9.33779     -4.8863893 ]\n",
      "   ...\n",
      "   ...27333   0.22582927   9.296732    -8.13161     -0.67411226\n",
      "     -2.9204676    1.675528   -12.187821     5.5746226 ]]]]))\n",
      "        op         = <needle.ops.ops_mathematic.Conv object at 0x7fb2db118e50>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fb2db118290>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fb2db118290>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Conv object at 0x7fb2db118e50>\n",
      "A = NDArray([[[[  8.820262     2.000786  ]\n",
      "   [  4.89369     11.204466  ]\n",
      "   [  9.33779     -4.8863893 ]\n",
      "   ...\n",
      "   [  0.22...\n",
      "   ...\n",
      "   [ -7.922368     4.2222714 ]\n",
      "   [ -6.064339     1.4188478 ]\n",
      "   [ -1.4109794   -5.791016  ]]]], device=cuda())\n",
      "B = NDArray([[[[ -8.0968      -2.5552022    8.703147    -1.4674252    4.5861077\n",
      "     -0.28521433   4.383634    -9.134557  ...2927   9.296732    -8.13161     -0.67411226\n",
      "     -2.9204676    1.675528   -12.187821     5.5746226 ]]]], device=cuda())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, A, B):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "A          = NDArray([[[[  8.820262     2.000786  ]\n",
      "   [  4.89369     11.204466  ]\n",
      "   [  9.33779     -4.8863893 ]\n",
      "   ...\n",
      "   [  0.22...\n",
      "   ...\n",
      "   [ -7.922368     4.2222714 ]\n",
      "   [ -6.064339     1.4188478 ]\n",
      "   [ -1.4109794   -5.791016  ]]]], device=cuda())\n",
      "B          = NDArray([[[[ -8.0968      -2.5552022    8.703147    -1.4674252    4.5861077\n",
      "     -0.28521433   4.383634    -9.134557  ...2927   9.296732    -8.13161     -0.67411226\n",
      "     -2.9204676    1.675528   -12.187821     5.5746226 ]]]], device=cuda())\n",
      "self       = <needle.ops.ops_mathematic.Conv object at 0x7fb2db118e50>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:504: NotImplementedError\n",
      "\u001b[31m\u001b[1m_ test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape5-W_shape5-2-0] _\u001b[0m\n",
      "\n",
      "Z_shape = (3, 14, 14, 8), W_shape = (3, 3, 8, 16), stride = 2, padding = 0\n",
      "backward = False, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mimport\u001b[39;49;00m \u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "W          = needle.Tensor([[[[  4.282669     8.133604    -4.6223483  ...   2.0798197\n",
      "      2.1704152   -0.38803983]\n",
      "   [  2.245844...2.618842     0.71487164]\n",
      "   [ -4.09374      3.7291534  -10.551835   ...   0.31741843\n",
      "      0.03223436   2.0512383 ]]]])\n",
      "W_shape    = (3, 3, 8, 16)\n",
      "Z          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e...3e+00]\n",
      "   [ 1.09905624e+01  2.23166728e+00  4.62055349e+00 ... -6.23090088e-01\n",
      "    -8.48226726e-01 -2.29984760e+00]]]])\n",
      "Z_shape    = (3, 14, 14, 8)\n",
      "_W         = array([[[[  4.282669  ,   8.133604  ,  -4.6223483 , ...,   2.0798197 ,\n",
      "            2.1704152 ,  -0.38803983],\n",
      "        ... [ -4.09374   ,   3.7291534 , -10.551835  , ...,   0.31741843,\n",
      "            0.03223436,   2.0512383 ]]]], dtype=float32)\n",
      "_Z         = array([[[[ 8.82026196e+00,  2.00078607e+00,  4.89369011e+00, ...,\n",
      "          -4.88638926e+00,  4.75044203e+00, -7.56786...166728e+00,  4.62055349e+00, ...,\n",
      "          -6.23090088e-01, -8.48226726e-01, -2.29984760e+00]]]],\n",
      "      dtype=float32)\n",
      "backward   = False\n",
      "device     = cuda()\n",
      "padding    = 0\n",
      "stride     = 2\n",
      "torch      = <module 'torch' from '/opt/conda/envs/train/lib/python3.11/site-packages/torch/__init__.py'>\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:429: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:514: in conv\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Conv(stride, padding)(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e...3e+00]\n",
      "   [ 1.09905624e+01  2.23166728e+00  4.62055349e+00 ... -6.23090088e-01\n",
      "    -8.48226726e-01 -2.29984760e+00]]]])\n",
      "        b          = needle.Tensor([[[[  4.282669     8.133604    -4.6223483  ...   2.0798197\n",
      "      2.1704152   -0.38803983]\n",
      "   [  2.245844...2.618842     0.71487164]\n",
      "   [ -4.09374      3.7291534  -10.551835   ...   0.31741843\n",
      "      0.03223436   2.0512383 ]]]])\n",
      "        padding    = 0\n",
      "        stride     = 2\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048....618842     0.71487164]\n",
      "   [ -4.09374      3.7291534  -10.551835   ...   0.31741843\n",
      "      0.03223436   2.0512383 ]]]]))\n",
      "        self       = <needle.ops.ops_mathematic.Conv object at 0x7fb2daedc350>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048....618842     0.71487164]\n",
      "   [ -4.09374      3.7291534  -10.551835   ...   0.31741843\n",
      "      0.03223436   2.0512383 ]]]]))\n",
      "        op         = <needle.ops.ops_mathematic.Conv object at 0x7fb2daedc350>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fb2daedca90>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fb2daedca90>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Conv object at 0x7fb2daedc350>\n",
      "A = NDArray([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e-01]\n",
      " ...9905624e+01  2.23166728e+00  4.62055349e+00 ... -6.23090088e-01\n",
      "    -8.48226726e-01 -2.29984760e+00]]]], device=cuda())\n",
      "B = NDArray([[[[  4.282669     8.133604    -4.6223483  ...   2.0798197\n",
      "      2.1704152   -0.38803983]\n",
      "   [  2.2458448   -0...71487164]\n",
      "   [ -4.09374      3.7291534  -10.551835   ...   0.31741843\n",
      "      0.03223436   2.0512383 ]]]], device=cuda())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, A, B):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "A          = NDArray([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e-01]\n",
      " ...9905624e+01  2.23166728e+00  4.62055349e+00 ... -6.23090088e-01\n",
      "    -8.48226726e-01 -2.29984760e+00]]]], device=cuda())\n",
      "B          = NDArray([[[[  4.282669     8.133604    -4.6223483  ...   2.0798197\n",
      "      2.1704152   -0.38803983]\n",
      "   [  2.2458448   -0...71487164]\n",
      "   [ -4.09374      3.7291534  -10.551835   ...   0.31741843\n",
      "      0.03223436   2.0512383 ]]]], device=cuda())\n",
      "self       = <needle.ops.ops_mathematic.Conv object at 0x7fb2daedc350>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:504: NotImplementedError\n",
      "\u001b[31m\u001b[1m_ test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape6-W_shape6-2-1] _\u001b[0m\n",
      "\n",
      "Z_shape = (3, 14, 14, 8), W_shape = (3, 3, 8, 16), stride = 2, padding = 1\n",
      "backward = False, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mimport\u001b[39;49;00m \u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "W          = needle.Tensor([[[[  4.282669     8.133604    -4.6223483  ...   2.0798197\n",
      "      2.1704152   -0.38803983]\n",
      "   [  2.245844...2.618842     0.71487164]\n",
      "   [ -4.09374      3.7291534  -10.551835   ...   0.31741843\n",
      "      0.03223436   2.0512383 ]]]])\n",
      "W_shape    = (3, 3, 8, 16)\n",
      "Z          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e...3e+00]\n",
      "   [ 1.09905624e+01  2.23166728e+00  4.62055349e+00 ... -6.23090088e-01\n",
      "    -8.48226726e-01 -2.29984760e+00]]]])\n",
      "Z_shape    = (3, 14, 14, 8)\n",
      "_W         = array([[[[  4.282669  ,   8.133604  ,  -4.6223483 , ...,   2.0798197 ,\n",
      "            2.1704152 ,  -0.38803983],\n",
      "        ... [ -4.09374   ,   3.7291534 , -10.551835  , ...,   0.31741843,\n",
      "            0.03223436,   2.0512383 ]]]], dtype=float32)\n",
      "_Z         = array([[[[ 8.82026196e+00,  2.00078607e+00,  4.89369011e+00, ...,\n",
      "          -4.88638926e+00,  4.75044203e+00, -7.56786...166728e+00,  4.62055349e+00, ...,\n",
      "          -6.23090088e-01, -8.48226726e-01, -2.29984760e+00]]]],\n",
      "      dtype=float32)\n",
      "backward   = False\n",
      "device     = cuda()\n",
      "padding    = 1\n",
      "stride     = 2\n",
      "torch      = <module 'torch' from '/opt/conda/envs/train/lib/python3.11/site-packages/torch/__init__.py'>\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:429: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:514: in conv\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Conv(stride, padding)(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e...3e+00]\n",
      "   [ 1.09905624e+01  2.23166728e+00  4.62055349e+00 ... -6.23090088e-01\n",
      "    -8.48226726e-01 -2.29984760e+00]]]])\n",
      "        b          = needle.Tensor([[[[  4.282669     8.133604    -4.6223483  ...   2.0798197\n",
      "      2.1704152   -0.38803983]\n",
      "   [  2.245844...2.618842     0.71487164]\n",
      "   [ -4.09374      3.7291534  -10.551835   ...   0.31741843\n",
      "      0.03223436   2.0512383 ]]]])\n",
      "        padding    = 1\n",
      "        stride     = 2\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048....618842     0.71487164]\n",
      "   [ -4.09374      3.7291534  -10.551835   ...   0.31741843\n",
      "      0.03223436   2.0512383 ]]]]))\n",
      "        self       = <needle.ops.ops_mathematic.Conv object at 0x7fb2db02c410>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048....618842     0.71487164]\n",
      "   [ -4.09374      3.7291534  -10.551835   ...   0.31741843\n",
      "      0.03223436   2.0512383 ]]]]))\n",
      "        op         = <needle.ops.ops_mathematic.Conv object at 0x7fb2db02c410>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fb2db02df90>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fb2db02df90>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Conv object at 0x7fb2db02c410>\n",
      "A = NDArray([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e-01]\n",
      " ...9905624e+01  2.23166728e+00  4.62055349e+00 ... -6.23090088e-01\n",
      "    -8.48226726e-01 -2.29984760e+00]]]], device=cuda())\n",
      "B = NDArray([[[[  4.282669     8.133604    -4.6223483  ...   2.0798197\n",
      "      2.1704152   -0.38803983]\n",
      "   [  2.2458448   -0...71487164]\n",
      "   [ -4.09374      3.7291534  -10.551835   ...   0.31741843\n",
      "      0.03223436   2.0512383 ]]]], device=cuda())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, A, B):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "A          = NDArray([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e-01]\n",
      " ...9905624e+01  2.23166728e+00  4.62055349e+00 ... -6.23090088e-01\n",
      "    -8.48226726e-01 -2.29984760e+00]]]], device=cuda())\n",
      "B          = NDArray([[[[  4.282669     8.133604    -4.6223483  ...   2.0798197\n",
      "      2.1704152   -0.38803983]\n",
      "   [  2.2458448   -0...71487164]\n",
      "   [ -4.09374      3.7291534  -10.551835   ...   0.31741843\n",
      "      0.03223436   2.0512383 ]]]], device=cuda())\n",
      "self       = <needle.ops.ops_mathematic.Conv object at 0x7fb2db02c410>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:504: NotImplementedError\n",
      "\u001b[31m\u001b[1m_ test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape7-W_shape7-2-2] _\u001b[0m\n",
      "\n",
      "Z_shape = (3, 16, 16, 8), W_shape = (3, 3, 8, 16), stride = 2, padding = 2\n",
      "backward = False, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mimport\u001b[39;49;00m \u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "W          = needle.Tensor([[[[  0.40910086  -0.64982927  11.320294   ...  -0.12077556\n",
      "      1.428692    -5.5845156 ]\n",
      "   [ -6.89178...-1.5437248  -10.136281  ]\n",
      "   [  1.1511405    0.7441038   16.463472   ...  -6.8931007\n",
      "     -0.5833115   -2.504636  ]]]])\n",
      "W_shape    = (3, 3, 8, 16)\n",
      "Z          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e...5e+00]\n",
      "   [ 6.10367346e+00 -2.05434513e+00 -4.41384506e+00 ... -6.15245628e+00\n",
      "     3.33514667e+00 -1.35630560e+00]]]])\n",
      "Z_shape    = (3, 16, 16, 8)\n",
      "_W         = array([[[[  0.40910086,  -0.64982927,  11.320294  , ...,  -0.12077556,\n",
      "            1.428692  ,  -5.5845156 ],\n",
      "        ... [  1.1511405 ,   0.7441038 ,  16.463472  , ...,  -6.8931007 ,\n",
      "           -0.5833115 ,  -2.504636  ]]]], dtype=float32)\n",
      "_Z         = array([[[[ 8.82026196e+00,  2.00078607e+00,  4.89369011e+00, ...,\n",
      "          -4.88638926e+00,  4.75044203e+00, -7.56786...434513e+00, -4.41384506e+00, ...,\n",
      "          -6.15245628e+00,  3.33514667e+00, -1.35630560e+00]]]],\n",
      "      dtype=float32)\n",
      "backward   = False\n",
      "device     = cuda()\n",
      "padding    = 2\n",
      "stride     = 2\n",
      "torch      = <module 'torch' from '/opt/conda/envs/train/lib/python3.11/site-packages/torch/__init__.py'>\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:429: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:514: in conv\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Conv(stride, padding)(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e...5e+00]\n",
      "   [ 6.10367346e+00 -2.05434513e+00 -4.41384506e+00 ... -6.15245628e+00\n",
      "     3.33514667e+00 -1.35630560e+00]]]])\n",
      "        b          = needle.Tensor([[[[  0.40910086  -0.64982927  11.320294   ...  -0.12077556\n",
      "      1.428692    -5.5845156 ]\n",
      "   [ -6.89178...-1.5437248  -10.136281  ]\n",
      "   [  1.1511405    0.7441038   16.463472   ...  -6.8931007\n",
      "     -0.5833115   -2.504636  ]]]])\n",
      "        padding    = 2\n",
      "        stride     = 2\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048...1.5437248  -10.136281  ]\n",
      "   [  1.1511405    0.7441038   16.463472   ...  -6.8931007\n",
      "     -0.5833115   -2.504636  ]]]]))\n",
      "        self       = <needle.ops.ops_mathematic.Conv object at 0x7fb2daf0e190>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048...1.5437248  -10.136281  ]\n",
      "   [  1.1511405    0.7441038   16.463472   ...  -6.8931007\n",
      "     -0.5833115   -2.504636  ]]]]))\n",
      "        op         = <needle.ops.ops_mathematic.Conv object at 0x7fb2daf0e190>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fb2daf0d810>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fb2daf0d810>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Conv object at 0x7fb2daf0e190>\n",
      "A = NDArray([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e-01]\n",
      " ...0367346e+00 -2.05434513e+00 -4.41384506e+00 ... -6.15245628e+00\n",
      "     3.33514667e+00 -1.35630560e+00]]]], device=cuda())\n",
      "B = NDArray([[[[  0.40910086  -0.64982927  11.320294   ...  -0.12077556\n",
      "      1.428692    -5.5845156 ]\n",
      "   [ -6.891782    -....136281  ]\n",
      "   [  1.1511405    0.7441038   16.463472   ...  -6.8931007\n",
      "     -0.5833115   -2.504636  ]]]], device=cuda())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, A, B):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "A          = NDArray([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e-01]\n",
      " ...0367346e+00 -2.05434513e+00 -4.41384506e+00 ... -6.15245628e+00\n",
      "     3.33514667e+00 -1.35630560e+00]]]], device=cuda())\n",
      "B          = NDArray([[[[  0.40910086  -0.64982927  11.320294   ...  -0.12077556\n",
      "      1.428692    -5.5845156 ]\n",
      "   [ -6.891782    -....136281  ]\n",
      "   [  1.1511405    0.7441038   16.463472   ...  -6.8931007\n",
      "     -0.5833115   -2.504636  ]]]], device=cuda())\n",
      "self       = <needle.ops.ops_mathematic.Conv object at 0x7fb2daf0e190>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:504: NotImplementedError\n",
      "\u001b[31m\u001b[1m_ test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape8-W_shape8-2-0] _\u001b[0m\n",
      "\n",
      "Z_shape = (3, 16, 16, 8), W_shape = (3, 3, 8, 14), stride = 2, padding = 0\n",
      "backward = False, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mimport\u001b[39;49;00m \u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "W          = needle.Tensor([[[[ 4.09100860e-01 -6.49829268e-01  1.13202944e+01 ... -5.56089449e+00\n",
      "    -4.84385538e+00 -1.20775558e...5e+00]\n",
      "   [-1.10714078e+00  2.54448557e+00 -7.73867989e+00 ... -1.13336074e+00\n",
      "    -1.21658230e+00 -4.79330921e+00]]]])\n",
      "W_shape    = (3, 3, 8, 14)\n",
      "Z          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e...5e+00]\n",
      "   [ 6.10367346e+00 -2.05434513e+00 -4.41384506e+00 ... -6.15245628e+00\n",
      "     3.33514667e+00 -1.35630560e+00]]]])\n",
      "Z_shape    = (3, 16, 16, 8)\n",
      "_W         = array([[[[ 4.09100860e-01, -6.49829268e-01,  1.13202944e+01, ...,\n",
      "          -5.56089449e+00, -4.84385538e+00, -1.20775...448557e+00, -7.73867989e+00, ...,\n",
      "          -1.13336074e+00, -1.21658230e+00, -4.79330921e+00]]]],\n",
      "      dtype=float32)\n",
      "_Z         = array([[[[ 8.82026196e+00,  2.00078607e+00,  4.89369011e+00, ...,\n",
      "          -4.88638926e+00,  4.75044203e+00, -7.56786...434513e+00, -4.41384506e+00, ...,\n",
      "          -6.15245628e+00,  3.33514667e+00, -1.35630560e+00]]]],\n",
      "      dtype=float32)\n",
      "backward   = False\n",
      "device     = cuda()\n",
      "padding    = 0\n",
      "stride     = 2\n",
      "torch      = <module 'torch' from '/opt/conda/envs/train/lib/python3.11/site-packages/torch/__init__.py'>\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:429: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:514: in conv\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Conv(stride, padding)(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e...5e+00]\n",
      "   [ 6.10367346e+00 -2.05434513e+00 -4.41384506e+00 ... -6.15245628e+00\n",
      "     3.33514667e+00 -1.35630560e+00]]]])\n",
      "        b          = needle.Tensor([[[[ 4.09100860e-01 -6.49829268e-01  1.13202944e+01 ... -5.56089449e+00\n",
      "    -4.84385538e+00 -1.20775558e...5e+00]\n",
      "   [-1.10714078e+00  2.54448557e+00 -7.73867989e+00 ... -1.13336074e+00\n",
      "    -1.21658230e+00 -4.79330921e+00]]]])\n",
      "        padding    = 0\n",
      "        stride     = 2\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048...e+00]\n",
      "   [-1.10714078e+00  2.54448557e+00 -7.73867989e+00 ... -1.13336074e+00\n",
      "    -1.21658230e+00 -4.79330921e+00]]]]))\n",
      "        self       = <needle.ops.ops_mathematic.Conv object at 0x7fb2db124e90>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048...e+00]\n",
      "   [-1.10714078e+00  2.54448557e+00 -7.73867989e+00 ... -1.13336074e+00\n",
      "    -1.21658230e+00 -4.79330921e+00]]]]))\n",
      "        op         = <needle.ops.ops_mathematic.Conv object at 0x7fb2db124e90>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fb2db125290>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fb2db125290>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Conv object at 0x7fb2db124e90>\n",
      "A = NDArray([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e-01]\n",
      " ...0367346e+00 -2.05434513e+00 -4.41384506e+00 ... -6.15245628e+00\n",
      "     3.33514667e+00 -1.35630560e+00]]]], device=cuda())\n",
      "B = NDArray([[[[ 4.09100860e-01 -6.49829268e-01  1.13202944e+01 ... -5.56089449e+00\n",
      "    -4.84385538e+00 -1.20775558e-01]\n",
      " ...0714078e+00  2.54448557e+00 -7.73867989e+00 ... -1.13336074e+00\n",
      "    -1.21658230e+00 -4.79330921e+00]]]], device=cuda())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, A, B):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "A          = NDArray([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e-01]\n",
      " ...0367346e+00 -2.05434513e+00 -4.41384506e+00 ... -6.15245628e+00\n",
      "     3.33514667e+00 -1.35630560e+00]]]], device=cuda())\n",
      "B          = NDArray([[[[ 4.09100860e-01 -6.49829268e-01  1.13202944e+01 ... -5.56089449e+00\n",
      "    -4.84385538e+00 -1.20775558e-01]\n",
      " ...0714078e+00  2.54448557e+00 -7.73867989e+00 ... -1.13336074e+00\n",
      "    -1.21658230e+00 -4.79330921e+00]]]], device=cuda())\n",
      "self       = <needle.ops.ops_mathematic.Conv object at 0x7fb2db124e90>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:504: NotImplementedError\n",
      "\u001b[31m\u001b[1m_ test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape9-W_shape9-2-0] _\u001b[0m\n",
      "\n",
      "Z_shape = (3, 16, 16, 2), W_shape = (3, 3, 2, 14), stride = 2, padding = 0\n",
      "backward = False, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mimport\u001b[39;49;00m \u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "W          = needle.Tensor([[[[ -8.0968      -2.5552022    8.703147    -1.4674252    4.5861077\n",
      "     -0.28521433   4.383634    -9.13...727333   0.22582927   9.296732    -8.13161     -0.67411226\n",
      "     -2.9204676    1.675528   -12.187821     5.5746226 ]]]])\n",
      "W_shape    = (3, 3, 2, 14)\n",
      "Z          = needle.Tensor([[[[  8.820262     2.000786  ]\n",
      "   [  4.89369     11.204466  ]\n",
      "   [  9.33779     -4.8863893 ]\n",
      "   ...\n",
      "   [...   -5.112822  ]\n",
      "   ...\n",
      "   [ -7.922368     4.2222714 ]\n",
      "   [ -6.064339     1.4188478 ]\n",
      "   [ -1.4109794   -5.791016  ]]]])\n",
      "Z_shape    = (3, 16, 16, 2)\n",
      "_W         = array([[[[ -8.0968    ,  -2.5552022 ,   8.703147  ,  -1.4674252 ,\n",
      "            4.5861077 ,  -0.28521433,   4.383634  , ...        -8.13161   ,  -0.67411226,  -2.9204676 ,   1.675528  ,\n",
      "          -12.187821  ,   5.5746226 ]]]], dtype=float32)\n",
      "_Z         = array([[[[  8.820262  ,   2.000786  ],\n",
      "         [  4.89369   ,  11.204466  ],\n",
      "         [  9.33779   ,  -4.8863893 ],\n",
      " ...22368  ,   4.2222714 ],\n",
      "         [ -6.064339  ,   1.4188478 ],\n",
      "         [ -1.4109794 ,  -5.791016  ]]]], dtype=float32)\n",
      "backward   = False\n",
      "device     = cuda()\n",
      "padding    = 0\n",
      "stride     = 2\n",
      "torch      = <module 'torch' from '/opt/conda/envs/train/lib/python3.11/site-packages/torch/__init__.py'>\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:429: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:514: in conv\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Conv(stride, padding)(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[[  8.820262     2.000786  ]\n",
      "   [  4.89369     11.204466  ]\n",
      "   [  9.33779     -4.8863893 ]\n",
      "   ...\n",
      "   [...   -5.112822  ]\n",
      "   ...\n",
      "   [ -7.922368     4.2222714 ]\n",
      "   [ -6.064339     1.4188478 ]\n",
      "   [ -1.4109794   -5.791016  ]]]])\n",
      "        b          = needle.Tensor([[[[ -8.0968      -2.5552022    8.703147    -1.4674252    4.5861077\n",
      "     -0.28521433   4.383634    -9.13...727333   0.22582927   9.296732    -8.13161     -0.67411226\n",
      "     -2.9204676    1.675528   -12.187821     5.5746226 ]]]])\n",
      "        padding    = 0\n",
      "        stride     = 2\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[  8.820262     2.000786  ]\n",
      "   [  4.89369     11.204466  ]\n",
      "   [  9.33779     -4.8863893 ]\n",
      "   ...\n",
      "   ...27333   0.22582927   9.296732    -8.13161     -0.67411226\n",
      "     -2.9204676    1.675528   -12.187821     5.5746226 ]]]]))\n",
      "        self       = <needle.ops.ops_mathematic.Conv object at 0x7fb2daf00f10>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[[  8.820262     2.000786  ]\n",
      "   [  4.89369     11.204466  ]\n",
      "   [  9.33779     -4.8863893 ]\n",
      "   ...\n",
      "   ...27333   0.22582927   9.296732    -8.13161     -0.67411226\n",
      "     -2.9204676    1.675528   -12.187821     5.5746226 ]]]]))\n",
      "        op         = <needle.ops.ops_mathematic.Conv object at 0x7fb2daf00f10>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fb2daf00110>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fb2daf00110>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Conv object at 0x7fb2daf00f10>\n",
      "A = NDArray([[[[  8.820262     2.000786  ]\n",
      "   [  4.89369     11.204466  ]\n",
      "   [  9.33779     -4.8863893 ]\n",
      "   ...\n",
      "   [  0.22...\n",
      "   ...\n",
      "   [ -7.922368     4.2222714 ]\n",
      "   [ -6.064339     1.4188478 ]\n",
      "   [ -1.4109794   -5.791016  ]]]], device=cuda())\n",
      "B = NDArray([[[[ -8.0968      -2.5552022    8.703147    -1.4674252    4.5861077\n",
      "     -0.28521433   4.383634    -9.134557  ...2927   9.296732    -8.13161     -0.67411226\n",
      "     -2.9204676    1.675528   -12.187821     5.5746226 ]]]], device=cuda())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, A, B):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "A          = NDArray([[[[  8.820262     2.000786  ]\n",
      "   [  4.89369     11.204466  ]\n",
      "   [  9.33779     -4.8863893 ]\n",
      "   ...\n",
      "   [  0.22...\n",
      "   ...\n",
      "   [ -7.922368     4.2222714 ]\n",
      "   [ -6.064339     1.4188478 ]\n",
      "   [ -1.4109794   -5.791016  ]]]], device=cuda())\n",
      "B          = NDArray([[[[ -8.0968      -2.5552022    8.703147    -1.4674252    4.5861077\n",
      "     -0.28521433   4.383634    -9.134557  ...2927   9.296732    -8.13161     -0.67411226\n",
      "     -2.9204676    1.675528   -12.187821     5.5746226 ]]]], device=cuda())\n",
      "self       = <needle.ops.ops_mathematic.Conv object at 0x7fb2daf00f10>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:504: NotImplementedError\n",
      "\u001b[31m\u001b[1m_ test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape10-W_shape10-1-0] _\u001b[0m\n",
      "\n",
      "Z_shape = (3, 16, 16, 24), W_shape = (3, 3, 24, 14), stride = 1, padding = 0\n",
      "backward = False, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mimport\u001b[39;49;00m \u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "W          = needle.Tensor([[[[ -1.3245817    1.0111231   -1.9236585  ...  -2.5907016\n",
      "     -0.36175704   4.6234684 ]\n",
      "   [  5.279258... -4.82095      4.98517   ]\n",
      "   [  3.2300415   -2.4161792   -3.420825   ...  -9.444658\n",
      "      8.537833     2.3818388 ]]]])\n",
      "W_shape    = (3, 3, 24, 14)\n",
      "Z          = needle.Tensor([[[[  8.820262     2.000786     4.89369    ...   3.2680929\n",
      "      4.322181    -3.7108252 ]\n",
      "   [ 11.348773...4.207389     8.935435  ]\n",
      "   [  2.5084004    3.1399443   -0.12958507 ...   0.13740699\n",
      "     -1.400853     0.07475674]]]])\n",
      "Z_shape    = (3, 16, 16, 24)\n",
      "_W         = array([[[[ -1.3245817 ,   1.0111231 ,  -1.9236585 , ...,  -2.5907016 ,\n",
      "           -0.36175704,   4.6234684 ],\n",
      "        ... [  3.2300415 ,  -2.4161792 ,  -3.420825  , ...,  -9.444658  ,\n",
      "            8.537833  ,   2.3818388 ]]]], dtype=float32)\n",
      "_Z         = array([[[[  8.820262  ,   2.000786  ,   4.89369   , ...,   3.2680929 ,\n",
      "            4.322181  ,  -3.7108252 ],\n",
      "        ... [  2.5084004 ,   3.1399443 ,  -0.12958507, ...,   0.13740699,\n",
      "           -1.400853  ,   0.07475674]]]], dtype=float32)\n",
      "backward   = False\n",
      "device     = cuda()\n",
      "padding    = 0\n",
      "stride     = 1\n",
      "torch      = <module 'torch' from '/opt/conda/envs/train/lib/python3.11/site-packages/torch/__init__.py'>\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:429: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:514: in conv\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Conv(stride, padding)(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[[  8.820262     2.000786     4.89369    ...   3.2680929\n",
      "      4.322181    -3.7108252 ]\n",
      "   [ 11.348773...4.207389     8.935435  ]\n",
      "   [  2.5084004    3.1399443   -0.12958507 ...   0.13740699\n",
      "     -1.400853     0.07475674]]]])\n",
      "        b          = needle.Tensor([[[[ -1.3245817    1.0111231   -1.9236585  ...  -2.5907016\n",
      "     -0.36175704   4.6234684 ]\n",
      "   [  5.279258... -4.82095      4.98517   ]\n",
      "   [  3.2300415   -2.4161792   -3.420825   ...  -9.444658\n",
      "      8.537833     2.3818388 ]]]])\n",
      "        padding    = 0\n",
      "        stride     = 1\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[  8.820262     2.000786     4.89369    ...   3.2680929\n",
      "      4.322181    -3.7108252 ]\n",
      "   [ 11.34877...-4.82095      4.98517   ]\n",
      "   [  3.2300415   -2.4161792   -3.420825   ...  -9.444658\n",
      "      8.537833     2.3818388 ]]]]))\n",
      "        self       = <needle.ops.ops_mathematic.Conv object at 0x7fb2db0c9190>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[[  8.820262     2.000786     4.89369    ...   3.2680929\n",
      "      4.322181    -3.7108252 ]\n",
      "   [ 11.34877...-4.82095      4.98517   ]\n",
      "   [  3.2300415   -2.4161792   -3.420825   ...  -9.444658\n",
      "      8.537833     2.3818388 ]]]]))\n",
      "        op         = <needle.ops.ops_mathematic.Conv object at 0x7fb2db0c9190>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fb2db0cb290>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fb2db0cb290>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Conv object at 0x7fb2db0c9190>\n",
      "A = NDArray([[[[  8.820262     2.000786     4.89369    ...   3.2680929\n",
      "      4.322181    -3.7108252 ]\n",
      "   [ 11.348773    -7...935435  ]\n",
      "   [  2.5084004    3.1399443   -0.12958507 ...   0.13740699\n",
      "     -1.400853     0.07475674]]]], device=cuda())\n",
      "B = NDArray([[[[ -1.3245817    1.0111231   -1.9236585  ...  -2.5907016\n",
      "     -0.36175704   4.6234684 ]\n",
      "   [  5.2792587   -1...4.98517   ]\n",
      "   [  3.2300415   -2.4161792   -3.420825   ...  -9.444658\n",
      "      8.537833     2.3818388 ]]]], device=cuda())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, A, B):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "A          = NDArray([[[[  8.820262     2.000786     4.89369    ...   3.2680929\n",
      "      4.322181    -3.7108252 ]\n",
      "   [ 11.348773    -7...935435  ]\n",
      "   [  2.5084004    3.1399443   -0.12958507 ...   0.13740699\n",
      "     -1.400853     0.07475674]]]], device=cuda())\n",
      "B          = NDArray([[[[ -1.3245817    1.0111231   -1.9236585  ...  -2.5907016\n",
      "     -0.36175704   4.6234684 ]\n",
      "   [  5.2792587   -1...4.98517   ]\n",
      "   [  3.2300415   -2.4161792   -3.420825   ...  -9.444658\n",
      "      8.537833     2.3818388 ]]]], device=cuda())\n",
      "self       = <needle.ops.ops_mathematic.Conv object at 0x7fb2db0c9190>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:504: NotImplementedError\n",
      "\u001b[31m\u001b[1m_ test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape11-W_shape11-1-0] _\u001b[0m\n",
      "\n",
      "Z_shape = (3, 14, 14, 8), W_shape = (5, 5, 8, 16), stride = 1, padding = 0\n",
      "backward = False, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mimport\u001b[39;49;00m \u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "W          = needle.Tensor([[[[ 4.28266907e+00  8.13360405e+00 -4.62234831e+00 ...  2.07981968e+00\n",
      "     2.17041516e+00 -3.88039827e...7e+00]\n",
      "   [-7.86027253e-01  4.30947495e+00 -4.97804356e+00 ... -5.69430470e-01\n",
      "     4.27775383e-01 -3.84437203e+00]]]])\n",
      "W_shape    = (5, 5, 8, 16)\n",
      "Z          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e...3e+00]\n",
      "   [ 1.09905624e+01  2.23166728e+00  4.62055349e+00 ... -6.23090088e-01\n",
      "    -8.48226726e-01 -2.29984760e+00]]]])\n",
      "Z_shape    = (3, 14, 14, 8)\n",
      "_W         = array([[[[ 4.28266907e+00,  8.13360405e+00, -4.62234831e+00, ...,\n",
      "           2.07981968e+00,  2.17041516e+00, -3.88039...947495e+00, -4.97804356e+00, ...,\n",
      "          -5.69430470e-01,  4.27775383e-01, -3.84437203e+00]]]],\n",
      "      dtype=float32)\n",
      "_Z         = array([[[[ 8.82026196e+00,  2.00078607e+00,  4.89369011e+00, ...,\n",
      "          -4.88638926e+00,  4.75044203e+00, -7.56786...166728e+00,  4.62055349e+00, ...,\n",
      "          -6.23090088e-01, -8.48226726e-01, -2.29984760e+00]]]],\n",
      "      dtype=float32)\n",
      "backward   = False\n",
      "device     = cuda()\n",
      "padding    = 0\n",
      "stride     = 1\n",
      "torch      = <module 'torch' from '/opt/conda/envs/train/lib/python3.11/site-packages/torch/__init__.py'>\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:429: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:514: in conv\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Conv(stride, padding)(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e...3e+00]\n",
      "   [ 1.09905624e+01  2.23166728e+00  4.62055349e+00 ... -6.23090088e-01\n",
      "    -8.48226726e-01 -2.29984760e+00]]]])\n",
      "        b          = needle.Tensor([[[[ 4.28266907e+00  8.13360405e+00 -4.62234831e+00 ...  2.07981968e+00\n",
      "     2.17041516e+00 -3.88039827e...7e+00]\n",
      "   [-7.86027253e-01  4.30947495e+00 -4.97804356e+00 ... -5.69430470e-01\n",
      "     4.27775383e-01 -3.84437203e+00]]]])\n",
      "        padding    = 0\n",
      "        stride     = 1\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048...e+00]\n",
      "   [-7.86027253e-01  4.30947495e+00 -4.97804356e+00 ... -5.69430470e-01\n",
      "     4.27775383e-01 -3.84437203e+00]]]]))\n",
      "        self       = <needle.ops.ops_mathematic.Conv object at 0x7fb2dadc3450>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048...e+00]\n",
      "   [-7.86027253e-01  4.30947495e+00 -4.97804356e+00 ... -5.69430470e-01\n",
      "     4.27775383e-01 -3.84437203e+00]]]]))\n",
      "        op         = <needle.ops.ops_mathematic.Conv object at 0x7fb2dadc3450>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fb2dadc2090>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fb2dadc2090>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Conv object at 0x7fb2dadc3450>\n",
      "A = NDArray([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e-01]\n",
      " ...9905624e+01  2.23166728e+00  4.62055349e+00 ... -6.23090088e-01\n",
      "    -8.48226726e-01 -2.29984760e+00]]]], device=cuda())\n",
      "B = NDArray([[[[ 4.28266907e+00  8.13360405e+00 -4.62234831e+00 ...  2.07981968e+00\n",
      "     2.17041516e+00 -3.88039827e-01]\n",
      " ...6027253e-01  4.30947495e+00 -4.97804356e+00 ... -5.69430470e-01\n",
      "     4.27775383e-01 -3.84437203e+00]]]], device=cuda())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, A, B):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "A          = NDArray([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e-01]\n",
      " ...9905624e+01  2.23166728e+00  4.62055349e+00 ... -6.23090088e-01\n",
      "    -8.48226726e-01 -2.29984760e+00]]]], device=cuda())\n",
      "B          = NDArray([[[[ 4.28266907e+00  8.13360405e+00 -4.62234831e+00 ...  2.07981968e+00\n",
      "     2.17041516e+00 -3.88039827e-01]\n",
      " ...6027253e-01  4.30947495e+00 -4.97804356e+00 ... -5.69430470e-01\n",
      "     4.27775383e-01 -3.84437203e+00]]]], device=cuda())\n",
      "self       = <needle.ops.ops_mathematic.Conv object at 0x7fb2dadc3450>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:504: NotImplementedError\n",
      "\u001b[31m\u001b[1m_ test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape12-W_shape12-1-0] _\u001b[0m\n",
      "\n",
      "Z_shape = (3, 17, 17, 8), W_shape = (5, 5, 8, 16), stride = 1, padding = 0\n",
      "backward = False, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mimport\u001b[39;49;00m \u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "W          = needle.Tensor([[[[ 5.93117094e+00 -4.61600447e+00 -7.43411541e+00 ...  9.49053860e+00\n",
      "     6.47611380e+00 -3.38525438e...7e+00]\n",
      "   [-3.93575430e+00 -6.98609781e+00  1.63659811e+00 ...  8.22474575e+00\n",
      "     2.56555057e+00  1.63004756e+00]]]])\n",
      "W_shape    = (5, 5, 8, 16)\n",
      "Z          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e...8e+00]\n",
      "   [ 2.80155873e+00  3.11298299e+00 -3.24756050e+00 ... -7.96233535e-01\n",
      "     4.72636795e+00 -5.75466204e+00]]]])\n",
      "Z_shape    = (3, 17, 17, 8)\n",
      "_W         = array([[[[ 5.93117094e+00, -4.61600447e+00, -7.43411541e+00, ...,\n",
      "           9.49053860e+00,  6.47611380e+00, -3.38525...609781e+00,  1.63659811e+00, ...,\n",
      "           8.22474575e+00,  2.56555057e+00,  1.63004756e+00]]]],\n",
      "      dtype=float32)\n",
      "_Z         = array([[[[ 8.82026196e+00,  2.00078607e+00,  4.89369011e+00, ...,\n",
      "          -4.88638926e+00,  4.75044203e+00, -7.56786...298299e+00, -3.24756050e+00, ...,\n",
      "          -7.96233535e-01,  4.72636795e+00, -5.75466204e+00]]]],\n",
      "      dtype=float32)\n",
      "backward   = False\n",
      "device     = cuda()\n",
      "padding    = 0\n",
      "stride     = 1\n",
      "torch      = <module 'torch' from '/opt/conda/envs/train/lib/python3.11/site-packages/torch/__init__.py'>\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:429: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:514: in conv\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Conv(stride, padding)(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e...8e+00]\n",
      "   [ 2.80155873e+00  3.11298299e+00 -3.24756050e+00 ... -7.96233535e-01\n",
      "     4.72636795e+00 -5.75466204e+00]]]])\n",
      "        b          = needle.Tensor([[[[ 5.93117094e+00 -4.61600447e+00 -7.43411541e+00 ...  9.49053860e+00\n",
      "     6.47611380e+00 -3.38525438e...7e+00]\n",
      "   [-3.93575430e+00 -6.98609781e+00  1.63659811e+00 ...  8.22474575e+00\n",
      "     2.56555057e+00  1.63004756e+00]]]])\n",
      "        padding    = 0\n",
      "        stride     = 1\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048...e+00]\n",
      "   [-3.93575430e+00 -6.98609781e+00  1.63659811e+00 ...  8.22474575e+00\n",
      "     2.56555057e+00  1.63004756e+00]]]]))\n",
      "        self       = <needle.ops.ops_mathematic.Conv object at 0x7fb2daff5310>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048...e+00]\n",
      "   [-3.93575430e+00 -6.98609781e+00  1.63659811e+00 ...  8.22474575e+00\n",
      "     2.56555057e+00  1.63004756e+00]]]]))\n",
      "        op         = <needle.ops.ops_mathematic.Conv object at 0x7fb2daff5310>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fb2daff6590>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fb2daff6590>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Conv object at 0x7fb2daff5310>\n",
      "A = NDArray([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e-01]\n",
      " ...0155873e+00  3.11298299e+00 -3.24756050e+00 ... -7.96233535e-01\n",
      "     4.72636795e+00 -5.75466204e+00]]]], device=cuda())\n",
      "B = NDArray([[[[ 5.93117094e+00 -4.61600447e+00 -7.43411541e+00 ...  9.49053860e+00\n",
      "     6.47611380e+00 -3.38525438e+00]\n",
      " ...3575430e+00 -6.98609781e+00  1.63659811e+00 ...  8.22474575e+00\n",
      "     2.56555057e+00  1.63004756e+00]]]], device=cuda())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, A, B):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "A          = NDArray([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e-01]\n",
      " ...0155873e+00  3.11298299e+00 -3.24756050e+00 ... -7.96233535e-01\n",
      "     4.72636795e+00 -5.75466204e+00]]]], device=cuda())\n",
      "B          = NDArray([[[[ 5.93117094e+00 -4.61600447e+00 -7.43411541e+00 ...  9.49053860e+00\n",
      "     6.47611380e+00 -3.38525438e+00]\n",
      " ...3575430e+00 -6.98609781e+00  1.63659811e+00 ...  8.22474575e+00\n",
      "     2.56555057e+00  1.63004756e+00]]]], device=cuda())\n",
      "self       = <needle.ops.ops_mathematic.Conv object at 0x7fb2daff5310>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:504: NotImplementedError\n",
      "\u001b[31m\u001b[1m_ test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape13-W_shape13-1-0] _\u001b[0m\n",
      "\n",
      "Z_shape = (3, 17, 17, 1), W_shape = (5, 5, 1, 16), stride = 1, padding = 0\n",
      "backward = False, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mimport\u001b[39;49;00m \u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "W          = needle.Tensor([[[[  1.5437562   -6.8538       4.3282647    5.4068804   -3.15688\n",
      "     -1.206689    -4.3909516    3.4969...448   -2.242325     0.6578698\n",
      "     -7.0278      -1.7489109   10.11736      2.5269346    1.7962458\n",
      "     -7.9124722 ]]]])\n",
      "W_shape    = (5, 5, 1, 16)\n",
      "Z          = needle.Tensor([[[[ 8.82026196e+00]\n",
      "   [ 2.00078607e+00]\n",
      "   [ 4.89369011e+00]\n",
      "   [ 1.12044659e+01]\n",
      "   [ 9.33778954e+00]...22066e-01]\n",
      "   [ 1.05310105e-01]\n",
      "   [ 4.97272283e-01]\n",
      "   [ 1.13696384e+00]\n",
      "   [-5.08369303e+00]\n",
      "   [-5.73876619e-01]]]])\n",
      "Z_shape    = (3, 17, 17, 1)\n",
      "_W         = array([[[[  1.5437562 ,  -6.8538    ,   4.3282647 ,   5.4068804 ,\n",
      "           -3.15688   ,  -1.206689  ,  -4.3909516 , ...  -7.0278    ,  -1.7489109 ,\n",
      "           10.11736   ,   2.5269346 ,   1.7962458 ,  -7.9124722 ]]]],\n",
      "      dtype=float32)\n",
      "_Z         = array([[[[ 8.82026196e+00],\n",
      "         [ 2.00078607e+00],\n",
      "         [ 4.89369011e+00],\n",
      "         [ 1.12044659e+01],\n",
      "      ... 4.97272283e-01],\n",
      "         [ 1.13696384e+00],\n",
      "         [-5.08369303e+00],\n",
      "         [-5.73876619e-01]]]], dtype=float32)\n",
      "backward   = False\n",
      "device     = cuda()\n",
      "padding    = 0\n",
      "stride     = 1\n",
      "torch      = <module 'torch' from '/opt/conda/envs/train/lib/python3.11/site-packages/torch/__init__.py'>\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:429: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:514: in conv\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Conv(stride, padding)(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[[ 8.82026196e+00]\n",
      "   [ 2.00078607e+00]\n",
      "   [ 4.89369011e+00]\n",
      "   [ 1.12044659e+01]\n",
      "   [ 9.33778954e+00]...22066e-01]\n",
      "   [ 1.05310105e-01]\n",
      "   [ 4.97272283e-01]\n",
      "   [ 1.13696384e+00]\n",
      "   [-5.08369303e+00]\n",
      "   [-5.73876619e-01]]]])\n",
      "        b          = needle.Tensor([[[[  1.5437562   -6.8538       4.3282647    5.4068804   -3.15688\n",
      "     -1.206689    -4.3909516    3.4969...448   -2.242325     0.6578698\n",
      "     -7.0278      -1.7489109   10.11736      2.5269346    1.7962458\n",
      "     -7.9124722 ]]]])\n",
      "        padding    = 0\n",
      "        stride     = 1\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[ 8.82026196e+00]\n",
      "   [ 2.00078607e+00]\n",
      "   [ 4.89369011e+00]\n",
      "   [ 1.12044659e+01]\n",
      "   [ 9.33778954e+00...48   -2.242325     0.6578698\n",
      "     -7.0278      -1.7489109   10.11736      2.5269346    1.7962458\n",
      "     -7.9124722 ]]]]))\n",
      "        self       = <needle.ops.ops_mathematic.Conv object at 0x7fb2db17c6d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[[ 8.82026196e+00]\n",
      "   [ 2.00078607e+00]\n",
      "   [ 4.89369011e+00]\n",
      "   [ 1.12044659e+01]\n",
      "   [ 9.33778954e+00...48   -2.242325     0.6578698\n",
      "     -7.0278      -1.7489109   10.11736      2.5269346    1.7962458\n",
      "     -7.9124722 ]]]]))\n",
      "        op         = <needle.ops.ops_mathematic.Conv object at 0x7fb2db17c6d0>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fb2db17c910>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fb2db17c910>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Conv object at 0x7fb2db17c6d0>\n",
      "A = NDArray([[[[ 8.82026196e+00]\n",
      "   [ 2.00078607e+00]\n",
      "   [ 4.89369011e+00]\n",
      "   [ 1.12044659e+01]\n",
      "   [ 9.33778954e+00]\n",
      "   [-... 1.05310105e-01]\n",
      "   [ 4.97272283e-01]\n",
      "   [ 1.13696384e+00]\n",
      "   [-5.08369303e+00]\n",
      "   [-5.73876619e-01]]]], device=cuda())\n",
      "B = NDArray([[[[  1.5437562   -6.8538       4.3282647    5.4068804   -3.15688\n",
      "     -1.206689    -4.3909516    3.4969025   ...     0.6578698\n",
      "     -7.0278      -1.7489109   10.11736      2.5269346    1.7962458\n",
      "     -7.9124722 ]]]], device=cuda())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, A, B):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "A          = NDArray([[[[ 8.82026196e+00]\n",
      "   [ 2.00078607e+00]\n",
      "   [ 4.89369011e+00]\n",
      "   [ 1.12044659e+01]\n",
      "   [ 9.33778954e+00]\n",
      "   [-... 1.05310105e-01]\n",
      "   [ 4.97272283e-01]\n",
      "   [ 1.13696384e+00]\n",
      "   [-5.08369303e+00]\n",
      "   [-5.73876619e-01]]]], device=cuda())\n",
      "B          = NDArray([[[[  1.5437562   -6.8538       4.3282647    5.4068804   -3.15688\n",
      "     -1.206689    -4.3909516    3.4969025   ...     0.6578698\n",
      "     -7.0278      -1.7489109   10.11736      2.5269346    1.7962458\n",
      "     -7.9124722 ]]]], device=cuda())\n",
      "self       = <needle.ops.ops_mathematic.Conv object at 0x7fb2db17c6d0>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:504: NotImplementedError\n",
      "\u001b[31m\u001b[1m_ test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape14-W_shape14-1-0] _\u001b[0m\n",
      "\n",
      "Z_shape = (3, 17, 17, 16), W_shape = (5, 5, 16, 1), stride = 1, padding = 0\n",
      "backward = False, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mimport\u001b[39;49;00m \u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "W          = needle.Tensor([[[[ 1.55579513e-02]\n",
      "   [-3.04658723e+00]\n",
      "   [ 4.09725952e+00]\n",
      "   [ 1.29906273e+00]\n",
      "   [-7.50546598e+00]...75421e+00]\n",
      "   [ 3.02805209e+00]\n",
      "   [-4.20992136e+00]\n",
      "   [ 4.11707735e+00]\n",
      "   [ 4.99041653e+00]\n",
      "   [ 5.11040497e+00]]]])\n",
      "W_shape    = (5, 5, 16, 1)\n",
      "Z          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ...  6.08375072e-01\n",
      "     2.21931624e+00  1.66837168e...3e+00]\n",
      "   [-1.25365603e+00 -1.71452928e+00  1.35930243e+01 ...  3.13170171e+00\n",
      "    -1.35708165e+00  1.29997072e+01]]]])\n",
      "Z_shape    = (3, 17, 17, 16)\n",
      "_W         = array([[[[ 1.55579513e-02],\n",
      "         [-3.04658723e+00],\n",
      "         [ 4.09725952e+00],\n",
      "         [ 1.29906273e+00],\n",
      "      ...-4.20992136e+00],\n",
      "         [ 4.11707735e+00],\n",
      "         [ 4.99041653e+00],\n",
      "         [ 5.11040497e+00]]]], dtype=float32)\n",
      "_Z         = array([[[[ 8.82026196e+00,  2.00078607e+00,  4.89369011e+00, ...,\n",
      "           6.08375072e-01,  2.21931624e+00,  1.66837...452928e+00,  1.35930243e+01, ...,\n",
      "           3.13170171e+00, -1.35708165e+00,  1.29997072e+01]]]],\n",
      "      dtype=float32)\n",
      "backward   = False\n",
      "device     = cuda()\n",
      "padding    = 0\n",
      "stride     = 1\n",
      "torch      = <module 'torch' from '/opt/conda/envs/train/lib/python3.11/site-packages/torch/__init__.py'>\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:429: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:514: in conv\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Conv(stride, padding)(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ...  6.08375072e-01\n",
      "     2.21931624e+00  1.66837168e...3e+00]\n",
      "   [-1.25365603e+00 -1.71452928e+00  1.35930243e+01 ...  3.13170171e+00\n",
      "    -1.35708165e+00  1.29997072e+01]]]])\n",
      "        b          = needle.Tensor([[[[ 1.55579513e-02]\n",
      "   [-3.04658723e+00]\n",
      "   [ 4.09725952e+00]\n",
      "   [ 1.29906273e+00]\n",
      "   [-7.50546598e+00]...75421e+00]\n",
      "   [ 3.02805209e+00]\n",
      "   [-4.20992136e+00]\n",
      "   [ 4.11707735e+00]\n",
      "   [ 4.99041653e+00]\n",
      "   [ 5.11040497e+00]]]])\n",
      "        padding    = 0\n",
      "        stride     = 1\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ...  6.08375072e-01\n",
      "     2.21931624e+00  1.66837168...5421e+00]\n",
      "   [ 3.02805209e+00]\n",
      "   [-4.20992136e+00]\n",
      "   [ 4.11707735e+00]\n",
      "   [ 4.99041653e+00]\n",
      "   [ 5.11040497e+00]]]]))\n",
      "        self       = <needle.ops.ops_mathematic.Conv object at 0x7fb2daf15d90>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ...  6.08375072e-01\n",
      "     2.21931624e+00  1.66837168...5421e+00]\n",
      "   [ 3.02805209e+00]\n",
      "   [-4.20992136e+00]\n",
      "   [ 4.11707735e+00]\n",
      "   [ 4.99041653e+00]\n",
      "   [ 5.11040497e+00]]]]))\n",
      "        op         = <needle.ops.ops_mathematic.Conv object at 0x7fb2daf15d90>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fb2daf17f90>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fb2daf17f90>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Conv object at 0x7fb2daf15d90>\n",
      "A = NDArray([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ...  6.08375072e-01\n",
      "     2.21931624e+00  1.66837168e+00]\n",
      " ...5365603e+00 -1.71452928e+00  1.35930243e+01 ...  3.13170171e+00\n",
      "    -1.35708165e+00  1.29997072e+01]]]], device=cuda())\n",
      "B = NDArray([[[[ 1.55579513e-02]\n",
      "   [-3.04658723e+00]\n",
      "   [ 4.09725952e+00]\n",
      "   [ 1.29906273e+00]\n",
      "   [-7.50546598e+00]\n",
      "   [-... 3.02805209e+00]\n",
      "   [-4.20992136e+00]\n",
      "   [ 4.11707735e+00]\n",
      "   [ 4.99041653e+00]\n",
      "   [ 5.11040497e+00]]]], device=cuda())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, A, B):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "A          = NDArray([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ...  6.08375072e-01\n",
      "     2.21931624e+00  1.66837168e+00]\n",
      " ...5365603e+00 -1.71452928e+00  1.35930243e+01 ...  3.13170171e+00\n",
      "    -1.35708165e+00  1.29997072e+01]]]], device=cuda())\n",
      "B          = NDArray([[[[ 1.55579513e-02]\n",
      "   [-3.04658723e+00]\n",
      "   [ 4.09725952e+00]\n",
      "   [ 1.29906273e+00]\n",
      "   [-7.50546598e+00]\n",
      "   [-... 3.02805209e+00]\n",
      "   [-4.20992136e+00]\n",
      "   [ 4.11707735e+00]\n",
      "   [ 4.99041653e+00]\n",
      "   [ 5.11040497e+00]]]], device=cuda())\n",
      "self       = <needle.ops.ops_mathematic.Conv object at 0x7fb2daf15d90>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:504: NotImplementedError\n",
      "\u001b[31m\u001b[1m_ test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape15-W_shape15-1-0] _\u001b[0m\n",
      "\n",
      "Z_shape = (3, 17, 17, 16), W_shape = (1, 1, 16, 1), stride = 1, padding = 0\n",
      "backward = False, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mimport\u001b[39;49;00m \u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "W          = needle.Tensor([[[[ 0.01555795]\n",
      "   [-3.0465872 ]\n",
      "   [ 4.0972595 ]\n",
      "   [ 1.2990627 ]\n",
      "   [-7.505466  ]\n",
      "   [-1.7335238 ]\n",
      "  ...[ 5.6893935 ]\n",
      "   [ 2.267663  ]\n",
      "   [-1.0709513 ]\n",
      "   [-5.0566583 ]\n",
      "   [-0.23777105]\n",
      "   [-9.931785  ]\n",
      "   [-0.44905776]]]])\n",
      "W_shape    = (1, 1, 16, 1)\n",
      "Z          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ...  6.08375072e-01\n",
      "     2.21931624e+00  1.66837168e...3e+00]\n",
      "   [-1.25365603e+00 -1.71452928e+00  1.35930243e+01 ...  3.13170171e+00\n",
      "    -1.35708165e+00  1.29997072e+01]]]])\n",
      "Z_shape    = (3, 17, 17, 16)\n",
      "_W         = array([[[[ 0.01555795],\n",
      "         [-3.0465872 ],\n",
      "         [ 4.0972595 ],\n",
      "         [ 1.2990627 ],\n",
      "         [-7.505466  ]...13 ],\n",
      "         [-5.0566583 ],\n",
      "         [-0.23777105],\n",
      "         [-9.931785  ],\n",
      "         [-0.44905776]]]], dtype=float32)\n",
      "_Z         = array([[[[ 8.82026196e+00,  2.00078607e+00,  4.89369011e+00, ...,\n",
      "           6.08375072e-01,  2.21931624e+00,  1.66837...452928e+00,  1.35930243e+01, ...,\n",
      "           3.13170171e+00, -1.35708165e+00,  1.29997072e+01]]]],\n",
      "      dtype=float32)\n",
      "backward   = False\n",
      "device     = cuda()\n",
      "padding    = 0\n",
      "stride     = 1\n",
      "torch      = <module 'torch' from '/opt/conda/envs/train/lib/python3.11/site-packages/torch/__init__.py'>\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:429: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:514: in conv\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Conv(stride, padding)(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ...  6.08375072e-01\n",
      "     2.21931624e+00  1.66837168e...3e+00]\n",
      "   [-1.25365603e+00 -1.71452928e+00  1.35930243e+01 ...  3.13170171e+00\n",
      "    -1.35708165e+00  1.29997072e+01]]]])\n",
      "        b          = needle.Tensor([[[[ 0.01555795]\n",
      "   [-3.0465872 ]\n",
      "   [ 4.0972595 ]\n",
      "   [ 1.2990627 ]\n",
      "   [-7.505466  ]\n",
      "   [-1.7335238 ]\n",
      "  ...[ 5.6893935 ]\n",
      "   [ 2.267663  ]\n",
      "   [-1.0709513 ]\n",
      "   [-5.0566583 ]\n",
      "   [-0.23777105]\n",
      "   [-9.931785  ]\n",
      "   [-0.44905776]]]])\n",
      "        padding    = 0\n",
      "        stride     = 1\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ...  6.08375072e-01\n",
      "     2.21931624e+00  1.66837168... 5.6893935 ]\n",
      "   [ 2.267663  ]\n",
      "   [-1.0709513 ]\n",
      "   [-5.0566583 ]\n",
      "   [-0.23777105]\n",
      "   [-9.931785  ]\n",
      "   [-0.44905776]]]]))\n",
      "        self       = <needle.ops.ops_mathematic.Conv object at 0x7fb2db04c690>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ...  6.08375072e-01\n",
      "     2.21931624e+00  1.66837168... 5.6893935 ]\n",
      "   [ 2.267663  ]\n",
      "   [-1.0709513 ]\n",
      "   [-5.0566583 ]\n",
      "   [-0.23777105]\n",
      "   [-9.931785  ]\n",
      "   [-0.44905776]]]]))\n",
      "        op         = <needle.ops.ops_mathematic.Conv object at 0x7fb2db04c690>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fb2db04dc10>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fb2db04dc10>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Conv object at 0x7fb2db04c690>\n",
      "A = NDArray([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ...  6.08375072e-01\n",
      "     2.21931624e+00  1.66837168e+00]\n",
      " ...5365603e+00 -1.71452928e+00  1.35930243e+01 ...  3.13170171e+00\n",
      "    -1.35708165e+00  1.29997072e+01]]]], device=cuda())\n",
      "B = NDArray([[[[ 0.01555795]\n",
      "   [-3.0465872 ]\n",
      "   [ 4.0972595 ]\n",
      "   [ 1.2990627 ]\n",
      "   [-7.505466  ]\n",
      "   [-1.7335238 ]\n",
      "   [-2.5...  [ 2.267663  ]\n",
      "   [-1.0709513 ]\n",
      "   [-5.0566583 ]\n",
      "   [-0.23777105]\n",
      "   [-9.931785  ]\n",
      "   [-0.44905776]]]], device=cuda())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, A, B):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "A          = NDArray([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ...  6.08375072e-01\n",
      "     2.21931624e+00  1.66837168e+00]\n",
      " ...5365603e+00 -1.71452928e+00  1.35930243e+01 ...  3.13170171e+00\n",
      "    -1.35708165e+00  1.29997072e+01]]]], device=cuda())\n",
      "B          = NDArray([[[[ 0.01555795]\n",
      "   [-3.0465872 ]\n",
      "   [ 4.0972595 ]\n",
      "   [ 1.2990627 ]\n",
      "   [-7.505466  ]\n",
      "   [-1.7335238 ]\n",
      "   [-2.5...  [ 2.267663  ]\n",
      "   [-1.0709513 ]\n",
      "   [-5.0566583 ]\n",
      "   [-0.23777105]\n",
      "   [-9.931785  ]\n",
      "   [-0.44905776]]]], device=cuda())\n",
      "self       = <needle.ops.ops_mathematic.Conv object at 0x7fb2db04c690>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:504: NotImplementedError\n",
      "\u001b[31m\u001b[1m_ test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape16-W_shape16-1-0] _\u001b[0m\n",
      "\n",
      "Z_shape = (1, 14, 14, 2), W_shape = (3, 3, 2, 2), stride = 1, padding = 0\n",
      "backward = False, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mimport\u001b[39;49;00m \u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "W          = needle.Tensor([[[[ -1.7671587   -8.082371  ]\n",
      "   [ -1.4591868   -3.807461  ]]\n",
      "\n",
      "  [[  4.2896194    5.705509  ]\n",
      "   [  7.3...7526     2.911123  ]\n",
      "   [-10.473016     0.61860955]]\n",
      "\n",
      "  [[ -0.65053475   0.46976614]\n",
      "   [  4.7152305  -13.698386  ]]]])\n",
      "W_shape    = (3, 3, 2, 2)\n",
      "Z          = needle.Tensor([[[[  8.820262     2.000786  ]\n",
      "   [  4.89369     11.204466  ]\n",
      "   [  9.33779     -4.8863893 ]\n",
      "   [  4.750...19315276  -8.283575  ]\n",
      "   [ -4.9275537   -7.359175  ]\n",
      "   [  8.240675     0.8211388 ]\n",
      "   [  2.8364513   -1.1133755 ]]]])\n",
      "Z_shape    = (1, 14, 14, 2)\n",
      "_W         = array([[[[ -1.7671587 ,  -8.082371  ],\n",
      "         [ -1.4591868 ,  -3.807461  ]],\n",
      "\n",
      "        [[  4.2896194 ,   5.705509  ],...016  ,   0.61860955]],\n",
      "\n",
      "        [[ -0.65053475,   0.46976614],\n",
      "         [  4.7152305 , -13.698386  ]]]], dtype=float32)\n",
      "_Z         = array([[[[  8.820262  ,   2.000786  ],\n",
      "         [  4.89369   ,  11.204466  ],\n",
      "         [  9.33779   ,  -4.8863893 ],\n",
      " ...275537 ,  -7.359175  ],\n",
      "         [  8.240675  ,   0.8211388 ],\n",
      "         [  2.8364513 ,  -1.1133755 ]]]], dtype=float32)\n",
      "backward   = False\n",
      "device     = cuda()\n",
      "padding    = 0\n",
      "stride     = 1\n",
      "torch      = <module 'torch' from '/opt/conda/envs/train/lib/python3.11/site-packages/torch/__init__.py'>\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:429: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:514: in conv\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Conv(stride, padding)(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[[  8.820262     2.000786  ]\n",
      "   [  4.89369     11.204466  ]\n",
      "   [  9.33779     -4.8863893 ]\n",
      "   [  4.750...19315276  -8.283575  ]\n",
      "   [ -4.9275537   -7.359175  ]\n",
      "   [  8.240675     0.8211388 ]\n",
      "   [  2.8364513   -1.1133755 ]]]])\n",
      "        b          = needle.Tensor([[[[ -1.7671587   -8.082371  ]\n",
      "   [ -1.4591868   -3.807461  ]]\n",
      "\n",
      "  [[  4.2896194    5.705509  ]\n",
      "   [  7.3...7526     2.911123  ]\n",
      "   [-10.473016     0.61860955]]\n",
      "\n",
      "  [[ -0.65053475   0.46976614]\n",
      "   [  4.7152305  -13.698386  ]]]])\n",
      "        padding    = 0\n",
      "        stride     = 1\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[  8.820262     2.000786  ]\n",
      "   [  4.89369     11.204466  ]\n",
      "   [  9.33779     -4.8863893 ]\n",
      "   [  4.75...526     2.911123  ]\n",
      "   [-10.473016     0.61860955]]\n",
      "\n",
      "  [[ -0.65053475   0.46976614]\n",
      "   [  4.7152305  -13.698386  ]]]]))\n",
      "        self       = <needle.ops.ops_mathematic.Conv object at 0x7fb2daedf510>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[[  8.820262     2.000786  ]\n",
      "   [  4.89369     11.204466  ]\n",
      "   [  9.33779     -4.8863893 ]\n",
      "   [  4.75...526     2.911123  ]\n",
      "   [-10.473016     0.61860955]]\n",
      "\n",
      "  [[ -0.65053475   0.46976614]\n",
      "   [  4.7152305  -13.698386  ]]]]))\n",
      "        op         = <needle.ops.ops_mathematic.Conv object at 0x7fb2daedf510>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fb2daedc9d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fb2daedc9d0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Conv object at 0x7fb2daedf510>\n",
      "A = NDArray([[[[  8.820262     2.000786  ]\n",
      "   [  4.89369     11.204466  ]\n",
      "   [  9.33779     -4.8863893 ]\n",
      "   [  4.750442   ...3575  ]\n",
      "   [ -4.9275537   -7.359175  ]\n",
      "   [  8.240675     0.8211388 ]\n",
      "   [  2.8364513   -1.1133755 ]]]], device=cuda())\n",
      "B = NDArray([[[[ -1.7671587   -8.082371  ]\n",
      "   [ -1.4591868   -3.807461  ]]\n",
      "\n",
      "  [[  4.2896194    5.705509  ]\n",
      "   [  7.3328934...23  ]\n",
      "   [-10.473016     0.61860955]]\n",
      "\n",
      "  [[ -0.65053475   0.46976614]\n",
      "   [  4.7152305  -13.698386  ]]]], device=cuda())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, A, B):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "A          = NDArray([[[[  8.820262     2.000786  ]\n",
      "   [  4.89369     11.204466  ]\n",
      "   [  9.33779     -4.8863893 ]\n",
      "   [  4.750442   ...3575  ]\n",
      "   [ -4.9275537   -7.359175  ]\n",
      "   [  8.240675     0.8211388 ]\n",
      "   [  2.8364513   -1.1133755 ]]]], device=cuda())\n",
      "B          = NDArray([[[[ -1.7671587   -8.082371  ]\n",
      "   [ -1.4591868   -3.807461  ]]\n",
      "\n",
      "  [[  4.2896194    5.705509  ]\n",
      "   [  7.3328934...23  ]\n",
      "   [-10.473016     0.61860955]]\n",
      "\n",
      "  [[ -0.65053475   0.46976614]\n",
      "   [  4.7152305  -13.698386  ]]]], device=cuda())\n",
      "self       = <needle.ops.ops_mathematic.Conv object at 0x7fb2daedf510>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:504: NotImplementedError\n",
      "\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape0-W_shape0-1-0]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape1-W_shape1-1-1]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape2-W_shape2-1-2]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape3-W_shape3-1-0]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape4-W_shape4-1-0]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape5-W_shape5-2-0]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape6-W_shape6-2-1]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape7-W_shape7-2-2]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape8-W_shape8-2-0]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape9-W_shape9-2-0]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape10-W_shape10-1-0]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape11-W_shape11-1-0]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape12-W_shape12-1-0]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape13-W_shape13-1-0]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape14-W_shape14-1-0]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape15-W_shape15-1-0]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape16-W_shape16-1-0]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape0-W_shape0-1-0]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape1-W_shape1-1-1]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape2-W_shape2-1-2]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape3-W_shape3-1-0]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape4-W_shape4-1-0]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape5-W_shape5-2-0]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape6-W_shape6-2-1]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape7-W_shape7-2-2]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape8-W_shape8-2-0]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape9-W_shape9-2-0]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape10-W_shape10-1-0]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape11-W_shape11-1-0]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape12-W_shape12-1-0]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape13-W_shape13-1-0]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape14-W_shape14-1-0]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape15-W_shape15-1-0]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape16-W_shape16-1-0]\u001b[0m - NotImplementedError\n",
      "\u001b[31m===================== \u001b[31m\u001b[1m34 failed\u001b[0m, \u001b[33m1769 deselected\u001b[0m\u001b[31m in 4.83s\u001b[0m\u001b[31m ======================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m pytest -l -v -k \"op_conv and forward\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the gradients of 2D multi-channel convolution can be technically quite challenging (especially \"rigorously\"). We will try to provide some useful hints here. Basically, we encourage you to make use of the surprising fact that _whatever makes the dimensions work out is typically right_.\n",
    "\n",
    "Ultimately, the backward pass of convolution can be done in terms of the convolution operator itself, with some clever manipulations using `flip`, `dilate`, and multiple applications of `transpose` to both the arguments and the results.\n",
    "\n",
    "In the last section, we essentially implemented convolution as a matrix product: ignoring the various restride and reshape operations, we basically have something like `X @ W`, where `X` is the input and `W` is the weight. We also have `out_grad`, which is the same shape as `X @ W`. Now, you have already implemented the backward pass of matrix multiplication in a previous assignment, and we can use this knowledge to get some insight into the backward pass of convolution. In particular, referencing your matmul backward implementation, you may notice (heuristically speaking here):\n",
    "\n",
    "`X.grad = out_grad @ W.transpose` \\\n",
    "`W.grad = X.transpose @ out_grad`\n",
    "\n",
    "Surprisingly enough, things work out if we just assume that these are also convolutions (and now assuming that `out_grad`, `W`, and `X` are tensors amenable to 2D multi-channel convolution instead of matrices):\n",
    "\n",
    "`X.grad = conv(out_grad, W)` \\\n",
    "`W.grad = conv(X, out_grad)`\n",
    "\n",
    "In which the \"\" indicates that you need to apply some additional operators to these terms in order to get the dimensions to work out, such as permuting/transposing axes, dilating, changing the `padding=` argument to the convolution function, or permuting/transposing axes of the resulting convolution.\n",
    "\n",
    "As we saw on the [last few slides here](https://dlsyscourse.org/slides/conv_nets.pdf) in class, the transpose of a convolution can be found by simply flipping the kernel. Since we're working in 2D instead of 1D, this means flipping the kernel both vertically and horizontally (thus why we implemented `flip`).\n",
    "\n",
    "Summarizing some hints for both `X.grad` and `W.grad`:\n",
    "\n",
    "`X.grad`\n",
    "- The convolution of `out_grad` and `W`, with some operations applied to those\n",
    "- `W` should be flipped over both the kernel dimensions\n",
    "- If the convolution is strided, increase the size of `out_grad` with a corresponding dilation\n",
    "- Do an example to analyze dimensions: note the shape you want for `X.grad`, and think about how you must permute/transpose the arguments and add padding to the convolution to achieve this shape \n",
    "    - This padding depends on both the kernel size and the `padding` argument to the convolution\n",
    "\n",
    "`W.grad`\n",
    "- The convolution of `X` and `out_grad`, with some operations applied to those\n",
    "- The gradients of `W` must be accumulated over the batches; how can you make the conv operator itself do this accumulation?\n",
    "    - Consider turning batches into channels via transpose/permute\n",
    "- Analyze dimensions: how can you modify `X` and `out_grad` so that the shape of their convolution matches the shape of `W`? You may need to transpose/permute the result.\n",
    "    - Remember to account for the `padding` argument passed to convolution\n",
    "\n",
    "General tips\n",
    "- Deal with strided convolutions last (you should be able to just drop in `dilate` when you've passed most of the tests)\n",
    "- Start with the case where `padding=0`, then consider changing `padding` arguments\n",
    "- You can \"permute\" axes with multiple calls to `transpose`\n",
    "\n",
    "It might also be useful to skip ahead to nn.Conv, pass the forward tests, and then use both the tests below and the nn.Conv backward tests to debug your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.11.10, pytest-8.3.4, pluggy-1.5.0 -- /opt/conda/envs/train/bin/python\n",
      "cachedir: .pytest_cache\n",
      "hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase(PosixPath('/mnt/zhumingzhu/work/00test/hw4/.hypothesis/examples'))\n",
      "rootdir: /mnt/zhumingzhu/work/00test/hw4\n",
      "plugins: hypothesis-6.115.5\n",
      "collected 1803 items / 1769 deselected / 34 selected                           \u001b[0m\u001b[1m\n",
      "\n",
      "tests/hw4/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape0-W_shape0-1-0] \u001b[31mFAILED\u001b[0m\u001b[31m [  2%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape1-W_shape1-1-1] \u001b[31mFAILED\u001b[0m\u001b[31m [  5%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape2-W_shape2-1-2] \u001b[31mFAILED\u001b[0m\u001b[31m [  8%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape3-W_shape3-1-0] \u001b[31mFAILED\u001b[0m\u001b[31m [ 11%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape4-W_shape4-1-0] \u001b[31mFAILED\u001b[0m\u001b[31m [ 14%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape5-W_shape5-2-0] \u001b[31mFAILED\u001b[0m\u001b[31m [ 17%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape6-W_shape6-2-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 20%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape7-W_shape7-2-2] \u001b[31mFAILED\u001b[0m\u001b[31m [ 23%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape8-W_shape8-2-0] \u001b[31mFAILED\u001b[0m\u001b[31m [ 26%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape9-W_shape9-2-0] \u001b[31mFAILED\u001b[0m\u001b[31m [ 29%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape10-W_shape10-1-0] \u001b[31mFAILED\u001b[0m\u001b[31m [ 32%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape11-W_shape11-1-0] \u001b[31mFAILED\u001b[0m\u001b[31m [ 35%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape12-W_shape12-1-0] \u001b[31mFAILED\u001b[0m\u001b[31m [ 38%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape13-W_shape13-1-0] \u001b[31mFAILED\u001b[0m\u001b[31m [ 41%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape14-W_shape14-1-0] \u001b[31mFAILED\u001b[0m\u001b[31m [ 44%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape15-W_shape15-1-0] \u001b[31mFAILED\u001b[0m\u001b[31m [ 47%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape16-W_shape16-1-0] \u001b[31mFAILED\u001b[0m\u001b[31m [ 50%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape0-W_shape0-1-0] \u001b[31mFAILED\u001b[0m\u001b[31m [ 52%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape1-W_shape1-1-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 55%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape2-W_shape2-1-2] \u001b[31mFAILED\u001b[0m\u001b[31m [ 58%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape3-W_shape3-1-0] \u001b[31mFAILED\u001b[0m\u001b[31m [ 61%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape4-W_shape4-1-0] \u001b[31mFAILED\u001b[0m\u001b[31m [ 64%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape5-W_shape5-2-0] \u001b[31mFAILED\u001b[0m\u001b[31m [ 67%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape6-W_shape6-2-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 70%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape7-W_shape7-2-2] \u001b[31mFAILED\u001b[0m\u001b[31m [ 73%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape8-W_shape8-2-0] \u001b[31mFAILED\u001b[0m\u001b[31m [ 76%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape9-W_shape9-2-0] \u001b[31mFAILED\u001b[0m\u001b[31m [ 79%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape10-W_shape10-1-0] \u001b[31mFAILED\u001b[0m\u001b[31m [ 82%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape11-W_shape11-1-0] \u001b[31mFAILED\u001b[0m\u001b[31m [ 85%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape12-W_shape12-1-0] \u001b[31mFAILED\u001b[0m\u001b[31m [ 88%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape13-W_shape13-1-0] \u001b[31mFAILED\u001b[0m\u001b[31m [ 91%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape14-W_shape14-1-0] \u001b[31mFAILED\u001b[0m\u001b[31m [ 94%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape15-W_shape15-1-0] \u001b[31mFAILED\u001b[0m\u001b[31m [ 97%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape16-W_shape16-1-0] \u001b[31mFAILED\u001b[0m\u001b[31m [100%]\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m_ test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape0-W_shape0-1-0] _\u001b[0m\n",
      "\n",
      "Z_shape = (3, 14, 14, 8), W_shape = (3, 3, 8, 16), stride = 1, padding = 0\n",
      "backward = True, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mimport\u001b[39;49;00m \u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "W          = needle.Tensor([[[[  4.282669     8.133604    -4.6223483  ...   2.0798197\n",
      "      2.1704152   -0.38803983]\n",
      "   [  2.245844...2.618842     0.71487164]\n",
      "   [ -4.09374      3.7291534  -10.551835   ...   0.31741843\n",
      "      0.03223436   2.0512383 ]]]])\n",
      "W_shape    = (3, 3, 8, 16)\n",
      "Z          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e...3e+00]\n",
      "   [ 1.09905624e+01  2.23166728e+00  4.62055349e+00 ... -6.23090088e-01\n",
      "    -8.48226726e-01 -2.29984760e+00]]]])\n",
      "Z_shape    = (3, 14, 14, 8)\n",
      "_W         = array([[[[  4.282669  ,   8.133604  ,  -4.6223483 , ...,   2.0798197 ,\n",
      "            2.1704152 ,  -0.38803983],\n",
      "        ... [ -4.09374   ,   3.7291534 , -10.551835  , ...,   0.31741843,\n",
      "            0.03223436,   2.0512383 ]]]], dtype=float32)\n",
      "_Z         = array([[[[ 8.82026196e+00,  2.00078607e+00,  4.89369011e+00, ...,\n",
      "          -4.88638926e+00,  4.75044203e+00, -7.56786...166728e+00,  4.62055349e+00, ...,\n",
      "          -6.23090088e-01, -8.48226726e-01, -2.29984760e+00]]]],\n",
      "      dtype=float32)\n",
      "backward   = True\n",
      "device     = cpu()\n",
      "padding    = 0\n",
      "stride     = 1\n",
      "torch      = <module 'torch' from '/opt/conda/envs/train/lib/python3.11/site-packages/torch/__init__.py'>\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:429: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:514: in conv\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Conv(stride, padding)(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e...3e+00]\n",
      "   [ 1.09905624e+01  2.23166728e+00  4.62055349e+00 ... -6.23090088e-01\n",
      "    -8.48226726e-01 -2.29984760e+00]]]])\n",
      "        b          = needle.Tensor([[[[  4.282669     8.133604    -4.6223483  ...   2.0798197\n",
      "      2.1704152   -0.38803983]\n",
      "   [  2.245844...2.618842     0.71487164]\n",
      "   [ -4.09374      3.7291534  -10.551835   ...   0.31741843\n",
      "      0.03223436   2.0512383 ]]]])\n",
      "        padding    = 0\n",
      "        stride     = 1\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048....618842     0.71487164]\n",
      "   [ -4.09374      3.7291534  -10.551835   ...   0.31741843\n",
      "      0.03223436   2.0512383 ]]]]))\n",
      "        self       = <needle.ops.ops_mathematic.Conv object at 0x7fde4fa2d7d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048....618842     0.71487164]\n",
      "   [ -4.09374      3.7291534  -10.551835   ...   0.31741843\n",
      "      0.03223436   2.0512383 ]]]]))\n",
      "        op         = <needle.ops.ops_mathematic.Conv object at 0x7fde4fa2d7d0>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fde4fa16b50>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fde4fa16b50>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Conv object at 0x7fde4fa2d7d0>\n",
      "A = NDArray([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e-01]\n",
      " ...09905624e+01  2.23166728e+00  4.62055349e+00 ... -6.23090088e-01\n",
      "    -8.48226726e-01 -2.29984760e+00]]]], device=cpu())\n",
      "B = NDArray([[[[  4.282669     8.133604    -4.6223483  ...   2.0798197\n",
      "      2.1704152   -0.38803983]\n",
      "   [  2.2458448   -0....71487164]\n",
      "   [ -4.09374      3.7291534  -10.551835   ...   0.31741843\n",
      "      0.03223436   2.0512383 ]]]], device=cpu())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, A, B):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "A          = NDArray([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e-01]\n",
      " ...09905624e+01  2.23166728e+00  4.62055349e+00 ... -6.23090088e-01\n",
      "    -8.48226726e-01 -2.29984760e+00]]]], device=cpu())\n",
      "B          = NDArray([[[[  4.282669     8.133604    -4.6223483  ...   2.0798197\n",
      "      2.1704152   -0.38803983]\n",
      "   [  2.2458448   -0....71487164]\n",
      "   [ -4.09374      3.7291534  -10.551835   ...   0.31741843\n",
      "      0.03223436   2.0512383 ]]]], device=cpu())\n",
      "self       = <needle.ops.ops_mathematic.Conv object at 0x7fde4fa2d7d0>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:504: NotImplementedError\n",
      "\u001b[31m\u001b[1m_ test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape1-W_shape1-1-1] _\u001b[0m\n",
      "\n",
      "Z_shape = (3, 14, 14, 8), W_shape = (3, 3, 8, 16), stride = 1, padding = 1\n",
      "backward = True, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mimport\u001b[39;49;00m \u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "W          = needle.Tensor([[[[  4.282669     8.133604    -4.6223483  ...   2.0798197\n",
      "      2.1704152   -0.38803983]\n",
      "   [  2.245844...2.618842     0.71487164]\n",
      "   [ -4.09374      3.7291534  -10.551835   ...   0.31741843\n",
      "      0.03223436   2.0512383 ]]]])\n",
      "W_shape    = (3, 3, 8, 16)\n",
      "Z          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e...3e+00]\n",
      "   [ 1.09905624e+01  2.23166728e+00  4.62055349e+00 ... -6.23090088e-01\n",
      "    -8.48226726e-01 -2.29984760e+00]]]])\n",
      "Z_shape    = (3, 14, 14, 8)\n",
      "_W         = array([[[[  4.282669  ,   8.133604  ,  -4.6223483 , ...,   2.0798197 ,\n",
      "            2.1704152 ,  -0.38803983],\n",
      "        ... [ -4.09374   ,   3.7291534 , -10.551835  , ...,   0.31741843,\n",
      "            0.03223436,   2.0512383 ]]]], dtype=float32)\n",
      "_Z         = array([[[[ 8.82026196e+00,  2.00078607e+00,  4.89369011e+00, ...,\n",
      "          -4.88638926e+00,  4.75044203e+00, -7.56786...166728e+00,  4.62055349e+00, ...,\n",
      "          -6.23090088e-01, -8.48226726e-01, -2.29984760e+00]]]],\n",
      "      dtype=float32)\n",
      "backward   = True\n",
      "device     = cpu()\n",
      "padding    = 1\n",
      "stride     = 1\n",
      "torch      = <module 'torch' from '/opt/conda/envs/train/lib/python3.11/site-packages/torch/__init__.py'>\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:429: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:514: in conv\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Conv(stride, padding)(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e...3e+00]\n",
      "   [ 1.09905624e+01  2.23166728e+00  4.62055349e+00 ... -6.23090088e-01\n",
      "    -8.48226726e-01 -2.29984760e+00]]]])\n",
      "        b          = needle.Tensor([[[[  4.282669     8.133604    -4.6223483  ...   2.0798197\n",
      "      2.1704152   -0.38803983]\n",
      "   [  2.245844...2.618842     0.71487164]\n",
      "   [ -4.09374      3.7291534  -10.551835   ...   0.31741843\n",
      "      0.03223436   2.0512383 ]]]])\n",
      "        padding    = 1\n",
      "        stride     = 1\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048....618842     0.71487164]\n",
      "   [ -4.09374      3.7291534  -10.551835   ...   0.31741843\n",
      "      0.03223436   2.0512383 ]]]]))\n",
      "        self       = <needle.ops.ops_mathematic.Conv object at 0x7fd94f506a50>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048....618842     0.71487164]\n",
      "   [ -4.09374      3.7291534  -10.551835   ...   0.31741843\n",
      "      0.03223436   2.0512383 ]]]]))\n",
      "        op         = <needle.ops.ops_mathematic.Conv object at 0x7fd94f506a50>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fd94f506410>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fd94f506410>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Conv object at 0x7fd94f506a50>\n",
      "A = NDArray([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e-01]\n",
      " ...09905624e+01  2.23166728e+00  4.62055349e+00 ... -6.23090088e-01\n",
      "    -8.48226726e-01 -2.29984760e+00]]]], device=cpu())\n",
      "B = NDArray([[[[  4.282669     8.133604    -4.6223483  ...   2.0798197\n",
      "      2.1704152   -0.38803983]\n",
      "   [  2.2458448   -0....71487164]\n",
      "   [ -4.09374      3.7291534  -10.551835   ...   0.31741843\n",
      "      0.03223436   2.0512383 ]]]], device=cpu())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, A, B):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "A          = NDArray([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e-01]\n",
      " ...09905624e+01  2.23166728e+00  4.62055349e+00 ... -6.23090088e-01\n",
      "    -8.48226726e-01 -2.29984760e+00]]]], device=cpu())\n",
      "B          = NDArray([[[[  4.282669     8.133604    -4.6223483  ...   2.0798197\n",
      "      2.1704152   -0.38803983]\n",
      "   [  2.2458448   -0....71487164]\n",
      "   [ -4.09374      3.7291534  -10.551835   ...   0.31741843\n",
      "      0.03223436   2.0512383 ]]]], device=cpu())\n",
      "self       = <needle.ops.ops_mathematic.Conv object at 0x7fd94f506a50>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:504: NotImplementedError\n",
      "\u001b[31m\u001b[1m_ test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape2-W_shape2-1-2] _\u001b[0m\n",
      "\n",
      "Z_shape = (3, 16, 16, 8), W_shape = (3, 3, 8, 16), stride = 1, padding = 2\n",
      "backward = True, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mimport\u001b[39;49;00m \u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "W          = needle.Tensor([[[[  0.40910086  -0.64982927  11.320294   ...  -0.12077556\n",
      "      1.428692    -5.5845156 ]\n",
      "   [ -6.89178...-1.5437248  -10.136281  ]\n",
      "   [  1.1511405    0.7441038   16.463472   ...  -6.8931007\n",
      "     -0.5833115   -2.504636  ]]]])\n",
      "W_shape    = (3, 3, 8, 16)\n",
      "Z          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e...5e+00]\n",
      "   [ 6.10367346e+00 -2.05434513e+00 -4.41384506e+00 ... -6.15245628e+00\n",
      "     3.33514667e+00 -1.35630560e+00]]]])\n",
      "Z_shape    = (3, 16, 16, 8)\n",
      "_W         = array([[[[  0.40910086,  -0.64982927,  11.320294  , ...,  -0.12077556,\n",
      "            1.428692  ,  -5.5845156 ],\n",
      "        ... [  1.1511405 ,   0.7441038 ,  16.463472  , ...,  -6.8931007 ,\n",
      "           -0.5833115 ,  -2.504636  ]]]], dtype=float32)\n",
      "_Z         = array([[[[ 8.82026196e+00,  2.00078607e+00,  4.89369011e+00, ...,\n",
      "          -4.88638926e+00,  4.75044203e+00, -7.56786...434513e+00, -4.41384506e+00, ...,\n",
      "          -6.15245628e+00,  3.33514667e+00, -1.35630560e+00]]]],\n",
      "      dtype=float32)\n",
      "backward   = True\n",
      "device     = cpu()\n",
      "padding    = 2\n",
      "stride     = 1\n",
      "torch      = <module 'torch' from '/opt/conda/envs/train/lib/python3.11/site-packages/torch/__init__.py'>\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:429: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:514: in conv\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Conv(stride, padding)(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e...5e+00]\n",
      "   [ 6.10367346e+00 -2.05434513e+00 -4.41384506e+00 ... -6.15245628e+00\n",
      "     3.33514667e+00 -1.35630560e+00]]]])\n",
      "        b          = needle.Tensor([[[[  0.40910086  -0.64982927  11.320294   ...  -0.12077556\n",
      "      1.428692    -5.5845156 ]\n",
      "   [ -6.89178...-1.5437248  -10.136281  ]\n",
      "   [  1.1511405    0.7441038   16.463472   ...  -6.8931007\n",
      "     -0.5833115   -2.504636  ]]]])\n",
      "        padding    = 2\n",
      "        stride     = 1\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048...1.5437248  -10.136281  ]\n",
      "   [  1.1511405    0.7441038   16.463472   ...  -6.8931007\n",
      "     -0.5833115   -2.504636  ]]]]))\n",
      "        self       = <needle.ops.ops_mathematic.Conv object at 0x7fd94f5cc810>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048...1.5437248  -10.136281  ]\n",
      "   [  1.1511405    0.7441038   16.463472   ...  -6.8931007\n",
      "     -0.5833115   -2.504636  ]]]]))\n",
      "        op         = <needle.ops.ops_mathematic.Conv object at 0x7fd94f5cc810>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fd94f5ccb50>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fd94f5ccb50>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Conv object at 0x7fd94f5cc810>\n",
      "A = NDArray([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e-01]\n",
      " ...10367346e+00 -2.05434513e+00 -4.41384506e+00 ... -6.15245628e+00\n",
      "     3.33514667e+00 -1.35630560e+00]]]], device=cpu())\n",
      "B = NDArray([[[[  0.40910086  -0.64982927  11.320294   ...  -0.12077556\n",
      "      1.428692    -5.5845156 ]\n",
      "   [ -6.891782    -...0.136281  ]\n",
      "   [  1.1511405    0.7441038   16.463472   ...  -6.8931007\n",
      "     -0.5833115   -2.504636  ]]]], device=cpu())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, A, B):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "A          = NDArray([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e-01]\n",
      " ...10367346e+00 -2.05434513e+00 -4.41384506e+00 ... -6.15245628e+00\n",
      "     3.33514667e+00 -1.35630560e+00]]]], device=cpu())\n",
      "B          = NDArray([[[[  0.40910086  -0.64982927  11.320294   ...  -0.12077556\n",
      "      1.428692    -5.5845156 ]\n",
      "   [ -6.891782    -...0.136281  ]\n",
      "   [  1.1511405    0.7441038   16.463472   ...  -6.8931007\n",
      "     -0.5833115   -2.504636  ]]]], device=cpu())\n",
      "self       = <needle.ops.ops_mathematic.Conv object at 0x7fd94f5cc810>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:504: NotImplementedError\n",
      "\u001b[31m\u001b[1m_ test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape3-W_shape3-1-0] _\u001b[0m\n",
      "\n",
      "Z_shape = (3, 16, 16, 8), W_shape = (3, 3, 8, 14), stride = 1, padding = 0\n",
      "backward = True, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mimport\u001b[39;49;00m \u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "W          = needle.Tensor([[[[ 4.09100860e-01 -6.49829268e-01  1.13202944e+01 ... -5.56089449e+00\n",
      "    -4.84385538e+00 -1.20775558e...5e+00]\n",
      "   [-1.10714078e+00  2.54448557e+00 -7.73867989e+00 ... -1.13336074e+00\n",
      "    -1.21658230e+00 -4.79330921e+00]]]])\n",
      "W_shape    = (3, 3, 8, 14)\n",
      "Z          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e...5e+00]\n",
      "   [ 6.10367346e+00 -2.05434513e+00 -4.41384506e+00 ... -6.15245628e+00\n",
      "     3.33514667e+00 -1.35630560e+00]]]])\n",
      "Z_shape    = (3, 16, 16, 8)\n",
      "_W         = array([[[[ 4.09100860e-01, -6.49829268e-01,  1.13202944e+01, ...,\n",
      "          -5.56089449e+00, -4.84385538e+00, -1.20775...448557e+00, -7.73867989e+00, ...,\n",
      "          -1.13336074e+00, -1.21658230e+00, -4.79330921e+00]]]],\n",
      "      dtype=float32)\n",
      "_Z         = array([[[[ 8.82026196e+00,  2.00078607e+00,  4.89369011e+00, ...,\n",
      "          -4.88638926e+00,  4.75044203e+00, -7.56786...434513e+00, -4.41384506e+00, ...,\n",
      "          -6.15245628e+00,  3.33514667e+00, -1.35630560e+00]]]],\n",
      "      dtype=float32)\n",
      "backward   = True\n",
      "device     = cpu()\n",
      "padding    = 0\n",
      "stride     = 1\n",
      "torch      = <module 'torch' from '/opt/conda/envs/train/lib/python3.11/site-packages/torch/__init__.py'>\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:429: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:514: in conv\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Conv(stride, padding)(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e...5e+00]\n",
      "   [ 6.10367346e+00 -2.05434513e+00 -4.41384506e+00 ... -6.15245628e+00\n",
      "     3.33514667e+00 -1.35630560e+00]]]])\n",
      "        b          = needle.Tensor([[[[ 4.09100860e-01 -6.49829268e-01  1.13202944e+01 ... -5.56089449e+00\n",
      "    -4.84385538e+00 -1.20775558e...5e+00]\n",
      "   [-1.10714078e+00  2.54448557e+00 -7.73867989e+00 ... -1.13336074e+00\n",
      "    -1.21658230e+00 -4.79330921e+00]]]])\n",
      "        padding    = 0\n",
      "        stride     = 1\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048...e+00]\n",
      "   [-1.10714078e+00  2.54448557e+00 -7.73867989e+00 ... -1.13336074e+00\n",
      "    -1.21658230e+00 -4.79330921e+00]]]]))\n",
      "        self       = <needle.ops.ops_mathematic.Conv object at 0x7fd94f577790>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048...e+00]\n",
      "   [-1.10714078e+00  2.54448557e+00 -7.73867989e+00 ... -1.13336074e+00\n",
      "    -1.21658230e+00 -4.79330921e+00]]]]))\n",
      "        op         = <needle.ops.ops_mathematic.Conv object at 0x7fd94f577790>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fd94f576c50>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fd94f576c50>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Conv object at 0x7fd94f577790>\n",
      "A = NDArray([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e-01]\n",
      " ...10367346e+00 -2.05434513e+00 -4.41384506e+00 ... -6.15245628e+00\n",
      "     3.33514667e+00 -1.35630560e+00]]]], device=cpu())\n",
      "B = NDArray([[[[ 4.09100860e-01 -6.49829268e-01  1.13202944e+01 ... -5.56089449e+00\n",
      "    -4.84385538e+00 -1.20775558e-01]\n",
      " ...10714078e+00  2.54448557e+00 -7.73867989e+00 ... -1.13336074e+00\n",
      "    -1.21658230e+00 -4.79330921e+00]]]], device=cpu())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, A, B):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "A          = NDArray([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e-01]\n",
      " ...10367346e+00 -2.05434513e+00 -4.41384506e+00 ... -6.15245628e+00\n",
      "     3.33514667e+00 -1.35630560e+00]]]], device=cpu())\n",
      "B          = NDArray([[[[ 4.09100860e-01 -6.49829268e-01  1.13202944e+01 ... -5.56089449e+00\n",
      "    -4.84385538e+00 -1.20775558e-01]\n",
      " ...10714078e+00  2.54448557e+00 -7.73867989e+00 ... -1.13336074e+00\n",
      "    -1.21658230e+00 -4.79330921e+00]]]], device=cpu())\n",
      "self       = <needle.ops.ops_mathematic.Conv object at 0x7fd94f577790>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:504: NotImplementedError\n",
      "\u001b[31m\u001b[1m_ test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape4-W_shape4-1-0] _\u001b[0m\n",
      "\n",
      "Z_shape = (3, 16, 16, 2), W_shape = (3, 3, 2, 14), stride = 1, padding = 0\n",
      "backward = True, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mimport\u001b[39;49;00m \u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "W          = needle.Tensor([[[[ -8.0968      -2.5552022    8.703147    -1.4674252    4.5861077\n",
      "     -0.28521433   4.383634    -9.13...727333   0.22582927   9.296732    -8.13161     -0.67411226\n",
      "     -2.9204676    1.675528   -12.187821     5.5746226 ]]]])\n",
      "W_shape    = (3, 3, 2, 14)\n",
      "Z          = needle.Tensor([[[[  8.820262     2.000786  ]\n",
      "   [  4.89369     11.204466  ]\n",
      "   [  9.33779     -4.8863893 ]\n",
      "   ...\n",
      "   [...   -5.112822  ]\n",
      "   ...\n",
      "   [ -7.922368     4.2222714 ]\n",
      "   [ -6.064339     1.4188478 ]\n",
      "   [ -1.4109794   -5.791016  ]]]])\n",
      "Z_shape    = (3, 16, 16, 2)\n",
      "_W         = array([[[[ -8.0968    ,  -2.5552022 ,   8.703147  ,  -1.4674252 ,\n",
      "            4.5861077 ,  -0.28521433,   4.383634  , ...        -8.13161   ,  -0.67411226,  -2.9204676 ,   1.675528  ,\n",
      "          -12.187821  ,   5.5746226 ]]]], dtype=float32)\n",
      "_Z         = array([[[[  8.820262  ,   2.000786  ],\n",
      "         [  4.89369   ,  11.204466  ],\n",
      "         [  9.33779   ,  -4.8863893 ],\n",
      " ...22368  ,   4.2222714 ],\n",
      "         [ -6.064339  ,   1.4188478 ],\n",
      "         [ -1.4109794 ,  -5.791016  ]]]], dtype=float32)\n",
      "backward   = True\n",
      "device     = cpu()\n",
      "padding    = 0\n",
      "stride     = 1\n",
      "torch      = <module 'torch' from '/opt/conda/envs/train/lib/python3.11/site-packages/torch/__init__.py'>\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:429: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:514: in conv\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Conv(stride, padding)(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[[  8.820262     2.000786  ]\n",
      "   [  4.89369     11.204466  ]\n",
      "   [  9.33779     -4.8863893 ]\n",
      "   ...\n",
      "   [...   -5.112822  ]\n",
      "   ...\n",
      "   [ -7.922368     4.2222714 ]\n",
      "   [ -6.064339     1.4188478 ]\n",
      "   [ -1.4109794   -5.791016  ]]]])\n",
      "        b          = needle.Tensor([[[[ -8.0968      -2.5552022    8.703147    -1.4674252    4.5861077\n",
      "     -0.28521433   4.383634    -9.13...727333   0.22582927   9.296732    -8.13161     -0.67411226\n",
      "     -2.9204676    1.675528   -12.187821     5.5746226 ]]]])\n",
      "        padding    = 0\n",
      "        stride     = 1\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[  8.820262     2.000786  ]\n",
      "   [  4.89369     11.204466  ]\n",
      "   [  9.33779     -4.8863893 ]\n",
      "   ...\n",
      "   ...27333   0.22582927   9.296732    -8.13161     -0.67411226\n",
      "     -2.9204676    1.675528   -12.187821     5.5746226 ]]]]))\n",
      "        self       = <needle.ops.ops_mathematic.Conv object at 0x7fd94f634bd0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[[  8.820262     2.000786  ]\n",
      "   [  4.89369     11.204466  ]\n",
      "   [  9.33779     -4.8863893 ]\n",
      "   ...\n",
      "   ...27333   0.22582927   9.296732    -8.13161     -0.67411226\n",
      "     -2.9204676    1.675528   -12.187821     5.5746226 ]]]]))\n",
      "        op         = <needle.ops.ops_mathematic.Conv object at 0x7fd94f634bd0>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fd94f634e10>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fd94f634e10>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Conv object at 0x7fd94f634bd0>\n",
      "A = NDArray([[[[  8.820262     2.000786  ]\n",
      "   [  4.89369     11.204466  ]\n",
      "   [  9.33779     -4.8863893 ]\n",
      "   ...\n",
      "   [  0.22...]\n",
      "   ...\n",
      "   [ -7.922368     4.2222714 ]\n",
      "   [ -6.064339     1.4188478 ]\n",
      "   [ -1.4109794   -5.791016  ]]]], device=cpu())\n",
      "B = NDArray([[[[ -8.0968      -2.5552022    8.703147    -1.4674252    4.5861077\n",
      "     -0.28521433   4.383634    -9.134557  ...82927   9.296732    -8.13161     -0.67411226\n",
      "     -2.9204676    1.675528   -12.187821     5.5746226 ]]]], device=cpu())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, A, B):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "A          = NDArray([[[[  8.820262     2.000786  ]\n",
      "   [  4.89369     11.204466  ]\n",
      "   [  9.33779     -4.8863893 ]\n",
      "   ...\n",
      "   [  0.22...]\n",
      "   ...\n",
      "   [ -7.922368     4.2222714 ]\n",
      "   [ -6.064339     1.4188478 ]\n",
      "   [ -1.4109794   -5.791016  ]]]], device=cpu())\n",
      "B          = NDArray([[[[ -8.0968      -2.5552022    8.703147    -1.4674252    4.5861077\n",
      "     -0.28521433   4.383634    -9.134557  ...82927   9.296732    -8.13161     -0.67411226\n",
      "     -2.9204676    1.675528   -12.187821     5.5746226 ]]]], device=cpu())\n",
      "self       = <needle.ops.ops_mathematic.Conv object at 0x7fd94f634bd0>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:504: NotImplementedError\n",
      "\u001b[31m\u001b[1m_ test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape5-W_shape5-2-0] _\u001b[0m\n",
      "\n",
      "Z_shape = (3, 14, 14, 8), W_shape = (3, 3, 8, 16), stride = 2, padding = 0\n",
      "backward = True, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mimport\u001b[39;49;00m \u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "W          = needle.Tensor([[[[  4.282669     8.133604    -4.6223483  ...   2.0798197\n",
      "      2.1704152   -0.38803983]\n",
      "   [  2.245844...2.618842     0.71487164]\n",
      "   [ -4.09374      3.7291534  -10.551835   ...   0.31741843\n",
      "      0.03223436   2.0512383 ]]]])\n",
      "W_shape    = (3, 3, 8, 16)\n",
      "Z          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e...3e+00]\n",
      "   [ 1.09905624e+01  2.23166728e+00  4.62055349e+00 ... -6.23090088e-01\n",
      "    -8.48226726e-01 -2.29984760e+00]]]])\n",
      "Z_shape    = (3, 14, 14, 8)\n",
      "_W         = array([[[[  4.282669  ,   8.133604  ,  -4.6223483 , ...,   2.0798197 ,\n",
      "            2.1704152 ,  -0.38803983],\n",
      "        ... [ -4.09374   ,   3.7291534 , -10.551835  , ...,   0.31741843,\n",
      "            0.03223436,   2.0512383 ]]]], dtype=float32)\n",
      "_Z         = array([[[[ 8.82026196e+00,  2.00078607e+00,  4.89369011e+00, ...,\n",
      "          -4.88638926e+00,  4.75044203e+00, -7.56786...166728e+00,  4.62055349e+00, ...,\n",
      "          -6.23090088e-01, -8.48226726e-01, -2.29984760e+00]]]],\n",
      "      dtype=float32)\n",
      "backward   = True\n",
      "device     = cpu()\n",
      "padding    = 0\n",
      "stride     = 2\n",
      "torch      = <module 'torch' from '/opt/conda/envs/train/lib/python3.11/site-packages/torch/__init__.py'>\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:429: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:514: in conv\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Conv(stride, padding)(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e...3e+00]\n",
      "   [ 1.09905624e+01  2.23166728e+00  4.62055349e+00 ... -6.23090088e-01\n",
      "    -8.48226726e-01 -2.29984760e+00]]]])\n",
      "        b          = needle.Tensor([[[[  4.282669     8.133604    -4.6223483  ...   2.0798197\n",
      "      2.1704152   -0.38803983]\n",
      "   [  2.245844...2.618842     0.71487164]\n",
      "   [ -4.09374      3.7291534  -10.551835   ...   0.31741843\n",
      "      0.03223436   2.0512383 ]]]])\n",
      "        padding    = 0\n",
      "        stride     = 2\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048....618842     0.71487164]\n",
      "   [ -4.09374      3.7291534  -10.551835   ...   0.31741843\n",
      "      0.03223436   2.0512383 ]]]]))\n",
      "        self       = <needle.ops.ops_mathematic.Conv object at 0x7fd94f5eb910>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048....618842     0.71487164]\n",
      "   [ -4.09374      3.7291534  -10.551835   ...   0.31741843\n",
      "      0.03223436   2.0512383 ]]]]))\n",
      "        op         = <needle.ops.ops_mathematic.Conv object at 0x7fd94f5eb910>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fde508fa650>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fde508fa650>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Conv object at 0x7fd94f5eb910>\n",
      "A = NDArray([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e-01]\n",
      " ...09905624e+01  2.23166728e+00  4.62055349e+00 ... -6.23090088e-01\n",
      "    -8.48226726e-01 -2.29984760e+00]]]], device=cpu())\n",
      "B = NDArray([[[[  4.282669     8.133604    -4.6223483  ...   2.0798197\n",
      "      2.1704152   -0.38803983]\n",
      "   [  2.2458448   -0....71487164]\n",
      "   [ -4.09374      3.7291534  -10.551835   ...   0.31741843\n",
      "      0.03223436   2.0512383 ]]]], device=cpu())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, A, B):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "A          = NDArray([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e-01]\n",
      " ...09905624e+01  2.23166728e+00  4.62055349e+00 ... -6.23090088e-01\n",
      "    -8.48226726e-01 -2.29984760e+00]]]], device=cpu())\n",
      "B          = NDArray([[[[  4.282669     8.133604    -4.6223483  ...   2.0798197\n",
      "      2.1704152   -0.38803983]\n",
      "   [  2.2458448   -0....71487164]\n",
      "   [ -4.09374      3.7291534  -10.551835   ...   0.31741843\n",
      "      0.03223436   2.0512383 ]]]], device=cpu())\n",
      "self       = <needle.ops.ops_mathematic.Conv object at 0x7fd94f5eb910>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:504: NotImplementedError\n",
      "\u001b[31m\u001b[1m_ test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape6-W_shape6-2-1] _\u001b[0m\n",
      "\n",
      "Z_shape = (3, 14, 14, 8), W_shape = (3, 3, 8, 16), stride = 2, padding = 1\n",
      "backward = True, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mimport\u001b[39;49;00m \u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "W          = needle.Tensor([[[[  4.282669     8.133604    -4.6223483  ...   2.0798197\n",
      "      2.1704152   -0.38803983]\n",
      "   [  2.245844...2.618842     0.71487164]\n",
      "   [ -4.09374      3.7291534  -10.551835   ...   0.31741843\n",
      "      0.03223436   2.0512383 ]]]])\n",
      "W_shape    = (3, 3, 8, 16)\n",
      "Z          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e...3e+00]\n",
      "   [ 1.09905624e+01  2.23166728e+00  4.62055349e+00 ... -6.23090088e-01\n",
      "    -8.48226726e-01 -2.29984760e+00]]]])\n",
      "Z_shape    = (3, 14, 14, 8)\n",
      "_W         = array([[[[  4.282669  ,   8.133604  ,  -4.6223483 , ...,   2.0798197 ,\n",
      "            2.1704152 ,  -0.38803983],\n",
      "        ... [ -4.09374   ,   3.7291534 , -10.551835  , ...,   0.31741843,\n",
      "            0.03223436,   2.0512383 ]]]], dtype=float32)\n",
      "_Z         = array([[[[ 8.82026196e+00,  2.00078607e+00,  4.89369011e+00, ...,\n",
      "          -4.88638926e+00,  4.75044203e+00, -7.56786...166728e+00,  4.62055349e+00, ...,\n",
      "          -6.23090088e-01, -8.48226726e-01, -2.29984760e+00]]]],\n",
      "      dtype=float32)\n",
      "backward   = True\n",
      "device     = cpu()\n",
      "padding    = 1\n",
      "stride     = 2\n",
      "torch      = <module 'torch' from '/opt/conda/envs/train/lib/python3.11/site-packages/torch/__init__.py'>\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:429: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:514: in conv\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Conv(stride, padding)(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e...3e+00]\n",
      "   [ 1.09905624e+01  2.23166728e+00  4.62055349e+00 ... -6.23090088e-01\n",
      "    -8.48226726e-01 -2.29984760e+00]]]])\n",
      "        b          = needle.Tensor([[[[  4.282669     8.133604    -4.6223483  ...   2.0798197\n",
      "      2.1704152   -0.38803983]\n",
      "   [  2.245844...2.618842     0.71487164]\n",
      "   [ -4.09374      3.7291534  -10.551835   ...   0.31741843\n",
      "      0.03223436   2.0512383 ]]]])\n",
      "        padding    = 1\n",
      "        stride     = 2\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048....618842     0.71487164]\n",
      "   [ -4.09374      3.7291534  -10.551835   ...   0.31741843\n",
      "      0.03223436   2.0512383 ]]]]))\n",
      "        self       = <needle.ops.ops_mathematic.Conv object at 0x7fd94f5cb510>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048....618842     0.71487164]\n",
      "   [ -4.09374      3.7291534  -10.551835   ...   0.31741843\n",
      "      0.03223436   2.0512383 ]]]]))\n",
      "        op         = <needle.ops.ops_mathematic.Conv object at 0x7fd94f5cb510>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fd94f5c9190>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fd94f5c9190>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Conv object at 0x7fd94f5cb510>\n",
      "A = NDArray([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e-01]\n",
      " ...09905624e+01  2.23166728e+00  4.62055349e+00 ... -6.23090088e-01\n",
      "    -8.48226726e-01 -2.29984760e+00]]]], device=cpu())\n",
      "B = NDArray([[[[  4.282669     8.133604    -4.6223483  ...   2.0798197\n",
      "      2.1704152   -0.38803983]\n",
      "   [  2.2458448   -0....71487164]\n",
      "   [ -4.09374      3.7291534  -10.551835   ...   0.31741843\n",
      "      0.03223436   2.0512383 ]]]], device=cpu())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, A, B):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "A          = NDArray([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e-01]\n",
      " ...09905624e+01  2.23166728e+00  4.62055349e+00 ... -6.23090088e-01\n",
      "    -8.48226726e-01 -2.29984760e+00]]]], device=cpu())\n",
      "B          = NDArray([[[[  4.282669     8.133604    -4.6223483  ...   2.0798197\n",
      "      2.1704152   -0.38803983]\n",
      "   [  2.2458448   -0....71487164]\n",
      "   [ -4.09374      3.7291534  -10.551835   ...   0.31741843\n",
      "      0.03223436   2.0512383 ]]]], device=cpu())\n",
      "self       = <needle.ops.ops_mathematic.Conv object at 0x7fd94f5cb510>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:504: NotImplementedError\n",
      "\u001b[31m\u001b[1m_ test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape7-W_shape7-2-2] _\u001b[0m\n",
      "\n",
      "Z_shape = (3, 16, 16, 8), W_shape = (3, 3, 8, 16), stride = 2, padding = 2\n",
      "backward = True, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mimport\u001b[39;49;00m \u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "W          = needle.Tensor([[[[  0.40910086  -0.64982927  11.320294   ...  -0.12077556\n",
      "      1.428692    -5.5845156 ]\n",
      "   [ -6.89178...-1.5437248  -10.136281  ]\n",
      "   [  1.1511405    0.7441038   16.463472   ...  -6.8931007\n",
      "     -0.5833115   -2.504636  ]]]])\n",
      "W_shape    = (3, 3, 8, 16)\n",
      "Z          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e...5e+00]\n",
      "   [ 6.10367346e+00 -2.05434513e+00 -4.41384506e+00 ... -6.15245628e+00\n",
      "     3.33514667e+00 -1.35630560e+00]]]])\n",
      "Z_shape    = (3, 16, 16, 8)\n",
      "_W         = array([[[[  0.40910086,  -0.64982927,  11.320294  , ...,  -0.12077556,\n",
      "            1.428692  ,  -5.5845156 ],\n",
      "        ... [  1.1511405 ,   0.7441038 ,  16.463472  , ...,  -6.8931007 ,\n",
      "           -0.5833115 ,  -2.504636  ]]]], dtype=float32)\n",
      "_Z         = array([[[[ 8.82026196e+00,  2.00078607e+00,  4.89369011e+00, ...,\n",
      "          -4.88638926e+00,  4.75044203e+00, -7.56786...434513e+00, -4.41384506e+00, ...,\n",
      "          -6.15245628e+00,  3.33514667e+00, -1.35630560e+00]]]],\n",
      "      dtype=float32)\n",
      "backward   = True\n",
      "device     = cpu()\n",
      "padding    = 2\n",
      "stride     = 2\n",
      "torch      = <module 'torch' from '/opt/conda/envs/train/lib/python3.11/site-packages/torch/__init__.py'>\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:429: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:514: in conv\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Conv(stride, padding)(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e...5e+00]\n",
      "   [ 6.10367346e+00 -2.05434513e+00 -4.41384506e+00 ... -6.15245628e+00\n",
      "     3.33514667e+00 -1.35630560e+00]]]])\n",
      "        b          = needle.Tensor([[[[  0.40910086  -0.64982927  11.320294   ...  -0.12077556\n",
      "      1.428692    -5.5845156 ]\n",
      "   [ -6.89178...-1.5437248  -10.136281  ]\n",
      "   [  1.1511405    0.7441038   16.463472   ...  -6.8931007\n",
      "     -0.5833115   -2.504636  ]]]])\n",
      "        padding    = 2\n",
      "        stride     = 2\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048...1.5437248  -10.136281  ]\n",
      "   [  1.1511405    0.7441038   16.463472   ...  -6.8931007\n",
      "     -0.5833115   -2.504636  ]]]]))\n",
      "        self       = <needle.ops.ops_mathematic.Conv object at 0x7fd94f6c8110>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048...1.5437248  -10.136281  ]\n",
      "   [  1.1511405    0.7441038   16.463472   ...  -6.8931007\n",
      "     -0.5833115   -2.504636  ]]]]))\n",
      "        op         = <needle.ops.ops_mathematic.Conv object at 0x7fd94f6c8110>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fd94f6cb3d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fd94f6cb3d0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Conv object at 0x7fd94f6c8110>\n",
      "A = NDArray([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e-01]\n",
      " ...10367346e+00 -2.05434513e+00 -4.41384506e+00 ... -6.15245628e+00\n",
      "     3.33514667e+00 -1.35630560e+00]]]], device=cpu())\n",
      "B = NDArray([[[[  0.40910086  -0.64982927  11.320294   ...  -0.12077556\n",
      "      1.428692    -5.5845156 ]\n",
      "   [ -6.891782    -...0.136281  ]\n",
      "   [  1.1511405    0.7441038   16.463472   ...  -6.8931007\n",
      "     -0.5833115   -2.504636  ]]]], device=cpu())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, A, B):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "A          = NDArray([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e-01]\n",
      " ...10367346e+00 -2.05434513e+00 -4.41384506e+00 ... -6.15245628e+00\n",
      "     3.33514667e+00 -1.35630560e+00]]]], device=cpu())\n",
      "B          = NDArray([[[[  0.40910086  -0.64982927  11.320294   ...  -0.12077556\n",
      "      1.428692    -5.5845156 ]\n",
      "   [ -6.891782    -...0.136281  ]\n",
      "   [  1.1511405    0.7441038   16.463472   ...  -6.8931007\n",
      "     -0.5833115   -2.504636  ]]]], device=cpu())\n",
      "self       = <needle.ops.ops_mathematic.Conv object at 0x7fd94f6c8110>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:504: NotImplementedError\n",
      "\u001b[31m\u001b[1m_ test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape8-W_shape8-2-0] _\u001b[0m\n",
      "\n",
      "Z_shape = (3, 16, 16, 8), W_shape = (3, 3, 8, 14), stride = 2, padding = 0\n",
      "backward = True, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mimport\u001b[39;49;00m \u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "W          = needle.Tensor([[[[ 4.09100860e-01 -6.49829268e-01  1.13202944e+01 ... -5.56089449e+00\n",
      "    -4.84385538e+00 -1.20775558e...5e+00]\n",
      "   [-1.10714078e+00  2.54448557e+00 -7.73867989e+00 ... -1.13336074e+00\n",
      "    -1.21658230e+00 -4.79330921e+00]]]])\n",
      "W_shape    = (3, 3, 8, 14)\n",
      "Z          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e...5e+00]\n",
      "   [ 6.10367346e+00 -2.05434513e+00 -4.41384506e+00 ... -6.15245628e+00\n",
      "     3.33514667e+00 -1.35630560e+00]]]])\n",
      "Z_shape    = (3, 16, 16, 8)\n",
      "_W         = array([[[[ 4.09100860e-01, -6.49829268e-01,  1.13202944e+01, ...,\n",
      "          -5.56089449e+00, -4.84385538e+00, -1.20775...448557e+00, -7.73867989e+00, ...,\n",
      "          -1.13336074e+00, -1.21658230e+00, -4.79330921e+00]]]],\n",
      "      dtype=float32)\n",
      "_Z         = array([[[[ 8.82026196e+00,  2.00078607e+00,  4.89369011e+00, ...,\n",
      "          -4.88638926e+00,  4.75044203e+00, -7.56786...434513e+00, -4.41384506e+00, ...,\n",
      "          -6.15245628e+00,  3.33514667e+00, -1.35630560e+00]]]],\n",
      "      dtype=float32)\n",
      "backward   = True\n",
      "device     = cpu()\n",
      "padding    = 0\n",
      "stride     = 2\n",
      "torch      = <module 'torch' from '/opt/conda/envs/train/lib/python3.11/site-packages/torch/__init__.py'>\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:429: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:514: in conv\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Conv(stride, padding)(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e...5e+00]\n",
      "   [ 6.10367346e+00 -2.05434513e+00 -4.41384506e+00 ... -6.15245628e+00\n",
      "     3.33514667e+00 -1.35630560e+00]]]])\n",
      "        b          = needle.Tensor([[[[ 4.09100860e-01 -6.49829268e-01  1.13202944e+01 ... -5.56089449e+00\n",
      "    -4.84385538e+00 -1.20775558e...5e+00]\n",
      "   [-1.10714078e+00  2.54448557e+00 -7.73867989e+00 ... -1.13336074e+00\n",
      "    -1.21658230e+00 -4.79330921e+00]]]])\n",
      "        padding    = 0\n",
      "        stride     = 2\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048...e+00]\n",
      "   [-1.10714078e+00  2.54448557e+00 -7.73867989e+00 ... -1.13336074e+00\n",
      "    -1.21658230e+00 -4.79330921e+00]]]]))\n",
      "        self       = <needle.ops.ops_mathematic.Conv object at 0x7fd94f7191d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048...e+00]\n",
      "   [-1.10714078e+00  2.54448557e+00 -7.73867989e+00 ... -1.13336074e+00\n",
      "    -1.21658230e+00 -4.79330921e+00]]]]))\n",
      "        op         = <needle.ops.ops_mathematic.Conv object at 0x7fd94f7191d0>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fd94f71a2d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fd94f71a2d0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Conv object at 0x7fd94f7191d0>\n",
      "A = NDArray([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e-01]\n",
      " ...10367346e+00 -2.05434513e+00 -4.41384506e+00 ... -6.15245628e+00\n",
      "     3.33514667e+00 -1.35630560e+00]]]], device=cpu())\n",
      "B = NDArray([[[[ 4.09100860e-01 -6.49829268e-01  1.13202944e+01 ... -5.56089449e+00\n",
      "    -4.84385538e+00 -1.20775558e-01]\n",
      " ...10714078e+00  2.54448557e+00 -7.73867989e+00 ... -1.13336074e+00\n",
      "    -1.21658230e+00 -4.79330921e+00]]]], device=cpu())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, A, B):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "A          = NDArray([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e-01]\n",
      " ...10367346e+00 -2.05434513e+00 -4.41384506e+00 ... -6.15245628e+00\n",
      "     3.33514667e+00 -1.35630560e+00]]]], device=cpu())\n",
      "B          = NDArray([[[[ 4.09100860e-01 -6.49829268e-01  1.13202944e+01 ... -5.56089449e+00\n",
      "    -4.84385538e+00 -1.20775558e-01]\n",
      " ...10714078e+00  2.54448557e+00 -7.73867989e+00 ... -1.13336074e+00\n",
      "    -1.21658230e+00 -4.79330921e+00]]]], device=cpu())\n",
      "self       = <needle.ops.ops_mathematic.Conv object at 0x7fd94f7191d0>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:504: NotImplementedError\n",
      "\u001b[31m\u001b[1m_ test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape9-W_shape9-2-0] _\u001b[0m\n",
      "\n",
      "Z_shape = (3, 16, 16, 2), W_shape = (3, 3, 2, 14), stride = 2, padding = 0\n",
      "backward = True, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mimport\u001b[39;49;00m \u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "W          = needle.Tensor([[[[ -8.0968      -2.5552022    8.703147    -1.4674252    4.5861077\n",
      "     -0.28521433   4.383634    -9.13...727333   0.22582927   9.296732    -8.13161     -0.67411226\n",
      "     -2.9204676    1.675528   -12.187821     5.5746226 ]]]])\n",
      "W_shape    = (3, 3, 2, 14)\n",
      "Z          = needle.Tensor([[[[  8.820262     2.000786  ]\n",
      "   [  4.89369     11.204466  ]\n",
      "   [  9.33779     -4.8863893 ]\n",
      "   ...\n",
      "   [...   -5.112822  ]\n",
      "   ...\n",
      "   [ -7.922368     4.2222714 ]\n",
      "   [ -6.064339     1.4188478 ]\n",
      "   [ -1.4109794   -5.791016  ]]]])\n",
      "Z_shape    = (3, 16, 16, 2)\n",
      "_W         = array([[[[ -8.0968    ,  -2.5552022 ,   8.703147  ,  -1.4674252 ,\n",
      "            4.5861077 ,  -0.28521433,   4.383634  , ...        -8.13161   ,  -0.67411226,  -2.9204676 ,   1.675528  ,\n",
      "          -12.187821  ,   5.5746226 ]]]], dtype=float32)\n",
      "_Z         = array([[[[  8.820262  ,   2.000786  ],\n",
      "         [  4.89369   ,  11.204466  ],\n",
      "         [  9.33779   ,  -4.8863893 ],\n",
      " ...22368  ,   4.2222714 ],\n",
      "         [ -6.064339  ,   1.4188478 ],\n",
      "         [ -1.4109794 ,  -5.791016  ]]]], dtype=float32)\n",
      "backward   = True\n",
      "device     = cpu()\n",
      "padding    = 0\n",
      "stride     = 2\n",
      "torch      = <module 'torch' from '/opt/conda/envs/train/lib/python3.11/site-packages/torch/__init__.py'>\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:429: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:514: in conv\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Conv(stride, padding)(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[[  8.820262     2.000786  ]\n",
      "   [  4.89369     11.204466  ]\n",
      "   [  9.33779     -4.8863893 ]\n",
      "   ...\n",
      "   [...   -5.112822  ]\n",
      "   ...\n",
      "   [ -7.922368     4.2222714 ]\n",
      "   [ -6.064339     1.4188478 ]\n",
      "   [ -1.4109794   -5.791016  ]]]])\n",
      "        b          = needle.Tensor([[[[ -8.0968      -2.5552022    8.703147    -1.4674252    4.5861077\n",
      "     -0.28521433   4.383634    -9.13...727333   0.22582927   9.296732    -8.13161     -0.67411226\n",
      "     -2.9204676    1.675528   -12.187821     5.5746226 ]]]])\n",
      "        padding    = 0\n",
      "        stride     = 2\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[  8.820262     2.000786  ]\n",
      "   [  4.89369     11.204466  ]\n",
      "   [  9.33779     -4.8863893 ]\n",
      "   ...\n",
      "   ...27333   0.22582927   9.296732    -8.13161     -0.67411226\n",
      "     -2.9204676    1.675528   -12.187821     5.5746226 ]]]]))\n",
      "        self       = <needle.ops.ops_mathematic.Conv object at 0x7fd94f6f1cd0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[[  8.820262     2.000786  ]\n",
      "   [  4.89369     11.204466  ]\n",
      "   [  9.33779     -4.8863893 ]\n",
      "   ...\n",
      "   ...27333   0.22582927   9.296732    -8.13161     -0.67411226\n",
      "     -2.9204676    1.675528   -12.187821     5.5746226 ]]]]))\n",
      "        op         = <needle.ops.ops_mathematic.Conv object at 0x7fd94f6f1cd0>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fd94f6f3590>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fd94f6f3590>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Conv object at 0x7fd94f6f1cd0>\n",
      "A = NDArray([[[[  8.820262     2.000786  ]\n",
      "   [  4.89369     11.204466  ]\n",
      "   [  9.33779     -4.8863893 ]\n",
      "   ...\n",
      "   [  0.22...]\n",
      "   ...\n",
      "   [ -7.922368     4.2222714 ]\n",
      "   [ -6.064339     1.4188478 ]\n",
      "   [ -1.4109794   -5.791016  ]]]], device=cpu())\n",
      "B = NDArray([[[[ -8.0968      -2.5552022    8.703147    -1.4674252    4.5861077\n",
      "     -0.28521433   4.383634    -9.134557  ...82927   9.296732    -8.13161     -0.67411226\n",
      "     -2.9204676    1.675528   -12.187821     5.5746226 ]]]], device=cpu())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, A, B):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "A          = NDArray([[[[  8.820262     2.000786  ]\n",
      "   [  4.89369     11.204466  ]\n",
      "   [  9.33779     -4.8863893 ]\n",
      "   ...\n",
      "   [  0.22...]\n",
      "   ...\n",
      "   [ -7.922368     4.2222714 ]\n",
      "   [ -6.064339     1.4188478 ]\n",
      "   [ -1.4109794   -5.791016  ]]]], device=cpu())\n",
      "B          = NDArray([[[[ -8.0968      -2.5552022    8.703147    -1.4674252    4.5861077\n",
      "     -0.28521433   4.383634    -9.134557  ...82927   9.296732    -8.13161     -0.67411226\n",
      "     -2.9204676    1.675528   -12.187821     5.5746226 ]]]], device=cpu())\n",
      "self       = <needle.ops.ops_mathematic.Conv object at 0x7fd94f6f1cd0>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:504: NotImplementedError\n",
      "\u001b[31m\u001b[1m_ test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape10-W_shape10-1-0] _\u001b[0m\n",
      "\n",
      "Z_shape = (3, 16, 16, 24), W_shape = (3, 3, 24, 14), stride = 1, padding = 0\n",
      "backward = True, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mimport\u001b[39;49;00m \u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "W          = needle.Tensor([[[[ -1.3245817    1.0111231   -1.9236585  ...  -2.5907016\n",
      "     -0.36175704   4.6234684 ]\n",
      "   [  5.279258... -4.82095      4.98517   ]\n",
      "   [  3.2300415   -2.4161792   -3.420825   ...  -9.444658\n",
      "      8.537833     2.3818388 ]]]])\n",
      "W_shape    = (3, 3, 24, 14)\n",
      "Z          = needle.Tensor([[[[  8.820262     2.000786     4.89369    ...   3.2680929\n",
      "      4.322181    -3.7108252 ]\n",
      "   [ 11.348773...4.207389     8.935435  ]\n",
      "   [  2.5084004    3.1399443   -0.12958507 ...   0.13740699\n",
      "     -1.400853     0.07475674]]]])\n",
      "Z_shape    = (3, 16, 16, 24)\n",
      "_W         = array([[[[ -1.3245817 ,   1.0111231 ,  -1.9236585 , ...,  -2.5907016 ,\n",
      "           -0.36175704,   4.6234684 ],\n",
      "        ... [  3.2300415 ,  -2.4161792 ,  -3.420825  , ...,  -9.444658  ,\n",
      "            8.537833  ,   2.3818388 ]]]], dtype=float32)\n",
      "_Z         = array([[[[  8.820262  ,   2.000786  ,   4.89369   , ...,   3.2680929 ,\n",
      "            4.322181  ,  -3.7108252 ],\n",
      "        ... [  2.5084004 ,   3.1399443 ,  -0.12958507, ...,   0.13740699,\n",
      "           -1.400853  ,   0.07475674]]]], dtype=float32)\n",
      "backward   = True\n",
      "device     = cpu()\n",
      "padding    = 0\n",
      "stride     = 1\n",
      "torch      = <module 'torch' from '/opt/conda/envs/train/lib/python3.11/site-packages/torch/__init__.py'>\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:429: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:514: in conv\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Conv(stride, padding)(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[[  8.820262     2.000786     4.89369    ...   3.2680929\n",
      "      4.322181    -3.7108252 ]\n",
      "   [ 11.348773...4.207389     8.935435  ]\n",
      "   [  2.5084004    3.1399443   -0.12958507 ...   0.13740699\n",
      "     -1.400853     0.07475674]]]])\n",
      "        b          = needle.Tensor([[[[ -1.3245817    1.0111231   -1.9236585  ...  -2.5907016\n",
      "     -0.36175704   4.6234684 ]\n",
      "   [  5.279258... -4.82095      4.98517   ]\n",
      "   [  3.2300415   -2.4161792   -3.420825   ...  -9.444658\n",
      "      8.537833     2.3818388 ]]]])\n",
      "        padding    = 0\n",
      "        stride     = 1\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[  8.820262     2.000786     4.89369    ...   3.2680929\n",
      "      4.322181    -3.7108252 ]\n",
      "   [ 11.34877...-4.82095      4.98517   ]\n",
      "   [  3.2300415   -2.4161792   -3.420825   ...  -9.444658\n",
      "      8.537833     2.3818388 ]]]]))\n",
      "        self       = <needle.ops.ops_mathematic.Conv object at 0x7fd94f65da50>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[[  8.820262     2.000786     4.89369    ...   3.2680929\n",
      "      4.322181    -3.7108252 ]\n",
      "   [ 11.34877...-4.82095      4.98517   ]\n",
      "   [  3.2300415   -2.4161792   -3.420825   ...  -9.444658\n",
      "      8.537833     2.3818388 ]]]]))\n",
      "        op         = <needle.ops.ops_mathematic.Conv object at 0x7fd94f65da50>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fd94f65d7d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fd94f65d7d0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Conv object at 0x7fd94f65da50>\n",
      "A = NDArray([[[[  8.820262     2.000786     4.89369    ...   3.2680929\n",
      "      4.322181    -3.7108252 ]\n",
      "   [ 11.348773    -7....935435  ]\n",
      "   [  2.5084004    3.1399443   -0.12958507 ...   0.13740699\n",
      "     -1.400853     0.07475674]]]], device=cpu())\n",
      "B = NDArray([[[[ -1.3245817    1.0111231   -1.9236585  ...  -2.5907016\n",
      "     -0.36175704   4.6234684 ]\n",
      "   [  5.2792587   -1... 4.98517   ]\n",
      "   [  3.2300415   -2.4161792   -3.420825   ...  -9.444658\n",
      "      8.537833     2.3818388 ]]]], device=cpu())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, A, B):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "A          = NDArray([[[[  8.820262     2.000786     4.89369    ...   3.2680929\n",
      "      4.322181    -3.7108252 ]\n",
      "   [ 11.348773    -7....935435  ]\n",
      "   [  2.5084004    3.1399443   -0.12958507 ...   0.13740699\n",
      "     -1.400853     0.07475674]]]], device=cpu())\n",
      "B          = NDArray([[[[ -1.3245817    1.0111231   -1.9236585  ...  -2.5907016\n",
      "     -0.36175704   4.6234684 ]\n",
      "   [  5.2792587   -1... 4.98517   ]\n",
      "   [  3.2300415   -2.4161792   -3.420825   ...  -9.444658\n",
      "      8.537833     2.3818388 ]]]], device=cpu())\n",
      "self       = <needle.ops.ops_mathematic.Conv object at 0x7fd94f65da50>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:504: NotImplementedError\n",
      "\u001b[31m\u001b[1m_ test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape11-W_shape11-1-0] _\u001b[0m\n",
      "\n",
      "Z_shape = (3, 14, 14, 8), W_shape = (5, 5, 8, 16), stride = 1, padding = 0\n",
      "backward = True, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mimport\u001b[39;49;00m \u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "W          = needle.Tensor([[[[ 4.28266907e+00  8.13360405e+00 -4.62234831e+00 ...  2.07981968e+00\n",
      "     2.17041516e+00 -3.88039827e...7e+00]\n",
      "   [-7.86027253e-01  4.30947495e+00 -4.97804356e+00 ... -5.69430470e-01\n",
      "     4.27775383e-01 -3.84437203e+00]]]])\n",
      "W_shape    = (5, 5, 8, 16)\n",
      "Z          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e...3e+00]\n",
      "   [ 1.09905624e+01  2.23166728e+00  4.62055349e+00 ... -6.23090088e-01\n",
      "    -8.48226726e-01 -2.29984760e+00]]]])\n",
      "Z_shape    = (3, 14, 14, 8)\n",
      "_W         = array([[[[ 4.28266907e+00,  8.13360405e+00, -4.62234831e+00, ...,\n",
      "           2.07981968e+00,  2.17041516e+00, -3.88039...947495e+00, -4.97804356e+00, ...,\n",
      "          -5.69430470e-01,  4.27775383e-01, -3.84437203e+00]]]],\n",
      "      dtype=float32)\n",
      "_Z         = array([[[[ 8.82026196e+00,  2.00078607e+00,  4.89369011e+00, ...,\n",
      "          -4.88638926e+00,  4.75044203e+00, -7.56786...166728e+00,  4.62055349e+00, ...,\n",
      "          -6.23090088e-01, -8.48226726e-01, -2.29984760e+00]]]],\n",
      "      dtype=float32)\n",
      "backward   = True\n",
      "device     = cpu()\n",
      "padding    = 0\n",
      "stride     = 1\n",
      "torch      = <module 'torch' from '/opt/conda/envs/train/lib/python3.11/site-packages/torch/__init__.py'>\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:429: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:514: in conv\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Conv(stride, padding)(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e...3e+00]\n",
      "   [ 1.09905624e+01  2.23166728e+00  4.62055349e+00 ... -6.23090088e-01\n",
      "    -8.48226726e-01 -2.29984760e+00]]]])\n",
      "        b          = needle.Tensor([[[[ 4.28266907e+00  8.13360405e+00 -4.62234831e+00 ...  2.07981968e+00\n",
      "     2.17041516e+00 -3.88039827e...7e+00]\n",
      "   [-7.86027253e-01  4.30947495e+00 -4.97804356e+00 ... -5.69430470e-01\n",
      "     4.27775383e-01 -3.84437203e+00]]]])\n",
      "        padding    = 0\n",
      "        stride     = 1\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048...e+00]\n",
      "   [-7.86027253e-01  4.30947495e+00 -4.97804356e+00 ... -5.69430470e-01\n",
      "     4.27775383e-01 -3.84437203e+00]]]]))\n",
      "        self       = <needle.ops.ops_mathematic.Conv object at 0x7fd94f76db10>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048...e+00]\n",
      "   [-7.86027253e-01  4.30947495e+00 -4.97804356e+00 ... -5.69430470e-01\n",
      "     4.27775383e-01 -3.84437203e+00]]]]))\n",
      "        op         = <needle.ops.ops_mathematic.Conv object at 0x7fd94f76db10>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fd94f76ec90>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fd94f76ec90>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Conv object at 0x7fd94f76db10>\n",
      "A = NDArray([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e-01]\n",
      " ...09905624e+01  2.23166728e+00  4.62055349e+00 ... -6.23090088e-01\n",
      "    -8.48226726e-01 -2.29984760e+00]]]], device=cpu())\n",
      "B = NDArray([[[[ 4.28266907e+00  8.13360405e+00 -4.62234831e+00 ...  2.07981968e+00\n",
      "     2.17041516e+00 -3.88039827e-01]\n",
      " ...86027253e-01  4.30947495e+00 -4.97804356e+00 ... -5.69430470e-01\n",
      "     4.27775383e-01 -3.84437203e+00]]]], device=cpu())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, A, B):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "A          = NDArray([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e-01]\n",
      " ...09905624e+01  2.23166728e+00  4.62055349e+00 ... -6.23090088e-01\n",
      "    -8.48226726e-01 -2.29984760e+00]]]], device=cpu())\n",
      "B          = NDArray([[[[ 4.28266907e+00  8.13360405e+00 -4.62234831e+00 ...  2.07981968e+00\n",
      "     2.17041516e+00 -3.88039827e-01]\n",
      " ...86027253e-01  4.30947495e+00 -4.97804356e+00 ... -5.69430470e-01\n",
      "     4.27775383e-01 -3.84437203e+00]]]], device=cpu())\n",
      "self       = <needle.ops.ops_mathematic.Conv object at 0x7fd94f76db10>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:504: NotImplementedError\n",
      "\u001b[31m\u001b[1m_ test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape12-W_shape12-1-0] _\u001b[0m\n",
      "\n",
      "Z_shape = (3, 17, 17, 8), W_shape = (5, 5, 8, 16), stride = 1, padding = 0\n",
      "backward = True, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mimport\u001b[39;49;00m \u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "W          = needle.Tensor([[[[ 5.93117094e+00 -4.61600447e+00 -7.43411541e+00 ...  9.49053860e+00\n",
      "     6.47611380e+00 -3.38525438e...7e+00]\n",
      "   [-3.93575430e+00 -6.98609781e+00  1.63659811e+00 ...  8.22474575e+00\n",
      "     2.56555057e+00  1.63004756e+00]]]])\n",
      "W_shape    = (5, 5, 8, 16)\n",
      "Z          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e...8e+00]\n",
      "   [ 2.80155873e+00  3.11298299e+00 -3.24756050e+00 ... -7.96233535e-01\n",
      "     4.72636795e+00 -5.75466204e+00]]]])\n",
      "Z_shape    = (3, 17, 17, 8)\n",
      "_W         = array([[[[ 5.93117094e+00, -4.61600447e+00, -7.43411541e+00, ...,\n",
      "           9.49053860e+00,  6.47611380e+00, -3.38525...609781e+00,  1.63659811e+00, ...,\n",
      "           8.22474575e+00,  2.56555057e+00,  1.63004756e+00]]]],\n",
      "      dtype=float32)\n",
      "_Z         = array([[[[ 8.82026196e+00,  2.00078607e+00,  4.89369011e+00, ...,\n",
      "          -4.88638926e+00,  4.75044203e+00, -7.56786...298299e+00, -3.24756050e+00, ...,\n",
      "          -7.96233535e-01,  4.72636795e+00, -5.75466204e+00]]]],\n",
      "      dtype=float32)\n",
      "backward   = True\n",
      "device     = cpu()\n",
      "padding    = 0\n",
      "stride     = 1\n",
      "torch      = <module 'torch' from '/opt/conda/envs/train/lib/python3.11/site-packages/torch/__init__.py'>\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:429: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:514: in conv\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Conv(stride, padding)(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e...8e+00]\n",
      "   [ 2.80155873e+00  3.11298299e+00 -3.24756050e+00 ... -7.96233535e-01\n",
      "     4.72636795e+00 -5.75466204e+00]]]])\n",
      "        b          = needle.Tensor([[[[ 5.93117094e+00 -4.61600447e+00 -7.43411541e+00 ...  9.49053860e+00\n",
      "     6.47611380e+00 -3.38525438e...7e+00]\n",
      "   [-3.93575430e+00 -6.98609781e+00  1.63659811e+00 ...  8.22474575e+00\n",
      "     2.56555057e+00  1.63004756e+00]]]])\n",
      "        padding    = 0\n",
      "        stride     = 1\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048...e+00]\n",
      "   [-3.93575430e+00 -6.98609781e+00  1.63659811e+00 ...  8.22474575e+00\n",
      "     2.56555057e+00  1.63004756e+00]]]]))\n",
      "        self       = <needle.ops.ops_mathematic.Conv object at 0x7fd94f6f2c50>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048...e+00]\n",
      "   [-3.93575430e+00 -6.98609781e+00  1.63659811e+00 ...  8.22474575e+00\n",
      "     2.56555057e+00  1.63004756e+00]]]]))\n",
      "        op         = <needle.ops.ops_mathematic.Conv object at 0x7fd94f6f2c50>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fd94f6f13d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fd94f6f13d0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Conv object at 0x7fd94f6f2c50>\n",
      "A = NDArray([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e-01]\n",
      " ...80155873e+00  3.11298299e+00 -3.24756050e+00 ... -7.96233535e-01\n",
      "     4.72636795e+00 -5.75466204e+00]]]], device=cpu())\n",
      "B = NDArray([[[[ 5.93117094e+00 -4.61600447e+00 -7.43411541e+00 ...  9.49053860e+00\n",
      "     6.47611380e+00 -3.38525438e+00]\n",
      " ...93575430e+00 -6.98609781e+00  1.63659811e+00 ...  8.22474575e+00\n",
      "     2.56555057e+00  1.63004756e+00]]]], device=cpu())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, A, B):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "A          = NDArray([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e-01]\n",
      " ...80155873e+00  3.11298299e+00 -3.24756050e+00 ... -7.96233535e-01\n",
      "     4.72636795e+00 -5.75466204e+00]]]], device=cpu())\n",
      "B          = NDArray([[[[ 5.93117094e+00 -4.61600447e+00 -7.43411541e+00 ...  9.49053860e+00\n",
      "     6.47611380e+00 -3.38525438e+00]\n",
      " ...93575430e+00 -6.98609781e+00  1.63659811e+00 ...  8.22474575e+00\n",
      "     2.56555057e+00  1.63004756e+00]]]], device=cpu())\n",
      "self       = <needle.ops.ops_mathematic.Conv object at 0x7fd94f6f2c50>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:504: NotImplementedError\n",
      "\u001b[31m\u001b[1m_ test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape13-W_shape13-1-0] _\u001b[0m\n",
      "\n",
      "Z_shape = (3, 17, 17, 1), W_shape = (5, 5, 1, 16), stride = 1, padding = 0\n",
      "backward = True, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mimport\u001b[39;49;00m \u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "W          = needle.Tensor([[[[  1.5437562   -6.8538       4.3282647    5.4068804   -3.15688\n",
      "     -1.206689    -4.3909516    3.4969...448   -2.242325     0.6578698\n",
      "     -7.0278      -1.7489109   10.11736      2.5269346    1.7962458\n",
      "     -7.9124722 ]]]])\n",
      "W_shape    = (5, 5, 1, 16)\n",
      "Z          = needle.Tensor([[[[ 8.82026196e+00]\n",
      "   [ 2.00078607e+00]\n",
      "   [ 4.89369011e+00]\n",
      "   [ 1.12044659e+01]\n",
      "   [ 9.33778954e+00]...22066e-01]\n",
      "   [ 1.05310105e-01]\n",
      "   [ 4.97272283e-01]\n",
      "   [ 1.13696384e+00]\n",
      "   [-5.08369303e+00]\n",
      "   [-5.73876619e-01]]]])\n",
      "Z_shape    = (3, 17, 17, 1)\n",
      "_W         = array([[[[  1.5437562 ,  -6.8538    ,   4.3282647 ,   5.4068804 ,\n",
      "           -3.15688   ,  -1.206689  ,  -4.3909516 , ...  -7.0278    ,  -1.7489109 ,\n",
      "           10.11736   ,   2.5269346 ,   1.7962458 ,  -7.9124722 ]]]],\n",
      "      dtype=float32)\n",
      "_Z         = array([[[[ 8.82026196e+00],\n",
      "         [ 2.00078607e+00],\n",
      "         [ 4.89369011e+00],\n",
      "         [ 1.12044659e+01],\n",
      "      ... 4.97272283e-01],\n",
      "         [ 1.13696384e+00],\n",
      "         [-5.08369303e+00],\n",
      "         [-5.73876619e-01]]]], dtype=float32)\n",
      "backward   = True\n",
      "device     = cpu()\n",
      "padding    = 0\n",
      "stride     = 1\n",
      "torch      = <module 'torch' from '/opt/conda/envs/train/lib/python3.11/site-packages/torch/__init__.py'>\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:429: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:514: in conv\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Conv(stride, padding)(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[[ 8.82026196e+00]\n",
      "   [ 2.00078607e+00]\n",
      "   [ 4.89369011e+00]\n",
      "   [ 1.12044659e+01]\n",
      "   [ 9.33778954e+00]...22066e-01]\n",
      "   [ 1.05310105e-01]\n",
      "   [ 4.97272283e-01]\n",
      "   [ 1.13696384e+00]\n",
      "   [-5.08369303e+00]\n",
      "   [-5.73876619e-01]]]])\n",
      "        b          = needle.Tensor([[[[  1.5437562   -6.8538       4.3282647    5.4068804   -3.15688\n",
      "     -1.206689    -4.3909516    3.4969...448   -2.242325     0.6578698\n",
      "     -7.0278      -1.7489109   10.11736      2.5269346    1.7962458\n",
      "     -7.9124722 ]]]])\n",
      "        padding    = 0\n",
      "        stride     = 1\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[ 8.82026196e+00]\n",
      "   [ 2.00078607e+00]\n",
      "   [ 4.89369011e+00]\n",
      "   [ 1.12044659e+01]\n",
      "   [ 9.33778954e+00...48   -2.242325     0.6578698\n",
      "     -7.0278      -1.7489109   10.11736      2.5269346    1.7962458\n",
      "     -7.9124722 ]]]]))\n",
      "        self       = <needle.ops.ops_mathematic.Conv object at 0x7fd94f575290>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[[ 8.82026196e+00]\n",
      "   [ 2.00078607e+00]\n",
      "   [ 4.89369011e+00]\n",
      "   [ 1.12044659e+01]\n",
      "   [ 9.33778954e+00...48   -2.242325     0.6578698\n",
      "     -7.0278      -1.7489109   10.11736      2.5269346    1.7962458\n",
      "     -7.9124722 ]]]]))\n",
      "        op         = <needle.ops.ops_mathematic.Conv object at 0x7fd94f575290>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fd94f574310>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fd94f574310>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Conv object at 0x7fd94f575290>\n",
      "A = NDArray([[[[ 8.82026196e+00]\n",
      "   [ 2.00078607e+00]\n",
      "   [ 4.89369011e+00]\n",
      "   [ 1.12044659e+01]\n",
      "   [ 9.33778954e+00]\n",
      "   [-...[ 1.05310105e-01]\n",
      "   [ 4.97272283e-01]\n",
      "   [ 1.13696384e+00]\n",
      "   [-5.08369303e+00]\n",
      "   [-5.73876619e-01]]]], device=cpu())\n",
      "B = NDArray([[[[  1.5437562   -6.8538       4.3282647    5.4068804   -3.15688\n",
      "     -1.206689    -4.3909516    3.4969025   ...5     0.6578698\n",
      "     -7.0278      -1.7489109   10.11736      2.5269346    1.7962458\n",
      "     -7.9124722 ]]]], device=cpu())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, A, B):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "A          = NDArray([[[[ 8.82026196e+00]\n",
      "   [ 2.00078607e+00]\n",
      "   [ 4.89369011e+00]\n",
      "   [ 1.12044659e+01]\n",
      "   [ 9.33778954e+00]\n",
      "   [-...[ 1.05310105e-01]\n",
      "   [ 4.97272283e-01]\n",
      "   [ 1.13696384e+00]\n",
      "   [-5.08369303e+00]\n",
      "   [-5.73876619e-01]]]], device=cpu())\n",
      "B          = NDArray([[[[  1.5437562   -6.8538       4.3282647    5.4068804   -3.15688\n",
      "     -1.206689    -4.3909516    3.4969025   ...5     0.6578698\n",
      "     -7.0278      -1.7489109   10.11736      2.5269346    1.7962458\n",
      "     -7.9124722 ]]]], device=cpu())\n",
      "self       = <needle.ops.ops_mathematic.Conv object at 0x7fd94f575290>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:504: NotImplementedError\n",
      "\u001b[31m\u001b[1m_ test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape14-W_shape14-1-0] _\u001b[0m\n",
      "\n",
      "Z_shape = (3, 17, 17, 16), W_shape = (5, 5, 16, 1), stride = 1, padding = 0\n",
      "backward = True, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mimport\u001b[39;49;00m \u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "W          = needle.Tensor([[[[ 1.55579513e-02]\n",
      "   [-3.04658723e+00]\n",
      "   [ 4.09725952e+00]\n",
      "   [ 1.29906273e+00]\n",
      "   [-7.50546598e+00]...75421e+00]\n",
      "   [ 3.02805209e+00]\n",
      "   [-4.20992136e+00]\n",
      "   [ 4.11707735e+00]\n",
      "   [ 4.99041653e+00]\n",
      "   [ 5.11040497e+00]]]])\n",
      "W_shape    = (5, 5, 16, 1)\n",
      "Z          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ...  6.08375072e-01\n",
      "     2.21931624e+00  1.66837168e...3e+00]\n",
      "   [-1.25365603e+00 -1.71452928e+00  1.35930243e+01 ...  3.13170171e+00\n",
      "    -1.35708165e+00  1.29997072e+01]]]])\n",
      "Z_shape    = (3, 17, 17, 16)\n",
      "_W         = array([[[[ 1.55579513e-02],\n",
      "         [-3.04658723e+00],\n",
      "         [ 4.09725952e+00],\n",
      "         [ 1.29906273e+00],\n",
      "      ...-4.20992136e+00],\n",
      "         [ 4.11707735e+00],\n",
      "         [ 4.99041653e+00],\n",
      "         [ 5.11040497e+00]]]], dtype=float32)\n",
      "_Z         = array([[[[ 8.82026196e+00,  2.00078607e+00,  4.89369011e+00, ...,\n",
      "           6.08375072e-01,  2.21931624e+00,  1.66837...452928e+00,  1.35930243e+01, ...,\n",
      "           3.13170171e+00, -1.35708165e+00,  1.29997072e+01]]]],\n",
      "      dtype=float32)\n",
      "backward   = True\n",
      "device     = cpu()\n",
      "padding    = 0\n",
      "stride     = 1\n",
      "torch      = <module 'torch' from '/opt/conda/envs/train/lib/python3.11/site-packages/torch/__init__.py'>\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:429: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:514: in conv\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Conv(stride, padding)(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ...  6.08375072e-01\n",
      "     2.21931624e+00  1.66837168e...3e+00]\n",
      "   [-1.25365603e+00 -1.71452928e+00  1.35930243e+01 ...  3.13170171e+00\n",
      "    -1.35708165e+00  1.29997072e+01]]]])\n",
      "        b          = needle.Tensor([[[[ 1.55579513e-02]\n",
      "   [-3.04658723e+00]\n",
      "   [ 4.09725952e+00]\n",
      "   [ 1.29906273e+00]\n",
      "   [-7.50546598e+00]...75421e+00]\n",
      "   [ 3.02805209e+00]\n",
      "   [-4.20992136e+00]\n",
      "   [ 4.11707735e+00]\n",
      "   [ 4.99041653e+00]\n",
      "   [ 5.11040497e+00]]]])\n",
      "        padding    = 0\n",
      "        stride     = 1\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ...  6.08375072e-01\n",
      "     2.21931624e+00  1.66837168...5421e+00]\n",
      "   [ 3.02805209e+00]\n",
      "   [-4.20992136e+00]\n",
      "   [ 4.11707735e+00]\n",
      "   [ 4.99041653e+00]\n",
      "   [ 5.11040497e+00]]]]))\n",
      "        self       = <needle.ops.ops_mathematic.Conv object at 0x7fd94f7052d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ...  6.08375072e-01\n",
      "     2.21931624e+00  1.66837168...5421e+00]\n",
      "   [ 3.02805209e+00]\n",
      "   [-4.20992136e+00]\n",
      "   [ 4.11707735e+00]\n",
      "   [ 4.99041653e+00]\n",
      "   [ 5.11040497e+00]]]]))\n",
      "        op         = <needle.ops.ops_mathematic.Conv object at 0x7fd94f7052d0>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fd94f705390>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fd94f705390>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Conv object at 0x7fd94f7052d0>\n",
      "A = NDArray([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ...  6.08375072e-01\n",
      "     2.21931624e+00  1.66837168e+00]\n",
      " ...25365603e+00 -1.71452928e+00  1.35930243e+01 ...  3.13170171e+00\n",
      "    -1.35708165e+00  1.29997072e+01]]]], device=cpu())\n",
      "B = NDArray([[[[ 1.55579513e-02]\n",
      "   [-3.04658723e+00]\n",
      "   [ 4.09725952e+00]\n",
      "   [ 1.29906273e+00]\n",
      "   [-7.50546598e+00]\n",
      "   [-...[ 3.02805209e+00]\n",
      "   [-4.20992136e+00]\n",
      "   [ 4.11707735e+00]\n",
      "   [ 4.99041653e+00]\n",
      "   [ 5.11040497e+00]]]], device=cpu())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, A, B):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "A          = NDArray([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ...  6.08375072e-01\n",
      "     2.21931624e+00  1.66837168e+00]\n",
      " ...25365603e+00 -1.71452928e+00  1.35930243e+01 ...  3.13170171e+00\n",
      "    -1.35708165e+00  1.29997072e+01]]]], device=cpu())\n",
      "B          = NDArray([[[[ 1.55579513e-02]\n",
      "   [-3.04658723e+00]\n",
      "   [ 4.09725952e+00]\n",
      "   [ 1.29906273e+00]\n",
      "   [-7.50546598e+00]\n",
      "   [-...[ 3.02805209e+00]\n",
      "   [-4.20992136e+00]\n",
      "   [ 4.11707735e+00]\n",
      "   [ 4.99041653e+00]\n",
      "   [ 5.11040497e+00]]]], device=cpu())\n",
      "self       = <needle.ops.ops_mathematic.Conv object at 0x7fd94f7052d0>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:504: NotImplementedError\n",
      "\u001b[31m\u001b[1m_ test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape15-W_shape15-1-0] _\u001b[0m\n",
      "\n",
      "Z_shape = (3, 17, 17, 16), W_shape = (1, 1, 16, 1), stride = 1, padding = 0\n",
      "backward = True, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mimport\u001b[39;49;00m \u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "W          = needle.Tensor([[[[ 0.01555795]\n",
      "   [-3.0465872 ]\n",
      "   [ 4.0972595 ]\n",
      "   [ 1.2990627 ]\n",
      "   [-7.505466  ]\n",
      "   [-1.7335238 ]\n",
      "  ...[ 5.6893935 ]\n",
      "   [ 2.267663  ]\n",
      "   [-1.0709513 ]\n",
      "   [-5.0566583 ]\n",
      "   [-0.23777105]\n",
      "   [-9.931785  ]\n",
      "   [-0.44905776]]]])\n",
      "W_shape    = (1, 1, 16, 1)\n",
      "Z          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ...  6.08375072e-01\n",
      "     2.21931624e+00  1.66837168e...3e+00]\n",
      "   [-1.25365603e+00 -1.71452928e+00  1.35930243e+01 ...  3.13170171e+00\n",
      "    -1.35708165e+00  1.29997072e+01]]]])\n",
      "Z_shape    = (3, 17, 17, 16)\n",
      "_W         = array([[[[ 0.01555795],\n",
      "         [-3.0465872 ],\n",
      "         [ 4.0972595 ],\n",
      "         [ 1.2990627 ],\n",
      "         [-7.505466  ]...13 ],\n",
      "         [-5.0566583 ],\n",
      "         [-0.23777105],\n",
      "         [-9.931785  ],\n",
      "         [-0.44905776]]]], dtype=float32)\n",
      "_Z         = array([[[[ 8.82026196e+00,  2.00078607e+00,  4.89369011e+00, ...,\n",
      "           6.08375072e-01,  2.21931624e+00,  1.66837...452928e+00,  1.35930243e+01, ...,\n",
      "           3.13170171e+00, -1.35708165e+00,  1.29997072e+01]]]],\n",
      "      dtype=float32)\n",
      "backward   = True\n",
      "device     = cpu()\n",
      "padding    = 0\n",
      "stride     = 1\n",
      "torch      = <module 'torch' from '/opt/conda/envs/train/lib/python3.11/site-packages/torch/__init__.py'>\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:429: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:514: in conv\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Conv(stride, padding)(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ...  6.08375072e-01\n",
      "     2.21931624e+00  1.66837168e...3e+00]\n",
      "   [-1.25365603e+00 -1.71452928e+00  1.35930243e+01 ...  3.13170171e+00\n",
      "    -1.35708165e+00  1.29997072e+01]]]])\n",
      "        b          = needle.Tensor([[[[ 0.01555795]\n",
      "   [-3.0465872 ]\n",
      "   [ 4.0972595 ]\n",
      "   [ 1.2990627 ]\n",
      "   [-7.505466  ]\n",
      "   [-1.7335238 ]\n",
      "  ...[ 5.6893935 ]\n",
      "   [ 2.267663  ]\n",
      "   [-1.0709513 ]\n",
      "   [-5.0566583 ]\n",
      "   [-0.23777105]\n",
      "   [-9.931785  ]\n",
      "   [-0.44905776]]]])\n",
      "        padding    = 0\n",
      "        stride     = 1\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ...  6.08375072e-01\n",
      "     2.21931624e+00  1.66837168... 5.6893935 ]\n",
      "   [ 2.267663  ]\n",
      "   [-1.0709513 ]\n",
      "   [-5.0566583 ]\n",
      "   [-0.23777105]\n",
      "   [-9.931785  ]\n",
      "   [-0.44905776]]]]))\n",
      "        self       = <needle.ops.ops_mathematic.Conv object at 0x7fd94f6bc310>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ...  6.08375072e-01\n",
      "     2.21931624e+00  1.66837168... 5.6893935 ]\n",
      "   [ 2.267663  ]\n",
      "   [-1.0709513 ]\n",
      "   [-5.0566583 ]\n",
      "   [-0.23777105]\n",
      "   [-9.931785  ]\n",
      "   [-0.44905776]]]]))\n",
      "        op         = <needle.ops.ops_mathematic.Conv object at 0x7fd94f6bc310>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fd94f6bd210>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fd94f6bd210>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Conv object at 0x7fd94f6bc310>\n",
      "A = NDArray([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ...  6.08375072e-01\n",
      "     2.21931624e+00  1.66837168e+00]\n",
      " ...25365603e+00 -1.71452928e+00  1.35930243e+01 ...  3.13170171e+00\n",
      "    -1.35708165e+00  1.29997072e+01]]]], device=cpu())\n",
      "B = NDArray([[[[ 0.01555795]\n",
      "   [-3.0465872 ]\n",
      "   [ 4.0972595 ]\n",
      "   [ 1.2990627 ]\n",
      "   [-7.505466  ]\n",
      "   [-1.7335238 ]\n",
      "   [-2.5...   [ 2.267663  ]\n",
      "   [-1.0709513 ]\n",
      "   [-5.0566583 ]\n",
      "   [-0.23777105]\n",
      "   [-9.931785  ]\n",
      "   [-0.44905776]]]], device=cpu())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, A, B):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "A          = NDArray([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ...  6.08375072e-01\n",
      "     2.21931624e+00  1.66837168e+00]\n",
      " ...25365603e+00 -1.71452928e+00  1.35930243e+01 ...  3.13170171e+00\n",
      "    -1.35708165e+00  1.29997072e+01]]]], device=cpu())\n",
      "B          = NDArray([[[[ 0.01555795]\n",
      "   [-3.0465872 ]\n",
      "   [ 4.0972595 ]\n",
      "   [ 1.2990627 ]\n",
      "   [-7.505466  ]\n",
      "   [-1.7335238 ]\n",
      "   [-2.5...   [ 2.267663  ]\n",
      "   [-1.0709513 ]\n",
      "   [-5.0566583 ]\n",
      "   [-0.23777105]\n",
      "   [-9.931785  ]\n",
      "   [-0.44905776]]]], device=cpu())\n",
      "self       = <needle.ops.ops_mathematic.Conv object at 0x7fd94f6bc310>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:504: NotImplementedError\n",
      "\u001b[31m\u001b[1m_ test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape16-W_shape16-1-0] _\u001b[0m\n",
      "\n",
      "Z_shape = (1, 14, 14, 2), W_shape = (3, 3, 2, 2), stride = 1, padding = 0\n",
      "backward = True, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mimport\u001b[39;49;00m \u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "W          = needle.Tensor([[[[ -1.7671587   -8.082371  ]\n",
      "   [ -1.4591868   -3.807461  ]]\n",
      "\n",
      "  [[  4.2896194    5.705509  ]\n",
      "   [  7.3...7526     2.911123  ]\n",
      "   [-10.473016     0.61860955]]\n",
      "\n",
      "  [[ -0.65053475   0.46976614]\n",
      "   [  4.7152305  -13.698386  ]]]])\n",
      "W_shape    = (3, 3, 2, 2)\n",
      "Z          = needle.Tensor([[[[  8.820262     2.000786  ]\n",
      "   [  4.89369     11.204466  ]\n",
      "   [  9.33779     -4.8863893 ]\n",
      "   [  4.750...19315276  -8.283575  ]\n",
      "   [ -4.9275537   -7.359175  ]\n",
      "   [  8.240675     0.8211388 ]\n",
      "   [  2.8364513   -1.1133755 ]]]])\n",
      "Z_shape    = (1, 14, 14, 2)\n",
      "_W         = array([[[[ -1.7671587 ,  -8.082371  ],\n",
      "         [ -1.4591868 ,  -3.807461  ]],\n",
      "\n",
      "        [[  4.2896194 ,   5.705509  ],...016  ,   0.61860955]],\n",
      "\n",
      "        [[ -0.65053475,   0.46976614],\n",
      "         [  4.7152305 , -13.698386  ]]]], dtype=float32)\n",
      "_Z         = array([[[[  8.820262  ,   2.000786  ],\n",
      "         [  4.89369   ,  11.204466  ],\n",
      "         [  9.33779   ,  -4.8863893 ],\n",
      " ...275537 ,  -7.359175  ],\n",
      "         [  8.240675  ,   0.8211388 ],\n",
      "         [  2.8364513 ,  -1.1133755 ]]]], dtype=float32)\n",
      "backward   = True\n",
      "device     = cpu()\n",
      "padding    = 0\n",
      "stride     = 1\n",
      "torch      = <module 'torch' from '/opt/conda/envs/train/lib/python3.11/site-packages/torch/__init__.py'>\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:429: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:514: in conv\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Conv(stride, padding)(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[[  8.820262     2.000786  ]\n",
      "   [  4.89369     11.204466  ]\n",
      "   [  9.33779     -4.8863893 ]\n",
      "   [  4.750...19315276  -8.283575  ]\n",
      "   [ -4.9275537   -7.359175  ]\n",
      "   [  8.240675     0.8211388 ]\n",
      "   [  2.8364513   -1.1133755 ]]]])\n",
      "        b          = needle.Tensor([[[[ -1.7671587   -8.082371  ]\n",
      "   [ -1.4591868   -3.807461  ]]\n",
      "\n",
      "  [[  4.2896194    5.705509  ]\n",
      "   [  7.3...7526     2.911123  ]\n",
      "   [-10.473016     0.61860955]]\n",
      "\n",
      "  [[ -0.65053475   0.46976614]\n",
      "   [  4.7152305  -13.698386  ]]]])\n",
      "        padding    = 0\n",
      "        stride     = 1\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[  8.820262     2.000786  ]\n",
      "   [  4.89369     11.204466  ]\n",
      "   [  9.33779     -4.8863893 ]\n",
      "   [  4.75...526     2.911123  ]\n",
      "   [-10.473016     0.61860955]]\n",
      "\n",
      "  [[ -0.65053475   0.46976614]\n",
      "   [  4.7152305  -13.698386  ]]]]))\n",
      "        self       = <needle.ops.ops_mathematic.Conv object at 0x7fd94f65c690>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[[  8.820262     2.000786  ]\n",
      "   [  4.89369     11.204466  ]\n",
      "   [  9.33779     -4.8863893 ]\n",
      "   [  4.75...526     2.911123  ]\n",
      "   [-10.473016     0.61860955]]\n",
      "\n",
      "  [[ -0.65053475   0.46976614]\n",
      "   [  4.7152305  -13.698386  ]]]]))\n",
      "        op         = <needle.ops.ops_mathematic.Conv object at 0x7fd94f65c690>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fd94f65d910>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fd94f65d910>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Conv object at 0x7fd94f65c690>\n",
      "A = NDArray([[[[  8.820262     2.000786  ]\n",
      "   [  4.89369     11.204466  ]\n",
      "   [  9.33779     -4.8863893 ]\n",
      "   [  4.750442   ...83575  ]\n",
      "   [ -4.9275537   -7.359175  ]\n",
      "   [  8.240675     0.8211388 ]\n",
      "   [  2.8364513   -1.1133755 ]]]], device=cpu())\n",
      "B = NDArray([[[[ -1.7671587   -8.082371  ]\n",
      "   [ -1.4591868   -3.807461  ]]\n",
      "\n",
      "  [[  4.2896194    5.705509  ]\n",
      "   [  7.3328934...123  ]\n",
      "   [-10.473016     0.61860955]]\n",
      "\n",
      "  [[ -0.65053475   0.46976614]\n",
      "   [  4.7152305  -13.698386  ]]]], device=cpu())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, A, B):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "A          = NDArray([[[[  8.820262     2.000786  ]\n",
      "   [  4.89369     11.204466  ]\n",
      "   [  9.33779     -4.8863893 ]\n",
      "   [  4.750442   ...83575  ]\n",
      "   [ -4.9275537   -7.359175  ]\n",
      "   [  8.240675     0.8211388 ]\n",
      "   [  2.8364513   -1.1133755 ]]]], device=cpu())\n",
      "B          = NDArray([[[[ -1.7671587   -8.082371  ]\n",
      "   [ -1.4591868   -3.807461  ]]\n",
      "\n",
      "  [[  4.2896194    5.705509  ]\n",
      "   [  7.3328934...123  ]\n",
      "   [-10.473016     0.61860955]]\n",
      "\n",
      "  [[ -0.65053475   0.46976614]\n",
      "   [  4.7152305  -13.698386  ]]]], device=cpu())\n",
      "self       = <needle.ops.ops_mathematic.Conv object at 0x7fd94f65c690>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:504: NotImplementedError\n",
      "\u001b[31m\u001b[1m_ test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape0-W_shape0-1-0] _\u001b[0m\n",
      "\n",
      "Z_shape = (3, 14, 14, 8), W_shape = (3, 3, 8, 16), stride = 1, padding = 0\n",
      "backward = True, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mimport\u001b[39;49;00m \u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "W          = needle.Tensor([[[[  4.282669     8.133604    -4.6223483  ...   2.0798197\n",
      "      2.1704152   -0.38803983]\n",
      "   [  2.245844...2.618842     0.71487164]\n",
      "   [ -4.09374      3.7291534  -10.551835   ...   0.31741843\n",
      "      0.03223436   2.0512383 ]]]])\n",
      "W_shape    = (3, 3, 8, 16)\n",
      "Z          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e...3e+00]\n",
      "   [ 1.09905624e+01  2.23166728e+00  4.62055349e+00 ... -6.23090088e-01\n",
      "    -8.48226726e-01 -2.29984760e+00]]]])\n",
      "Z_shape    = (3, 14, 14, 8)\n",
      "_W         = array([[[[  4.282669  ,   8.133604  ,  -4.6223483 , ...,   2.0798197 ,\n",
      "            2.1704152 ,  -0.38803983],\n",
      "        ... [ -4.09374   ,   3.7291534 , -10.551835  , ...,   0.31741843,\n",
      "            0.03223436,   2.0512383 ]]]], dtype=float32)\n",
      "_Z         = array([[[[ 8.82026196e+00,  2.00078607e+00,  4.89369011e+00, ...,\n",
      "          -4.88638926e+00,  4.75044203e+00, -7.56786...166728e+00,  4.62055349e+00, ...,\n",
      "          -6.23090088e-01, -8.48226726e-01, -2.29984760e+00]]]],\n",
      "      dtype=float32)\n",
      "backward   = True\n",
      "device     = cuda()\n",
      "padding    = 0\n",
      "stride     = 1\n",
      "torch      = <module 'torch' from '/opt/conda/envs/train/lib/python3.11/site-packages/torch/__init__.py'>\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:429: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:514: in conv\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Conv(stride, padding)(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e...3e+00]\n",
      "   [ 1.09905624e+01  2.23166728e+00  4.62055349e+00 ... -6.23090088e-01\n",
      "    -8.48226726e-01 -2.29984760e+00]]]])\n",
      "        b          = needle.Tensor([[[[  4.282669     8.133604    -4.6223483  ...   2.0798197\n",
      "      2.1704152   -0.38803983]\n",
      "   [  2.245844...2.618842     0.71487164]\n",
      "   [ -4.09374      3.7291534  -10.551835   ...   0.31741843\n",
      "      0.03223436   2.0512383 ]]]])\n",
      "        padding    = 0\n",
      "        stride     = 1\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048....618842     0.71487164]\n",
      "   [ -4.09374      3.7291534  -10.551835   ...   0.31741843\n",
      "      0.03223436   2.0512383 ]]]]))\n",
      "        self       = <needle.ops.ops_mathematic.Conv object at 0x7fd94f75ce10>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048....618842     0.71487164]\n",
      "   [ -4.09374      3.7291534  -10.551835   ...   0.31741843\n",
      "      0.03223436   2.0512383 ]]]]))\n",
      "        op         = <needle.ops.ops_mathematic.Conv object at 0x7fd94f75ce10>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fd94f75d650>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fd94f75d650>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Conv object at 0x7fd94f75ce10>\n",
      "A = NDArray([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e-01]\n",
      " ...9905624e+01  2.23166728e+00  4.62055349e+00 ... -6.23090088e-01\n",
      "    -8.48226726e-01 -2.29984760e+00]]]], device=cuda())\n",
      "B = NDArray([[[[  4.282669     8.133604    -4.6223483  ...   2.0798197\n",
      "      2.1704152   -0.38803983]\n",
      "   [  2.2458448   -0...71487164]\n",
      "   [ -4.09374      3.7291534  -10.551835   ...   0.31741843\n",
      "      0.03223436   2.0512383 ]]]], device=cuda())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, A, B):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "A          = NDArray([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e-01]\n",
      " ...9905624e+01  2.23166728e+00  4.62055349e+00 ... -6.23090088e-01\n",
      "    -8.48226726e-01 -2.29984760e+00]]]], device=cuda())\n",
      "B          = NDArray([[[[  4.282669     8.133604    -4.6223483  ...   2.0798197\n",
      "      2.1704152   -0.38803983]\n",
      "   [  2.2458448   -0...71487164]\n",
      "   [ -4.09374      3.7291534  -10.551835   ...   0.31741843\n",
      "      0.03223436   2.0512383 ]]]], device=cuda())\n",
      "self       = <needle.ops.ops_mathematic.Conv object at 0x7fd94f75ce10>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:504: NotImplementedError\n",
      "\u001b[31m\u001b[1m_ test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape1-W_shape1-1-1] _\u001b[0m\n",
      "\n",
      "Z_shape = (3, 14, 14, 8), W_shape = (3, 3, 8, 16), stride = 1, padding = 1\n",
      "backward = True, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mimport\u001b[39;49;00m \u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "W          = needle.Tensor([[[[  4.282669     8.133604    -4.6223483  ...   2.0798197\n",
      "      2.1704152   -0.38803983]\n",
      "   [  2.245844...2.618842     0.71487164]\n",
      "   [ -4.09374      3.7291534  -10.551835   ...   0.31741843\n",
      "      0.03223436   2.0512383 ]]]])\n",
      "W_shape    = (3, 3, 8, 16)\n",
      "Z          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e...3e+00]\n",
      "   [ 1.09905624e+01  2.23166728e+00  4.62055349e+00 ... -6.23090088e-01\n",
      "    -8.48226726e-01 -2.29984760e+00]]]])\n",
      "Z_shape    = (3, 14, 14, 8)\n",
      "_W         = array([[[[  4.282669  ,   8.133604  ,  -4.6223483 , ...,   2.0798197 ,\n",
      "            2.1704152 ,  -0.38803983],\n",
      "        ... [ -4.09374   ,   3.7291534 , -10.551835  , ...,   0.31741843,\n",
      "            0.03223436,   2.0512383 ]]]], dtype=float32)\n",
      "_Z         = array([[[[ 8.82026196e+00,  2.00078607e+00,  4.89369011e+00, ...,\n",
      "          -4.88638926e+00,  4.75044203e+00, -7.56786...166728e+00,  4.62055349e+00, ...,\n",
      "          -6.23090088e-01, -8.48226726e-01, -2.29984760e+00]]]],\n",
      "      dtype=float32)\n",
      "backward   = True\n",
      "device     = cuda()\n",
      "padding    = 1\n",
      "stride     = 1\n",
      "torch      = <module 'torch' from '/opt/conda/envs/train/lib/python3.11/site-packages/torch/__init__.py'>\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:429: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:514: in conv\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Conv(stride, padding)(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e...3e+00]\n",
      "   [ 1.09905624e+01  2.23166728e+00  4.62055349e+00 ... -6.23090088e-01\n",
      "    -8.48226726e-01 -2.29984760e+00]]]])\n",
      "        b          = needle.Tensor([[[[  4.282669     8.133604    -4.6223483  ...   2.0798197\n",
      "      2.1704152   -0.38803983]\n",
      "   [  2.245844...2.618842     0.71487164]\n",
      "   [ -4.09374      3.7291534  -10.551835   ...   0.31741843\n",
      "      0.03223436   2.0512383 ]]]])\n",
      "        padding    = 1\n",
      "        stride     = 1\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048....618842     0.71487164]\n",
      "   [ -4.09374      3.7291534  -10.551835   ...   0.31741843\n",
      "      0.03223436   2.0512383 ]]]]))\n",
      "        self       = <needle.ops.ops_mathematic.Conv object at 0x7fd94f6628d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048....618842     0.71487164]\n",
      "   [ -4.09374      3.7291534  -10.551835   ...   0.31741843\n",
      "      0.03223436   2.0512383 ]]]]))\n",
      "        op         = <needle.ops.ops_mathematic.Conv object at 0x7fd94f6628d0>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fd94f662050>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fd94f662050>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Conv object at 0x7fd94f6628d0>\n",
      "A = NDArray([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e-01]\n",
      " ...9905624e+01  2.23166728e+00  4.62055349e+00 ... -6.23090088e-01\n",
      "    -8.48226726e-01 -2.29984760e+00]]]], device=cuda())\n",
      "B = NDArray([[[[  4.282669     8.133604    -4.6223483  ...   2.0798197\n",
      "      2.1704152   -0.38803983]\n",
      "   [  2.2458448   -0...71487164]\n",
      "   [ -4.09374      3.7291534  -10.551835   ...   0.31741843\n",
      "      0.03223436   2.0512383 ]]]], device=cuda())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, A, B):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "A          = NDArray([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e-01]\n",
      " ...9905624e+01  2.23166728e+00  4.62055349e+00 ... -6.23090088e-01\n",
      "    -8.48226726e-01 -2.29984760e+00]]]], device=cuda())\n",
      "B          = NDArray([[[[  4.282669     8.133604    -4.6223483  ...   2.0798197\n",
      "      2.1704152   -0.38803983]\n",
      "   [  2.2458448   -0...71487164]\n",
      "   [ -4.09374      3.7291534  -10.551835   ...   0.31741843\n",
      "      0.03223436   2.0512383 ]]]], device=cuda())\n",
      "self       = <needle.ops.ops_mathematic.Conv object at 0x7fd94f6628d0>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:504: NotImplementedError\n",
      "\u001b[31m\u001b[1m_ test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape2-W_shape2-1-2] _\u001b[0m\n",
      "\n",
      "Z_shape = (3, 16, 16, 8), W_shape = (3, 3, 8, 16), stride = 1, padding = 2\n",
      "backward = True, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mimport\u001b[39;49;00m \u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "W          = needle.Tensor([[[[  0.40910086  -0.64982927  11.320294   ...  -0.12077556\n",
      "      1.428692    -5.5845156 ]\n",
      "   [ -6.89178...-1.5437248  -10.136281  ]\n",
      "   [  1.1511405    0.7441038   16.463472   ...  -6.8931007\n",
      "     -0.5833115   -2.504636  ]]]])\n",
      "W_shape    = (3, 3, 8, 16)\n",
      "Z          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e...5e+00]\n",
      "   [ 6.10367346e+00 -2.05434513e+00 -4.41384506e+00 ... -6.15245628e+00\n",
      "     3.33514667e+00 -1.35630560e+00]]]])\n",
      "Z_shape    = (3, 16, 16, 8)\n",
      "_W         = array([[[[  0.40910086,  -0.64982927,  11.320294  , ...,  -0.12077556,\n",
      "            1.428692  ,  -5.5845156 ],\n",
      "        ... [  1.1511405 ,   0.7441038 ,  16.463472  , ...,  -6.8931007 ,\n",
      "           -0.5833115 ,  -2.504636  ]]]], dtype=float32)\n",
      "_Z         = array([[[[ 8.82026196e+00,  2.00078607e+00,  4.89369011e+00, ...,\n",
      "          -4.88638926e+00,  4.75044203e+00, -7.56786...434513e+00, -4.41384506e+00, ...,\n",
      "          -6.15245628e+00,  3.33514667e+00, -1.35630560e+00]]]],\n",
      "      dtype=float32)\n",
      "backward   = True\n",
      "device     = cuda()\n",
      "padding    = 2\n",
      "stride     = 1\n",
      "torch      = <module 'torch' from '/opt/conda/envs/train/lib/python3.11/site-packages/torch/__init__.py'>\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:429: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:514: in conv\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Conv(stride, padding)(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e...5e+00]\n",
      "   [ 6.10367346e+00 -2.05434513e+00 -4.41384506e+00 ... -6.15245628e+00\n",
      "     3.33514667e+00 -1.35630560e+00]]]])\n",
      "        b          = needle.Tensor([[[[  0.40910086  -0.64982927  11.320294   ...  -0.12077556\n",
      "      1.428692    -5.5845156 ]\n",
      "   [ -6.89178...-1.5437248  -10.136281  ]\n",
      "   [  1.1511405    0.7441038   16.463472   ...  -6.8931007\n",
      "     -0.5833115   -2.504636  ]]]])\n",
      "        padding    = 2\n",
      "        stride     = 1\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048...1.5437248  -10.136281  ]\n",
      "   [  1.1511405    0.7441038   16.463472   ...  -6.8931007\n",
      "     -0.5833115   -2.504636  ]]]]))\n",
      "        self       = <needle.ops.ops_mathematic.Conv object at 0x7fd94f718ad0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048...1.5437248  -10.136281  ]\n",
      "   [  1.1511405    0.7441038   16.463472   ...  -6.8931007\n",
      "     -0.5833115   -2.504636  ]]]]))\n",
      "        op         = <needle.ops.ops_mathematic.Conv object at 0x7fd94f718ad0>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fd94f71b950>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fd94f71b950>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Conv object at 0x7fd94f718ad0>\n",
      "A = NDArray([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e-01]\n",
      " ...0367346e+00 -2.05434513e+00 -4.41384506e+00 ... -6.15245628e+00\n",
      "     3.33514667e+00 -1.35630560e+00]]]], device=cuda())\n",
      "B = NDArray([[[[  0.40910086  -0.64982927  11.320294   ...  -0.12077556\n",
      "      1.428692    -5.5845156 ]\n",
      "   [ -6.891782    -....136281  ]\n",
      "   [  1.1511405    0.7441038   16.463472   ...  -6.8931007\n",
      "     -0.5833115   -2.504636  ]]]], device=cuda())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, A, B):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "A          = NDArray([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e-01]\n",
      " ...0367346e+00 -2.05434513e+00 -4.41384506e+00 ... -6.15245628e+00\n",
      "     3.33514667e+00 -1.35630560e+00]]]], device=cuda())\n",
      "B          = NDArray([[[[  0.40910086  -0.64982927  11.320294   ...  -0.12077556\n",
      "      1.428692    -5.5845156 ]\n",
      "   [ -6.891782    -....136281  ]\n",
      "   [  1.1511405    0.7441038   16.463472   ...  -6.8931007\n",
      "     -0.5833115   -2.504636  ]]]], device=cuda())\n",
      "self       = <needle.ops.ops_mathematic.Conv object at 0x7fd94f718ad0>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:504: NotImplementedError\n",
      "\u001b[31m\u001b[1m_ test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape3-W_shape3-1-0] _\u001b[0m\n",
      "\n",
      "Z_shape = (3, 16, 16, 8), W_shape = (3, 3, 8, 14), stride = 1, padding = 0\n",
      "backward = True, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mimport\u001b[39;49;00m \u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "W          = needle.Tensor([[[[ 4.09100860e-01 -6.49829268e-01  1.13202944e+01 ... -5.56089449e+00\n",
      "    -4.84385538e+00 -1.20775558e...5e+00]\n",
      "   [-1.10714078e+00  2.54448557e+00 -7.73867989e+00 ... -1.13336074e+00\n",
      "    -1.21658230e+00 -4.79330921e+00]]]])\n",
      "W_shape    = (3, 3, 8, 14)\n",
      "Z          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e...5e+00]\n",
      "   [ 6.10367346e+00 -2.05434513e+00 -4.41384506e+00 ... -6.15245628e+00\n",
      "     3.33514667e+00 -1.35630560e+00]]]])\n",
      "Z_shape    = (3, 16, 16, 8)\n",
      "_W         = array([[[[ 4.09100860e-01, -6.49829268e-01,  1.13202944e+01, ...,\n",
      "          -5.56089449e+00, -4.84385538e+00, -1.20775...448557e+00, -7.73867989e+00, ...,\n",
      "          -1.13336074e+00, -1.21658230e+00, -4.79330921e+00]]]],\n",
      "      dtype=float32)\n",
      "_Z         = array([[[[ 8.82026196e+00,  2.00078607e+00,  4.89369011e+00, ...,\n",
      "          -4.88638926e+00,  4.75044203e+00, -7.56786...434513e+00, -4.41384506e+00, ...,\n",
      "          -6.15245628e+00,  3.33514667e+00, -1.35630560e+00]]]],\n",
      "      dtype=float32)\n",
      "backward   = True\n",
      "device     = cuda()\n",
      "padding    = 0\n",
      "stride     = 1\n",
      "torch      = <module 'torch' from '/opt/conda/envs/train/lib/python3.11/site-packages/torch/__init__.py'>\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:429: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:514: in conv\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Conv(stride, padding)(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e...5e+00]\n",
      "   [ 6.10367346e+00 -2.05434513e+00 -4.41384506e+00 ... -6.15245628e+00\n",
      "     3.33514667e+00 -1.35630560e+00]]]])\n",
      "        b          = needle.Tensor([[[[ 4.09100860e-01 -6.49829268e-01  1.13202944e+01 ... -5.56089449e+00\n",
      "    -4.84385538e+00 -1.20775558e...5e+00]\n",
      "   [-1.10714078e+00  2.54448557e+00 -7.73867989e+00 ... -1.13336074e+00\n",
      "    -1.21658230e+00 -4.79330921e+00]]]])\n",
      "        padding    = 0\n",
      "        stride     = 1\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048...e+00]\n",
      "   [-1.10714078e+00  2.54448557e+00 -7.73867989e+00 ... -1.13336074e+00\n",
      "    -1.21658230e+00 -4.79330921e+00]]]]))\n",
      "        self       = <needle.ops.ops_mathematic.Conv object at 0x7fd94f505190>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048...e+00]\n",
      "   [-1.10714078e+00  2.54448557e+00 -7.73867989e+00 ... -1.13336074e+00\n",
      "    -1.21658230e+00 -4.79330921e+00]]]]))\n",
      "        op         = <needle.ops.ops_mathematic.Conv object at 0x7fd94f505190>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fd94f505c50>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fd94f505c50>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Conv object at 0x7fd94f505190>\n",
      "A = NDArray([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e-01]\n",
      " ...0367346e+00 -2.05434513e+00 -4.41384506e+00 ... -6.15245628e+00\n",
      "     3.33514667e+00 -1.35630560e+00]]]], device=cuda())\n",
      "B = NDArray([[[[ 4.09100860e-01 -6.49829268e-01  1.13202944e+01 ... -5.56089449e+00\n",
      "    -4.84385538e+00 -1.20775558e-01]\n",
      " ...0714078e+00  2.54448557e+00 -7.73867989e+00 ... -1.13336074e+00\n",
      "    -1.21658230e+00 -4.79330921e+00]]]], device=cuda())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, A, B):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "A          = NDArray([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e-01]\n",
      " ...0367346e+00 -2.05434513e+00 -4.41384506e+00 ... -6.15245628e+00\n",
      "     3.33514667e+00 -1.35630560e+00]]]], device=cuda())\n",
      "B          = NDArray([[[[ 4.09100860e-01 -6.49829268e-01  1.13202944e+01 ... -5.56089449e+00\n",
      "    -4.84385538e+00 -1.20775558e-01]\n",
      " ...0714078e+00  2.54448557e+00 -7.73867989e+00 ... -1.13336074e+00\n",
      "    -1.21658230e+00 -4.79330921e+00]]]], device=cuda())\n",
      "self       = <needle.ops.ops_mathematic.Conv object at 0x7fd94f505190>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:504: NotImplementedError\n",
      "\u001b[31m\u001b[1m_ test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape4-W_shape4-1-0] _\u001b[0m\n",
      "\n",
      "Z_shape = (3, 16, 16, 2), W_shape = (3, 3, 2, 14), stride = 1, padding = 0\n",
      "backward = True, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mimport\u001b[39;49;00m \u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "W          = needle.Tensor([[[[ -8.0968      -2.5552022    8.703147    -1.4674252    4.5861077\n",
      "     -0.28521433   4.383634    -9.13...727333   0.22582927   9.296732    -8.13161     -0.67411226\n",
      "     -2.9204676    1.675528   -12.187821     5.5746226 ]]]])\n",
      "W_shape    = (3, 3, 2, 14)\n",
      "Z          = needle.Tensor([[[[  8.820262     2.000786  ]\n",
      "   [  4.89369     11.204466  ]\n",
      "   [  9.33779     -4.8863893 ]\n",
      "   ...\n",
      "   [...   -5.112822  ]\n",
      "   ...\n",
      "   [ -7.922368     4.2222714 ]\n",
      "   [ -6.064339     1.4188478 ]\n",
      "   [ -1.4109794   -5.791016  ]]]])\n",
      "Z_shape    = (3, 16, 16, 2)\n",
      "_W         = array([[[[ -8.0968    ,  -2.5552022 ,   8.703147  ,  -1.4674252 ,\n",
      "            4.5861077 ,  -0.28521433,   4.383634  , ...        -8.13161   ,  -0.67411226,  -2.9204676 ,   1.675528  ,\n",
      "          -12.187821  ,   5.5746226 ]]]], dtype=float32)\n",
      "_Z         = array([[[[  8.820262  ,   2.000786  ],\n",
      "         [  4.89369   ,  11.204466  ],\n",
      "         [  9.33779   ,  -4.8863893 ],\n",
      " ...22368  ,   4.2222714 ],\n",
      "         [ -6.064339  ,   1.4188478 ],\n",
      "         [ -1.4109794 ,  -5.791016  ]]]], dtype=float32)\n",
      "backward   = True\n",
      "device     = cuda()\n",
      "padding    = 0\n",
      "stride     = 1\n",
      "torch      = <module 'torch' from '/opt/conda/envs/train/lib/python3.11/site-packages/torch/__init__.py'>\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:429: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:514: in conv\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Conv(stride, padding)(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[[  8.820262     2.000786  ]\n",
      "   [  4.89369     11.204466  ]\n",
      "   [  9.33779     -4.8863893 ]\n",
      "   ...\n",
      "   [...   -5.112822  ]\n",
      "   ...\n",
      "   [ -7.922368     4.2222714 ]\n",
      "   [ -6.064339     1.4188478 ]\n",
      "   [ -1.4109794   -5.791016  ]]]])\n",
      "        b          = needle.Tensor([[[[ -8.0968      -2.5552022    8.703147    -1.4674252    4.5861077\n",
      "     -0.28521433   4.383634    -9.13...727333   0.22582927   9.296732    -8.13161     -0.67411226\n",
      "     -2.9204676    1.675528   -12.187821     5.5746226 ]]]])\n",
      "        padding    = 0\n",
      "        stride     = 1\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[  8.820262     2.000786  ]\n",
      "   [  4.89369     11.204466  ]\n",
      "   [  9.33779     -4.8863893 ]\n",
      "   ...\n",
      "   ...27333   0.22582927   9.296732    -8.13161     -0.67411226\n",
      "     -2.9204676    1.675528   -12.187821     5.5746226 ]]]]))\n",
      "        self       = <needle.ops.ops_mathematic.Conv object at 0x7fd94f6fa5d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[[  8.820262     2.000786  ]\n",
      "   [  4.89369     11.204466  ]\n",
      "   [  9.33779     -4.8863893 ]\n",
      "   ...\n",
      "   ...27333   0.22582927   9.296732    -8.13161     -0.67411226\n",
      "     -2.9204676    1.675528   -12.187821     5.5746226 ]]]]))\n",
      "        op         = <needle.ops.ops_mathematic.Conv object at 0x7fd94f6fa5d0>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fd94f6f91d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fd94f6f91d0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Conv object at 0x7fd94f6fa5d0>\n",
      "A = NDArray([[[[  8.820262     2.000786  ]\n",
      "   [  4.89369     11.204466  ]\n",
      "   [  9.33779     -4.8863893 ]\n",
      "   ...\n",
      "   [  0.22...\n",
      "   ...\n",
      "   [ -7.922368     4.2222714 ]\n",
      "   [ -6.064339     1.4188478 ]\n",
      "   [ -1.4109794   -5.791016  ]]]], device=cuda())\n",
      "B = NDArray([[[[ -8.0968      -2.5552022    8.703147    -1.4674252    4.5861077\n",
      "     -0.28521433   4.383634    -9.134557  ...2927   9.296732    -8.13161     -0.67411226\n",
      "     -2.9204676    1.675528   -12.187821     5.5746226 ]]]], device=cuda())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, A, B):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "A          = NDArray([[[[  8.820262     2.000786  ]\n",
      "   [  4.89369     11.204466  ]\n",
      "   [  9.33779     -4.8863893 ]\n",
      "   ...\n",
      "   [  0.22...\n",
      "   ...\n",
      "   [ -7.922368     4.2222714 ]\n",
      "   [ -6.064339     1.4188478 ]\n",
      "   [ -1.4109794   -5.791016  ]]]], device=cuda())\n",
      "B          = NDArray([[[[ -8.0968      -2.5552022    8.703147    -1.4674252    4.5861077\n",
      "     -0.28521433   4.383634    -9.134557  ...2927   9.296732    -8.13161     -0.67411226\n",
      "     -2.9204676    1.675528   -12.187821     5.5746226 ]]]], device=cuda())\n",
      "self       = <needle.ops.ops_mathematic.Conv object at 0x7fd94f6fa5d0>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:504: NotImplementedError\n",
      "\u001b[31m\u001b[1m_ test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape5-W_shape5-2-0] _\u001b[0m\n",
      "\n",
      "Z_shape = (3, 14, 14, 8), W_shape = (3, 3, 8, 16), stride = 2, padding = 0\n",
      "backward = True, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mimport\u001b[39;49;00m \u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "W          = needle.Tensor([[[[  4.282669     8.133604    -4.6223483  ...   2.0798197\n",
      "      2.1704152   -0.38803983]\n",
      "   [  2.245844...2.618842     0.71487164]\n",
      "   [ -4.09374      3.7291534  -10.551835   ...   0.31741843\n",
      "      0.03223436   2.0512383 ]]]])\n",
      "W_shape    = (3, 3, 8, 16)\n",
      "Z          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e...3e+00]\n",
      "   [ 1.09905624e+01  2.23166728e+00  4.62055349e+00 ... -6.23090088e-01\n",
      "    -8.48226726e-01 -2.29984760e+00]]]])\n",
      "Z_shape    = (3, 14, 14, 8)\n",
      "_W         = array([[[[  4.282669  ,   8.133604  ,  -4.6223483 , ...,   2.0798197 ,\n",
      "            2.1704152 ,  -0.38803983],\n",
      "        ... [ -4.09374   ,   3.7291534 , -10.551835  , ...,   0.31741843,\n",
      "            0.03223436,   2.0512383 ]]]], dtype=float32)\n",
      "_Z         = array([[[[ 8.82026196e+00,  2.00078607e+00,  4.89369011e+00, ...,\n",
      "          -4.88638926e+00,  4.75044203e+00, -7.56786...166728e+00,  4.62055349e+00, ...,\n",
      "          -6.23090088e-01, -8.48226726e-01, -2.29984760e+00]]]],\n",
      "      dtype=float32)\n",
      "backward   = True\n",
      "device     = cuda()\n",
      "padding    = 0\n",
      "stride     = 2\n",
      "torch      = <module 'torch' from '/opt/conda/envs/train/lib/python3.11/site-packages/torch/__init__.py'>\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:429: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:514: in conv\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Conv(stride, padding)(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e...3e+00]\n",
      "   [ 1.09905624e+01  2.23166728e+00  4.62055349e+00 ... -6.23090088e-01\n",
      "    -8.48226726e-01 -2.29984760e+00]]]])\n",
      "        b          = needle.Tensor([[[[  4.282669     8.133604    -4.6223483  ...   2.0798197\n",
      "      2.1704152   -0.38803983]\n",
      "   [  2.245844...2.618842     0.71487164]\n",
      "   [ -4.09374      3.7291534  -10.551835   ...   0.31741843\n",
      "      0.03223436   2.0512383 ]]]])\n",
      "        padding    = 0\n",
      "        stride     = 2\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048....618842     0.71487164]\n",
      "   [ -4.09374      3.7291534  -10.551835   ...   0.31741843\n",
      "      0.03223436   2.0512383 ]]]]))\n",
      "        self       = <needle.ops.ops_mathematic.Conv object at 0x7fd94f729d90>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048....618842     0.71487164]\n",
      "   [ -4.09374      3.7291534  -10.551835   ...   0.31741843\n",
      "      0.03223436   2.0512383 ]]]]))\n",
      "        op         = <needle.ops.ops_mathematic.Conv object at 0x7fd94f729d90>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fd94f729f90>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fd94f729f90>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Conv object at 0x7fd94f729d90>\n",
      "A = NDArray([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e-01]\n",
      " ...9905624e+01  2.23166728e+00  4.62055349e+00 ... -6.23090088e-01\n",
      "    -8.48226726e-01 -2.29984760e+00]]]], device=cuda())\n",
      "B = NDArray([[[[  4.282669     8.133604    -4.6223483  ...   2.0798197\n",
      "      2.1704152   -0.38803983]\n",
      "   [  2.2458448   -0...71487164]\n",
      "   [ -4.09374      3.7291534  -10.551835   ...   0.31741843\n",
      "      0.03223436   2.0512383 ]]]], device=cuda())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, A, B):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "A          = NDArray([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e-01]\n",
      " ...9905624e+01  2.23166728e+00  4.62055349e+00 ... -6.23090088e-01\n",
      "    -8.48226726e-01 -2.29984760e+00]]]], device=cuda())\n",
      "B          = NDArray([[[[  4.282669     8.133604    -4.6223483  ...   2.0798197\n",
      "      2.1704152   -0.38803983]\n",
      "   [  2.2458448   -0...71487164]\n",
      "   [ -4.09374      3.7291534  -10.551835   ...   0.31741843\n",
      "      0.03223436   2.0512383 ]]]], device=cuda())\n",
      "self       = <needle.ops.ops_mathematic.Conv object at 0x7fd94f729d90>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:504: NotImplementedError\n",
      "\u001b[31m\u001b[1m_ test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape6-W_shape6-2-1] _\u001b[0m\n",
      "\n",
      "Z_shape = (3, 14, 14, 8), W_shape = (3, 3, 8, 16), stride = 2, padding = 1\n",
      "backward = True, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mimport\u001b[39;49;00m \u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "W          = needle.Tensor([[[[  4.282669     8.133604    -4.6223483  ...   2.0798197\n",
      "      2.1704152   -0.38803983]\n",
      "   [  2.245844...2.618842     0.71487164]\n",
      "   [ -4.09374      3.7291534  -10.551835   ...   0.31741843\n",
      "      0.03223436   2.0512383 ]]]])\n",
      "W_shape    = (3, 3, 8, 16)\n",
      "Z          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e...3e+00]\n",
      "   [ 1.09905624e+01  2.23166728e+00  4.62055349e+00 ... -6.23090088e-01\n",
      "    -8.48226726e-01 -2.29984760e+00]]]])\n",
      "Z_shape    = (3, 14, 14, 8)\n",
      "_W         = array([[[[  4.282669  ,   8.133604  ,  -4.6223483 , ...,   2.0798197 ,\n",
      "            2.1704152 ,  -0.38803983],\n",
      "        ... [ -4.09374   ,   3.7291534 , -10.551835  , ...,   0.31741843,\n",
      "            0.03223436,   2.0512383 ]]]], dtype=float32)\n",
      "_Z         = array([[[[ 8.82026196e+00,  2.00078607e+00,  4.89369011e+00, ...,\n",
      "          -4.88638926e+00,  4.75044203e+00, -7.56786...166728e+00,  4.62055349e+00, ...,\n",
      "          -6.23090088e-01, -8.48226726e-01, -2.29984760e+00]]]],\n",
      "      dtype=float32)\n",
      "backward   = True\n",
      "device     = cuda()\n",
      "padding    = 1\n",
      "stride     = 2\n",
      "torch      = <module 'torch' from '/opt/conda/envs/train/lib/python3.11/site-packages/torch/__init__.py'>\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:429: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:514: in conv\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Conv(stride, padding)(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e...3e+00]\n",
      "   [ 1.09905624e+01  2.23166728e+00  4.62055349e+00 ... -6.23090088e-01\n",
      "    -8.48226726e-01 -2.29984760e+00]]]])\n",
      "        b          = needle.Tensor([[[[  4.282669     8.133604    -4.6223483  ...   2.0798197\n",
      "      2.1704152   -0.38803983]\n",
      "   [  2.245844...2.618842     0.71487164]\n",
      "   [ -4.09374      3.7291534  -10.551835   ...   0.31741843\n",
      "      0.03223436   2.0512383 ]]]])\n",
      "        padding    = 1\n",
      "        stride     = 2\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048....618842     0.71487164]\n",
      "   [ -4.09374      3.7291534  -10.551835   ...   0.31741843\n",
      "      0.03223436   2.0512383 ]]]]))\n",
      "        self       = <needle.ops.ops_mathematic.Conv object at 0x7fd94f541210>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048....618842     0.71487164]\n",
      "   [ -4.09374      3.7291534  -10.551835   ...   0.31741843\n",
      "      0.03223436   2.0512383 ]]]]))\n",
      "        op         = <needle.ops.ops_mathematic.Conv object at 0x7fd94f541210>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fd94f542150>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fd94f542150>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Conv object at 0x7fd94f541210>\n",
      "A = NDArray([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e-01]\n",
      " ...9905624e+01  2.23166728e+00  4.62055349e+00 ... -6.23090088e-01\n",
      "    -8.48226726e-01 -2.29984760e+00]]]], device=cuda())\n",
      "B = NDArray([[[[  4.282669     8.133604    -4.6223483  ...   2.0798197\n",
      "      2.1704152   -0.38803983]\n",
      "   [  2.2458448   -0...71487164]\n",
      "   [ -4.09374      3.7291534  -10.551835   ...   0.31741843\n",
      "      0.03223436   2.0512383 ]]]], device=cuda())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, A, B):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "A          = NDArray([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e-01]\n",
      " ...9905624e+01  2.23166728e+00  4.62055349e+00 ... -6.23090088e-01\n",
      "    -8.48226726e-01 -2.29984760e+00]]]], device=cuda())\n",
      "B          = NDArray([[[[  4.282669     8.133604    -4.6223483  ...   2.0798197\n",
      "      2.1704152   -0.38803983]\n",
      "   [  2.2458448   -0...71487164]\n",
      "   [ -4.09374      3.7291534  -10.551835   ...   0.31741843\n",
      "      0.03223436   2.0512383 ]]]], device=cuda())\n",
      "self       = <needle.ops.ops_mathematic.Conv object at 0x7fd94f541210>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:504: NotImplementedError\n",
      "\u001b[31m\u001b[1m_ test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape7-W_shape7-2-2] _\u001b[0m\n",
      "\n",
      "Z_shape = (3, 16, 16, 8), W_shape = (3, 3, 8, 16), stride = 2, padding = 2\n",
      "backward = True, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mimport\u001b[39;49;00m \u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "W          = needle.Tensor([[[[  0.40910086  -0.64982927  11.320294   ...  -0.12077556\n",
      "      1.428692    -5.5845156 ]\n",
      "   [ -6.89178...-1.5437248  -10.136281  ]\n",
      "   [  1.1511405    0.7441038   16.463472   ...  -6.8931007\n",
      "     -0.5833115   -2.504636  ]]]])\n",
      "W_shape    = (3, 3, 8, 16)\n",
      "Z          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e...5e+00]\n",
      "   [ 6.10367346e+00 -2.05434513e+00 -4.41384506e+00 ... -6.15245628e+00\n",
      "     3.33514667e+00 -1.35630560e+00]]]])\n",
      "Z_shape    = (3, 16, 16, 8)\n",
      "_W         = array([[[[  0.40910086,  -0.64982927,  11.320294  , ...,  -0.12077556,\n",
      "            1.428692  ,  -5.5845156 ],\n",
      "        ... [  1.1511405 ,   0.7441038 ,  16.463472  , ...,  -6.8931007 ,\n",
      "           -0.5833115 ,  -2.504636  ]]]], dtype=float32)\n",
      "_Z         = array([[[[ 8.82026196e+00,  2.00078607e+00,  4.89369011e+00, ...,\n",
      "          -4.88638926e+00,  4.75044203e+00, -7.56786...434513e+00, -4.41384506e+00, ...,\n",
      "          -6.15245628e+00,  3.33514667e+00, -1.35630560e+00]]]],\n",
      "      dtype=float32)\n",
      "backward   = True\n",
      "device     = cuda()\n",
      "padding    = 2\n",
      "stride     = 2\n",
      "torch      = <module 'torch' from '/opt/conda/envs/train/lib/python3.11/site-packages/torch/__init__.py'>\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:429: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:514: in conv\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Conv(stride, padding)(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e...5e+00]\n",
      "   [ 6.10367346e+00 -2.05434513e+00 -4.41384506e+00 ... -6.15245628e+00\n",
      "     3.33514667e+00 -1.35630560e+00]]]])\n",
      "        b          = needle.Tensor([[[[  0.40910086  -0.64982927  11.320294   ...  -0.12077556\n",
      "      1.428692    -5.5845156 ]\n",
      "   [ -6.89178...-1.5437248  -10.136281  ]\n",
      "   [  1.1511405    0.7441038   16.463472   ...  -6.8931007\n",
      "     -0.5833115   -2.504636  ]]]])\n",
      "        padding    = 2\n",
      "        stride     = 2\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048...1.5437248  -10.136281  ]\n",
      "   [  1.1511405    0.7441038   16.463472   ...  -6.8931007\n",
      "     -0.5833115   -2.504636  ]]]]))\n",
      "        self       = <needle.ops.ops_mathematic.Conv object at 0x7fd94f502f10>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048...1.5437248  -10.136281  ]\n",
      "   [  1.1511405    0.7441038   16.463472   ...  -6.8931007\n",
      "     -0.5833115   -2.504636  ]]]]))\n",
      "        op         = <needle.ops.ops_mathematic.Conv object at 0x7fd94f502f10>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fd94f501e10>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fd94f501e10>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Conv object at 0x7fd94f502f10>\n",
      "A = NDArray([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e-01]\n",
      " ...0367346e+00 -2.05434513e+00 -4.41384506e+00 ... -6.15245628e+00\n",
      "     3.33514667e+00 -1.35630560e+00]]]], device=cuda())\n",
      "B = NDArray([[[[  0.40910086  -0.64982927  11.320294   ...  -0.12077556\n",
      "      1.428692    -5.5845156 ]\n",
      "   [ -6.891782    -....136281  ]\n",
      "   [  1.1511405    0.7441038   16.463472   ...  -6.8931007\n",
      "     -0.5833115   -2.504636  ]]]], device=cuda())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, A, B):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "A          = NDArray([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e-01]\n",
      " ...0367346e+00 -2.05434513e+00 -4.41384506e+00 ... -6.15245628e+00\n",
      "     3.33514667e+00 -1.35630560e+00]]]], device=cuda())\n",
      "B          = NDArray([[[[  0.40910086  -0.64982927  11.320294   ...  -0.12077556\n",
      "      1.428692    -5.5845156 ]\n",
      "   [ -6.891782    -....136281  ]\n",
      "   [  1.1511405    0.7441038   16.463472   ...  -6.8931007\n",
      "     -0.5833115   -2.504636  ]]]], device=cuda())\n",
      "self       = <needle.ops.ops_mathematic.Conv object at 0x7fd94f502f10>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:504: NotImplementedError\n",
      "\u001b[31m\u001b[1m_ test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape8-W_shape8-2-0] _\u001b[0m\n",
      "\n",
      "Z_shape = (3, 16, 16, 8), W_shape = (3, 3, 8, 14), stride = 2, padding = 0\n",
      "backward = True, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mimport\u001b[39;49;00m \u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "W          = needle.Tensor([[[[ 4.09100860e-01 -6.49829268e-01  1.13202944e+01 ... -5.56089449e+00\n",
      "    -4.84385538e+00 -1.20775558e...5e+00]\n",
      "   [-1.10714078e+00  2.54448557e+00 -7.73867989e+00 ... -1.13336074e+00\n",
      "    -1.21658230e+00 -4.79330921e+00]]]])\n",
      "W_shape    = (3, 3, 8, 14)\n",
      "Z          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e...5e+00]\n",
      "   [ 6.10367346e+00 -2.05434513e+00 -4.41384506e+00 ... -6.15245628e+00\n",
      "     3.33514667e+00 -1.35630560e+00]]]])\n",
      "Z_shape    = (3, 16, 16, 8)\n",
      "_W         = array([[[[ 4.09100860e-01, -6.49829268e-01,  1.13202944e+01, ...,\n",
      "          -5.56089449e+00, -4.84385538e+00, -1.20775...448557e+00, -7.73867989e+00, ...,\n",
      "          -1.13336074e+00, -1.21658230e+00, -4.79330921e+00]]]],\n",
      "      dtype=float32)\n",
      "_Z         = array([[[[ 8.82026196e+00,  2.00078607e+00,  4.89369011e+00, ...,\n",
      "          -4.88638926e+00,  4.75044203e+00, -7.56786...434513e+00, -4.41384506e+00, ...,\n",
      "          -6.15245628e+00,  3.33514667e+00, -1.35630560e+00]]]],\n",
      "      dtype=float32)\n",
      "backward   = True\n",
      "device     = cuda()\n",
      "padding    = 0\n",
      "stride     = 2\n",
      "torch      = <module 'torch' from '/opt/conda/envs/train/lib/python3.11/site-packages/torch/__init__.py'>\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:429: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:514: in conv\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Conv(stride, padding)(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e...5e+00]\n",
      "   [ 6.10367346e+00 -2.05434513e+00 -4.41384506e+00 ... -6.15245628e+00\n",
      "     3.33514667e+00 -1.35630560e+00]]]])\n",
      "        b          = needle.Tensor([[[[ 4.09100860e-01 -6.49829268e-01  1.13202944e+01 ... -5.56089449e+00\n",
      "    -4.84385538e+00 -1.20775558e...5e+00]\n",
      "   [-1.10714078e+00  2.54448557e+00 -7.73867989e+00 ... -1.13336074e+00\n",
      "    -1.21658230e+00 -4.79330921e+00]]]])\n",
      "        padding    = 0\n",
      "        stride     = 2\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048...e+00]\n",
      "   [-1.10714078e+00  2.54448557e+00 -7.73867989e+00 ... -1.13336074e+00\n",
      "    -1.21658230e+00 -4.79330921e+00]]]]))\n",
      "        self       = <needle.ops.ops_mathematic.Conv object at 0x7fd94f4f8350>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048...e+00]\n",
      "   [-1.10714078e+00  2.54448557e+00 -7.73867989e+00 ... -1.13336074e+00\n",
      "    -1.21658230e+00 -4.79330921e+00]]]]))\n",
      "        op         = <needle.ops.ops_mathematic.Conv object at 0x7fd94f4f8350>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fd94f4fbbd0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fd94f4fbbd0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Conv object at 0x7fd94f4f8350>\n",
      "A = NDArray([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e-01]\n",
      " ...0367346e+00 -2.05434513e+00 -4.41384506e+00 ... -6.15245628e+00\n",
      "     3.33514667e+00 -1.35630560e+00]]]], device=cuda())\n",
      "B = NDArray([[[[ 4.09100860e-01 -6.49829268e-01  1.13202944e+01 ... -5.56089449e+00\n",
      "    -4.84385538e+00 -1.20775558e-01]\n",
      " ...0714078e+00  2.54448557e+00 -7.73867989e+00 ... -1.13336074e+00\n",
      "    -1.21658230e+00 -4.79330921e+00]]]], device=cuda())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, A, B):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "A          = NDArray([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e-01]\n",
      " ...0367346e+00 -2.05434513e+00 -4.41384506e+00 ... -6.15245628e+00\n",
      "     3.33514667e+00 -1.35630560e+00]]]], device=cuda())\n",
      "B          = NDArray([[[[ 4.09100860e-01 -6.49829268e-01  1.13202944e+01 ... -5.56089449e+00\n",
      "    -4.84385538e+00 -1.20775558e-01]\n",
      " ...0714078e+00  2.54448557e+00 -7.73867989e+00 ... -1.13336074e+00\n",
      "    -1.21658230e+00 -4.79330921e+00]]]], device=cuda())\n",
      "self       = <needle.ops.ops_mathematic.Conv object at 0x7fd94f4f8350>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:504: NotImplementedError\n",
      "\u001b[31m\u001b[1m_ test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape9-W_shape9-2-0] _\u001b[0m\n",
      "\n",
      "Z_shape = (3, 16, 16, 2), W_shape = (3, 3, 2, 14), stride = 2, padding = 0\n",
      "backward = True, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mimport\u001b[39;49;00m \u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "W          = needle.Tensor([[[[ -8.0968      -2.5552022    8.703147    -1.4674252    4.5861077\n",
      "     -0.28521433   4.383634    -9.13...727333   0.22582927   9.296732    -8.13161     -0.67411226\n",
      "     -2.9204676    1.675528   -12.187821     5.5746226 ]]]])\n",
      "W_shape    = (3, 3, 2, 14)\n",
      "Z          = needle.Tensor([[[[  8.820262     2.000786  ]\n",
      "   [  4.89369     11.204466  ]\n",
      "   [  9.33779     -4.8863893 ]\n",
      "   ...\n",
      "   [...   -5.112822  ]\n",
      "   ...\n",
      "   [ -7.922368     4.2222714 ]\n",
      "   [ -6.064339     1.4188478 ]\n",
      "   [ -1.4109794   -5.791016  ]]]])\n",
      "Z_shape    = (3, 16, 16, 2)\n",
      "_W         = array([[[[ -8.0968    ,  -2.5552022 ,   8.703147  ,  -1.4674252 ,\n",
      "            4.5861077 ,  -0.28521433,   4.383634  , ...        -8.13161   ,  -0.67411226,  -2.9204676 ,   1.675528  ,\n",
      "          -12.187821  ,   5.5746226 ]]]], dtype=float32)\n",
      "_Z         = array([[[[  8.820262  ,   2.000786  ],\n",
      "         [  4.89369   ,  11.204466  ],\n",
      "         [  9.33779   ,  -4.8863893 ],\n",
      " ...22368  ,   4.2222714 ],\n",
      "         [ -6.064339  ,   1.4188478 ],\n",
      "         [ -1.4109794 ,  -5.791016  ]]]], dtype=float32)\n",
      "backward   = True\n",
      "device     = cuda()\n",
      "padding    = 0\n",
      "stride     = 2\n",
      "torch      = <module 'torch' from '/opt/conda/envs/train/lib/python3.11/site-packages/torch/__init__.py'>\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:429: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:514: in conv\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Conv(stride, padding)(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[[  8.820262     2.000786  ]\n",
      "   [  4.89369     11.204466  ]\n",
      "   [  9.33779     -4.8863893 ]\n",
      "   ...\n",
      "   [...   -5.112822  ]\n",
      "   ...\n",
      "   [ -7.922368     4.2222714 ]\n",
      "   [ -6.064339     1.4188478 ]\n",
      "   [ -1.4109794   -5.791016  ]]]])\n",
      "        b          = needle.Tensor([[[[ -8.0968      -2.5552022    8.703147    -1.4674252    4.5861077\n",
      "     -0.28521433   4.383634    -9.13...727333   0.22582927   9.296732    -8.13161     -0.67411226\n",
      "     -2.9204676    1.675528   -12.187821     5.5746226 ]]]])\n",
      "        padding    = 0\n",
      "        stride     = 2\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[  8.820262     2.000786  ]\n",
      "   [  4.89369     11.204466  ]\n",
      "   [  9.33779     -4.8863893 ]\n",
      "   ...\n",
      "   ...27333   0.22582927   9.296732    -8.13161     -0.67411226\n",
      "     -2.9204676    1.675528   -12.187821     5.5746226 ]]]]))\n",
      "        self       = <needle.ops.ops_mathematic.Conv object at 0x7fd94f5c8650>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[[  8.820262     2.000786  ]\n",
      "   [  4.89369     11.204466  ]\n",
      "   [  9.33779     -4.8863893 ]\n",
      "   ...\n",
      "   ...27333   0.22582927   9.296732    -8.13161     -0.67411226\n",
      "     -2.9204676    1.675528   -12.187821     5.5746226 ]]]]))\n",
      "        op         = <needle.ops.ops_mathematic.Conv object at 0x7fd94f5c8650>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fd94f5c8150>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fd94f5c8150>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Conv object at 0x7fd94f5c8650>\n",
      "A = NDArray([[[[  8.820262     2.000786  ]\n",
      "   [  4.89369     11.204466  ]\n",
      "   [  9.33779     -4.8863893 ]\n",
      "   ...\n",
      "   [  0.22...\n",
      "   ...\n",
      "   [ -7.922368     4.2222714 ]\n",
      "   [ -6.064339     1.4188478 ]\n",
      "   [ -1.4109794   -5.791016  ]]]], device=cuda())\n",
      "B = NDArray([[[[ -8.0968      -2.5552022    8.703147    -1.4674252    4.5861077\n",
      "     -0.28521433   4.383634    -9.134557  ...2927   9.296732    -8.13161     -0.67411226\n",
      "     -2.9204676    1.675528   -12.187821     5.5746226 ]]]], device=cuda())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, A, B):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "A          = NDArray([[[[  8.820262     2.000786  ]\n",
      "   [  4.89369     11.204466  ]\n",
      "   [  9.33779     -4.8863893 ]\n",
      "   ...\n",
      "   [  0.22...\n",
      "   ...\n",
      "   [ -7.922368     4.2222714 ]\n",
      "   [ -6.064339     1.4188478 ]\n",
      "   [ -1.4109794   -5.791016  ]]]], device=cuda())\n",
      "B          = NDArray([[[[ -8.0968      -2.5552022    8.703147    -1.4674252    4.5861077\n",
      "     -0.28521433   4.383634    -9.134557  ...2927   9.296732    -8.13161     -0.67411226\n",
      "     -2.9204676    1.675528   -12.187821     5.5746226 ]]]], device=cuda())\n",
      "self       = <needle.ops.ops_mathematic.Conv object at 0x7fd94f5c8650>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:504: NotImplementedError\n",
      "\u001b[31m\u001b[1m_ test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape10-W_shape10-1-0] _\u001b[0m\n",
      "\n",
      "Z_shape = (3, 16, 16, 24), W_shape = (3, 3, 24, 14), stride = 1, padding = 0\n",
      "backward = True, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mimport\u001b[39;49;00m \u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "W          = needle.Tensor([[[[ -1.3245817    1.0111231   -1.9236585  ...  -2.5907016\n",
      "     -0.36175704   4.6234684 ]\n",
      "   [  5.279258... -4.82095      4.98517   ]\n",
      "   [  3.2300415   -2.4161792   -3.420825   ...  -9.444658\n",
      "      8.537833     2.3818388 ]]]])\n",
      "W_shape    = (3, 3, 24, 14)\n",
      "Z          = needle.Tensor([[[[  8.820262     2.000786     4.89369    ...   3.2680929\n",
      "      4.322181    -3.7108252 ]\n",
      "   [ 11.348773...4.207389     8.935435  ]\n",
      "   [  2.5084004    3.1399443   -0.12958507 ...   0.13740699\n",
      "     -1.400853     0.07475674]]]])\n",
      "Z_shape    = (3, 16, 16, 24)\n",
      "_W         = array([[[[ -1.3245817 ,   1.0111231 ,  -1.9236585 , ...,  -2.5907016 ,\n",
      "           -0.36175704,   4.6234684 ],\n",
      "        ... [  3.2300415 ,  -2.4161792 ,  -3.420825  , ...,  -9.444658  ,\n",
      "            8.537833  ,   2.3818388 ]]]], dtype=float32)\n",
      "_Z         = array([[[[  8.820262  ,   2.000786  ,   4.89369   , ...,   3.2680929 ,\n",
      "            4.322181  ,  -3.7108252 ],\n",
      "        ... [  2.5084004 ,   3.1399443 ,  -0.12958507, ...,   0.13740699,\n",
      "           -1.400853  ,   0.07475674]]]], dtype=float32)\n",
      "backward   = True\n",
      "device     = cuda()\n",
      "padding    = 0\n",
      "stride     = 1\n",
      "torch      = <module 'torch' from '/opt/conda/envs/train/lib/python3.11/site-packages/torch/__init__.py'>\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:429: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:514: in conv\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Conv(stride, padding)(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[[  8.820262     2.000786     4.89369    ...   3.2680929\n",
      "      4.322181    -3.7108252 ]\n",
      "   [ 11.348773...4.207389     8.935435  ]\n",
      "   [  2.5084004    3.1399443   -0.12958507 ...   0.13740699\n",
      "     -1.400853     0.07475674]]]])\n",
      "        b          = needle.Tensor([[[[ -1.3245817    1.0111231   -1.9236585  ...  -2.5907016\n",
      "     -0.36175704   4.6234684 ]\n",
      "   [  5.279258... -4.82095      4.98517   ]\n",
      "   [  3.2300415   -2.4161792   -3.420825   ...  -9.444658\n",
      "      8.537833     2.3818388 ]]]])\n",
      "        padding    = 0\n",
      "        stride     = 1\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[  8.820262     2.000786     4.89369    ...   3.2680929\n",
      "      4.322181    -3.7108252 ]\n",
      "   [ 11.34877...-4.82095      4.98517   ]\n",
      "   [  3.2300415   -2.4161792   -3.420825   ...  -9.444658\n",
      "      8.537833     2.3818388 ]]]]))\n",
      "        self       = <needle.ops.ops_mathematic.Conv object at 0x7fd94f674ed0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[[  8.820262     2.000786     4.89369    ...   3.2680929\n",
      "      4.322181    -3.7108252 ]\n",
      "   [ 11.34877...-4.82095      4.98517   ]\n",
      "   [  3.2300415   -2.4161792   -3.420825   ...  -9.444658\n",
      "      8.537833     2.3818388 ]]]]))\n",
      "        op         = <needle.ops.ops_mathematic.Conv object at 0x7fd94f674ed0>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fd94f676550>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fd94f676550>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Conv object at 0x7fd94f674ed0>\n",
      "A = NDArray([[[[  8.820262     2.000786     4.89369    ...   3.2680929\n",
      "      4.322181    -3.7108252 ]\n",
      "   [ 11.348773    -7...935435  ]\n",
      "   [  2.5084004    3.1399443   -0.12958507 ...   0.13740699\n",
      "     -1.400853     0.07475674]]]], device=cuda())\n",
      "B = NDArray([[[[ -1.3245817    1.0111231   -1.9236585  ...  -2.5907016\n",
      "     -0.36175704   4.6234684 ]\n",
      "   [  5.2792587   -1...4.98517   ]\n",
      "   [  3.2300415   -2.4161792   -3.420825   ...  -9.444658\n",
      "      8.537833     2.3818388 ]]]], device=cuda())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, A, B):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "A          = NDArray([[[[  8.820262     2.000786     4.89369    ...   3.2680929\n",
      "      4.322181    -3.7108252 ]\n",
      "   [ 11.348773    -7...935435  ]\n",
      "   [  2.5084004    3.1399443   -0.12958507 ...   0.13740699\n",
      "     -1.400853     0.07475674]]]], device=cuda())\n",
      "B          = NDArray([[[[ -1.3245817    1.0111231   -1.9236585  ...  -2.5907016\n",
      "     -0.36175704   4.6234684 ]\n",
      "   [  5.2792587   -1...4.98517   ]\n",
      "   [  3.2300415   -2.4161792   -3.420825   ...  -9.444658\n",
      "      8.537833     2.3818388 ]]]], device=cuda())\n",
      "self       = <needle.ops.ops_mathematic.Conv object at 0x7fd94f674ed0>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:504: NotImplementedError\n",
      "\u001b[31m\u001b[1m_ test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape11-W_shape11-1-0] _\u001b[0m\n",
      "\n",
      "Z_shape = (3, 14, 14, 8), W_shape = (5, 5, 8, 16), stride = 1, padding = 0\n",
      "backward = True, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mimport\u001b[39;49;00m \u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "W          = needle.Tensor([[[[ 4.28266907e+00  8.13360405e+00 -4.62234831e+00 ...  2.07981968e+00\n",
      "     2.17041516e+00 -3.88039827e...7e+00]\n",
      "   [-7.86027253e-01  4.30947495e+00 -4.97804356e+00 ... -5.69430470e-01\n",
      "     4.27775383e-01 -3.84437203e+00]]]])\n",
      "W_shape    = (5, 5, 8, 16)\n",
      "Z          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e...3e+00]\n",
      "   [ 1.09905624e+01  2.23166728e+00  4.62055349e+00 ... -6.23090088e-01\n",
      "    -8.48226726e-01 -2.29984760e+00]]]])\n",
      "Z_shape    = (3, 14, 14, 8)\n",
      "_W         = array([[[[ 4.28266907e+00,  8.13360405e+00, -4.62234831e+00, ...,\n",
      "           2.07981968e+00,  2.17041516e+00, -3.88039...947495e+00, -4.97804356e+00, ...,\n",
      "          -5.69430470e-01,  4.27775383e-01, -3.84437203e+00]]]],\n",
      "      dtype=float32)\n",
      "_Z         = array([[[[ 8.82026196e+00,  2.00078607e+00,  4.89369011e+00, ...,\n",
      "          -4.88638926e+00,  4.75044203e+00, -7.56786...166728e+00,  4.62055349e+00, ...,\n",
      "          -6.23090088e-01, -8.48226726e-01, -2.29984760e+00]]]],\n",
      "      dtype=float32)\n",
      "backward   = True\n",
      "device     = cuda()\n",
      "padding    = 0\n",
      "stride     = 1\n",
      "torch      = <module 'torch' from '/opt/conda/envs/train/lib/python3.11/site-packages/torch/__init__.py'>\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:429: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:514: in conv\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Conv(stride, padding)(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e...3e+00]\n",
      "   [ 1.09905624e+01  2.23166728e+00  4.62055349e+00 ... -6.23090088e-01\n",
      "    -8.48226726e-01 -2.29984760e+00]]]])\n",
      "        b          = needle.Tensor([[[[ 4.28266907e+00  8.13360405e+00 -4.62234831e+00 ...  2.07981968e+00\n",
      "     2.17041516e+00 -3.88039827e...7e+00]\n",
      "   [-7.86027253e-01  4.30947495e+00 -4.97804356e+00 ... -5.69430470e-01\n",
      "     4.27775383e-01 -3.84437203e+00]]]])\n",
      "        padding    = 0\n",
      "        stride     = 1\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048...e+00]\n",
      "   [-7.86027253e-01  4.30947495e+00 -4.97804356e+00 ... -5.69430470e-01\n",
      "     4.27775383e-01 -3.84437203e+00]]]]))\n",
      "        self       = <needle.ops.ops_mathematic.Conv object at 0x7fd94f595a10>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048...e+00]\n",
      "   [-7.86027253e-01  4.30947495e+00 -4.97804356e+00 ... -5.69430470e-01\n",
      "     4.27775383e-01 -3.84437203e+00]]]]))\n",
      "        op         = <needle.ops.ops_mathematic.Conv object at 0x7fd94f595a10>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fd94f597bd0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fd94f597bd0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Conv object at 0x7fd94f595a10>\n",
      "A = NDArray([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e-01]\n",
      " ...9905624e+01  2.23166728e+00  4.62055349e+00 ... -6.23090088e-01\n",
      "    -8.48226726e-01 -2.29984760e+00]]]], device=cuda())\n",
      "B = NDArray([[[[ 4.28266907e+00  8.13360405e+00 -4.62234831e+00 ...  2.07981968e+00\n",
      "     2.17041516e+00 -3.88039827e-01]\n",
      " ...6027253e-01  4.30947495e+00 -4.97804356e+00 ... -5.69430470e-01\n",
      "     4.27775383e-01 -3.84437203e+00]]]], device=cuda())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, A, B):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "A          = NDArray([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e-01]\n",
      " ...9905624e+01  2.23166728e+00  4.62055349e+00 ... -6.23090088e-01\n",
      "    -8.48226726e-01 -2.29984760e+00]]]], device=cuda())\n",
      "B          = NDArray([[[[ 4.28266907e+00  8.13360405e+00 -4.62234831e+00 ...  2.07981968e+00\n",
      "     2.17041516e+00 -3.88039827e-01]\n",
      " ...6027253e-01  4.30947495e+00 -4.97804356e+00 ... -5.69430470e-01\n",
      "     4.27775383e-01 -3.84437203e+00]]]], device=cuda())\n",
      "self       = <needle.ops.ops_mathematic.Conv object at 0x7fd94f595a10>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:504: NotImplementedError\n",
      "\u001b[31m\u001b[1m_ test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape12-W_shape12-1-0] _\u001b[0m\n",
      "\n",
      "Z_shape = (3, 17, 17, 8), W_shape = (5, 5, 8, 16), stride = 1, padding = 0\n",
      "backward = True, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mimport\u001b[39;49;00m \u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "W          = needle.Tensor([[[[ 5.93117094e+00 -4.61600447e+00 -7.43411541e+00 ...  9.49053860e+00\n",
      "     6.47611380e+00 -3.38525438e...7e+00]\n",
      "   [-3.93575430e+00 -6.98609781e+00  1.63659811e+00 ...  8.22474575e+00\n",
      "     2.56555057e+00  1.63004756e+00]]]])\n",
      "W_shape    = (5, 5, 8, 16)\n",
      "Z          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e...8e+00]\n",
      "   [ 2.80155873e+00  3.11298299e+00 -3.24756050e+00 ... -7.96233535e-01\n",
      "     4.72636795e+00 -5.75466204e+00]]]])\n",
      "Z_shape    = (3, 17, 17, 8)\n",
      "_W         = array([[[[ 5.93117094e+00, -4.61600447e+00, -7.43411541e+00, ...,\n",
      "           9.49053860e+00,  6.47611380e+00, -3.38525...609781e+00,  1.63659811e+00, ...,\n",
      "           8.22474575e+00,  2.56555057e+00,  1.63004756e+00]]]],\n",
      "      dtype=float32)\n",
      "_Z         = array([[[[ 8.82026196e+00,  2.00078607e+00,  4.89369011e+00, ...,\n",
      "          -4.88638926e+00,  4.75044203e+00, -7.56786...298299e+00, -3.24756050e+00, ...,\n",
      "          -7.96233535e-01,  4.72636795e+00, -5.75466204e+00]]]],\n",
      "      dtype=float32)\n",
      "backward   = True\n",
      "device     = cuda()\n",
      "padding    = 0\n",
      "stride     = 1\n",
      "torch      = <module 'torch' from '/opt/conda/envs/train/lib/python3.11/site-packages/torch/__init__.py'>\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:429: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:514: in conv\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Conv(stride, padding)(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e...8e+00]\n",
      "   [ 2.80155873e+00  3.11298299e+00 -3.24756050e+00 ... -7.96233535e-01\n",
      "     4.72636795e+00 -5.75466204e+00]]]])\n",
      "        b          = needle.Tensor([[[[ 5.93117094e+00 -4.61600447e+00 -7.43411541e+00 ...  9.49053860e+00\n",
      "     6.47611380e+00 -3.38525438e...7e+00]\n",
      "   [-3.93575430e+00 -6.98609781e+00  1.63659811e+00 ...  8.22474575e+00\n",
      "     2.56555057e+00  1.63004756e+00]]]])\n",
      "        padding    = 0\n",
      "        stride     = 1\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048...e+00]\n",
      "   [-3.93575430e+00 -6.98609781e+00  1.63659811e+00 ...  8.22474575e+00\n",
      "     2.56555057e+00  1.63004756e+00]]]]))\n",
      "        self       = <needle.ops.ops_mathematic.Conv object at 0x7fd94f6bc510>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048...e+00]\n",
      "   [-3.93575430e+00 -6.98609781e+00  1.63659811e+00 ...  8.22474575e+00\n",
      "     2.56555057e+00  1.63004756e+00]]]]))\n",
      "        op         = <needle.ops.ops_mathematic.Conv object at 0x7fd94f6bc510>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fd94f6bc590>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fd94f6bc590>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Conv object at 0x7fd94f6bc510>\n",
      "A = NDArray([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e-01]\n",
      " ...0155873e+00  3.11298299e+00 -3.24756050e+00 ... -7.96233535e-01\n",
      "     4.72636795e+00 -5.75466204e+00]]]], device=cuda())\n",
      "B = NDArray([[[[ 5.93117094e+00 -4.61600447e+00 -7.43411541e+00 ...  9.49053860e+00\n",
      "     6.47611380e+00 -3.38525438e+00]\n",
      " ...3575430e+00 -6.98609781e+00  1.63659811e+00 ...  8.22474575e+00\n",
      "     2.56555057e+00  1.63004756e+00]]]], device=cuda())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, A, B):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "A          = NDArray([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e-01]\n",
      " ...0155873e+00  3.11298299e+00 -3.24756050e+00 ... -7.96233535e-01\n",
      "     4.72636795e+00 -5.75466204e+00]]]], device=cuda())\n",
      "B          = NDArray([[[[ 5.93117094e+00 -4.61600447e+00 -7.43411541e+00 ...  9.49053860e+00\n",
      "     6.47611380e+00 -3.38525438e+00]\n",
      " ...3575430e+00 -6.98609781e+00  1.63659811e+00 ...  8.22474575e+00\n",
      "     2.56555057e+00  1.63004756e+00]]]], device=cuda())\n",
      "self       = <needle.ops.ops_mathematic.Conv object at 0x7fd94f6bc510>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:504: NotImplementedError\n",
      "\u001b[31m\u001b[1m_ test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape13-W_shape13-1-0] _\u001b[0m\n",
      "\n",
      "Z_shape = (3, 17, 17, 1), W_shape = (5, 5, 1, 16), stride = 1, padding = 0\n",
      "backward = True, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mimport\u001b[39;49;00m \u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "W          = needle.Tensor([[[[  1.5437562   -6.8538       4.3282647    5.4068804   -3.15688\n",
      "     -1.206689    -4.3909516    3.4969...448   -2.242325     0.6578698\n",
      "     -7.0278      -1.7489109   10.11736      2.5269346    1.7962458\n",
      "     -7.9124722 ]]]])\n",
      "W_shape    = (5, 5, 1, 16)\n",
      "Z          = needle.Tensor([[[[ 8.82026196e+00]\n",
      "   [ 2.00078607e+00]\n",
      "   [ 4.89369011e+00]\n",
      "   [ 1.12044659e+01]\n",
      "   [ 9.33778954e+00]...22066e-01]\n",
      "   [ 1.05310105e-01]\n",
      "   [ 4.97272283e-01]\n",
      "   [ 1.13696384e+00]\n",
      "   [-5.08369303e+00]\n",
      "   [-5.73876619e-01]]]])\n",
      "Z_shape    = (3, 17, 17, 1)\n",
      "_W         = array([[[[  1.5437562 ,  -6.8538    ,   4.3282647 ,   5.4068804 ,\n",
      "           -3.15688   ,  -1.206689  ,  -4.3909516 , ...  -7.0278    ,  -1.7489109 ,\n",
      "           10.11736   ,   2.5269346 ,   1.7962458 ,  -7.9124722 ]]]],\n",
      "      dtype=float32)\n",
      "_Z         = array([[[[ 8.82026196e+00],\n",
      "         [ 2.00078607e+00],\n",
      "         [ 4.89369011e+00],\n",
      "         [ 1.12044659e+01],\n",
      "      ... 4.97272283e-01],\n",
      "         [ 1.13696384e+00],\n",
      "         [-5.08369303e+00],\n",
      "         [-5.73876619e-01]]]], dtype=float32)\n",
      "backward   = True\n",
      "device     = cuda()\n",
      "padding    = 0\n",
      "stride     = 1\n",
      "torch      = <module 'torch' from '/opt/conda/envs/train/lib/python3.11/site-packages/torch/__init__.py'>\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:429: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:514: in conv\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Conv(stride, padding)(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[[ 8.82026196e+00]\n",
      "   [ 2.00078607e+00]\n",
      "   [ 4.89369011e+00]\n",
      "   [ 1.12044659e+01]\n",
      "   [ 9.33778954e+00]...22066e-01]\n",
      "   [ 1.05310105e-01]\n",
      "   [ 4.97272283e-01]\n",
      "   [ 1.13696384e+00]\n",
      "   [-5.08369303e+00]\n",
      "   [-5.73876619e-01]]]])\n",
      "        b          = needle.Tensor([[[[  1.5437562   -6.8538       4.3282647    5.4068804   -3.15688\n",
      "     -1.206689    -4.3909516    3.4969...448   -2.242325     0.6578698\n",
      "     -7.0278      -1.7489109   10.11736      2.5269346    1.7962458\n",
      "     -7.9124722 ]]]])\n",
      "        padding    = 0\n",
      "        stride     = 1\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[ 8.82026196e+00]\n",
      "   [ 2.00078607e+00]\n",
      "   [ 4.89369011e+00]\n",
      "   [ 1.12044659e+01]\n",
      "   [ 9.33778954e+00...48   -2.242325     0.6578698\n",
      "     -7.0278      -1.7489109   10.11736      2.5269346    1.7962458\n",
      "     -7.9124722 ]]]]))\n",
      "        self       = <needle.ops.ops_mathematic.Conv object at 0x7fd94f5e1e10>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[[ 8.82026196e+00]\n",
      "   [ 2.00078607e+00]\n",
      "   [ 4.89369011e+00]\n",
      "   [ 1.12044659e+01]\n",
      "   [ 9.33778954e+00...48   -2.242325     0.6578698\n",
      "     -7.0278      -1.7489109   10.11736      2.5269346    1.7962458\n",
      "     -7.9124722 ]]]]))\n",
      "        op         = <needle.ops.ops_mathematic.Conv object at 0x7fd94f5e1e10>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fd94f5e1f90>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fd94f5e1f90>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Conv object at 0x7fd94f5e1e10>\n",
      "A = NDArray([[[[ 8.82026196e+00]\n",
      "   [ 2.00078607e+00]\n",
      "   [ 4.89369011e+00]\n",
      "   [ 1.12044659e+01]\n",
      "   [ 9.33778954e+00]\n",
      "   [-... 1.05310105e-01]\n",
      "   [ 4.97272283e-01]\n",
      "   [ 1.13696384e+00]\n",
      "   [-5.08369303e+00]\n",
      "   [-5.73876619e-01]]]], device=cuda())\n",
      "B = NDArray([[[[  1.5437562   -6.8538       4.3282647    5.4068804   -3.15688\n",
      "     -1.206689    -4.3909516    3.4969025   ...     0.6578698\n",
      "     -7.0278      -1.7489109   10.11736      2.5269346    1.7962458\n",
      "     -7.9124722 ]]]], device=cuda())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, A, B):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "A          = NDArray([[[[ 8.82026196e+00]\n",
      "   [ 2.00078607e+00]\n",
      "   [ 4.89369011e+00]\n",
      "   [ 1.12044659e+01]\n",
      "   [ 9.33778954e+00]\n",
      "   [-... 1.05310105e-01]\n",
      "   [ 4.97272283e-01]\n",
      "   [ 1.13696384e+00]\n",
      "   [-5.08369303e+00]\n",
      "   [-5.73876619e-01]]]], device=cuda())\n",
      "B          = NDArray([[[[  1.5437562   -6.8538       4.3282647    5.4068804   -3.15688\n",
      "     -1.206689    -4.3909516    3.4969025   ...     0.6578698\n",
      "     -7.0278      -1.7489109   10.11736      2.5269346    1.7962458\n",
      "     -7.9124722 ]]]], device=cuda())\n",
      "self       = <needle.ops.ops_mathematic.Conv object at 0x7fd94f5e1e10>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:504: NotImplementedError\n",
      "\u001b[31m\u001b[1m_ test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape14-W_shape14-1-0] _\u001b[0m\n",
      "\n",
      "Z_shape = (3, 17, 17, 16), W_shape = (5, 5, 16, 1), stride = 1, padding = 0\n",
      "backward = True, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mimport\u001b[39;49;00m \u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "W          = needle.Tensor([[[[ 1.55579513e-02]\n",
      "   [-3.04658723e+00]\n",
      "   [ 4.09725952e+00]\n",
      "   [ 1.29906273e+00]\n",
      "   [-7.50546598e+00]...75421e+00]\n",
      "   [ 3.02805209e+00]\n",
      "   [-4.20992136e+00]\n",
      "   [ 4.11707735e+00]\n",
      "   [ 4.99041653e+00]\n",
      "   [ 5.11040497e+00]]]])\n",
      "W_shape    = (5, 5, 16, 1)\n",
      "Z          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ...  6.08375072e-01\n",
      "     2.21931624e+00  1.66837168e...3e+00]\n",
      "   [-1.25365603e+00 -1.71452928e+00  1.35930243e+01 ...  3.13170171e+00\n",
      "    -1.35708165e+00  1.29997072e+01]]]])\n",
      "Z_shape    = (3, 17, 17, 16)\n",
      "_W         = array([[[[ 1.55579513e-02],\n",
      "         [-3.04658723e+00],\n",
      "         [ 4.09725952e+00],\n",
      "         [ 1.29906273e+00],\n",
      "      ...-4.20992136e+00],\n",
      "         [ 4.11707735e+00],\n",
      "         [ 4.99041653e+00],\n",
      "         [ 5.11040497e+00]]]], dtype=float32)\n",
      "_Z         = array([[[[ 8.82026196e+00,  2.00078607e+00,  4.89369011e+00, ...,\n",
      "           6.08375072e-01,  2.21931624e+00,  1.66837...452928e+00,  1.35930243e+01, ...,\n",
      "           3.13170171e+00, -1.35708165e+00,  1.29997072e+01]]]],\n",
      "      dtype=float32)\n",
      "backward   = True\n",
      "device     = cuda()\n",
      "padding    = 0\n",
      "stride     = 1\n",
      "torch      = <module 'torch' from '/opt/conda/envs/train/lib/python3.11/site-packages/torch/__init__.py'>\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:429: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:514: in conv\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Conv(stride, padding)(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ...  6.08375072e-01\n",
      "     2.21931624e+00  1.66837168e...3e+00]\n",
      "   [-1.25365603e+00 -1.71452928e+00  1.35930243e+01 ...  3.13170171e+00\n",
      "    -1.35708165e+00  1.29997072e+01]]]])\n",
      "        b          = needle.Tensor([[[[ 1.55579513e-02]\n",
      "   [-3.04658723e+00]\n",
      "   [ 4.09725952e+00]\n",
      "   [ 1.29906273e+00]\n",
      "   [-7.50546598e+00]...75421e+00]\n",
      "   [ 3.02805209e+00]\n",
      "   [-4.20992136e+00]\n",
      "   [ 4.11707735e+00]\n",
      "   [ 4.99041653e+00]\n",
      "   [ 5.11040497e+00]]]])\n",
      "        padding    = 0\n",
      "        stride     = 1\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ...  6.08375072e-01\n",
      "     2.21931624e+00  1.66837168...5421e+00]\n",
      "   [ 3.02805209e+00]\n",
      "   [-4.20992136e+00]\n",
      "   [ 4.11707735e+00]\n",
      "   [ 4.99041653e+00]\n",
      "   [ 5.11040497e+00]]]]))\n",
      "        self       = <needle.ops.ops_mathematic.Conv object at 0x7fd94f5991d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ...  6.08375072e-01\n",
      "     2.21931624e+00  1.66837168...5421e+00]\n",
      "   [ 3.02805209e+00]\n",
      "   [-4.20992136e+00]\n",
      "   [ 4.11707735e+00]\n",
      "   [ 4.99041653e+00]\n",
      "   [ 5.11040497e+00]]]]))\n",
      "        op         = <needle.ops.ops_mathematic.Conv object at 0x7fd94f5991d0>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fd94f59a690>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fd94f59a690>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Conv object at 0x7fd94f5991d0>\n",
      "A = NDArray([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ...  6.08375072e-01\n",
      "     2.21931624e+00  1.66837168e+00]\n",
      " ...5365603e+00 -1.71452928e+00  1.35930243e+01 ...  3.13170171e+00\n",
      "    -1.35708165e+00  1.29997072e+01]]]], device=cuda())\n",
      "B = NDArray([[[[ 1.55579513e-02]\n",
      "   [-3.04658723e+00]\n",
      "   [ 4.09725952e+00]\n",
      "   [ 1.29906273e+00]\n",
      "   [-7.50546598e+00]\n",
      "   [-... 3.02805209e+00]\n",
      "   [-4.20992136e+00]\n",
      "   [ 4.11707735e+00]\n",
      "   [ 4.99041653e+00]\n",
      "   [ 5.11040497e+00]]]], device=cuda())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, A, B):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "A          = NDArray([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ...  6.08375072e-01\n",
      "     2.21931624e+00  1.66837168e+00]\n",
      " ...5365603e+00 -1.71452928e+00  1.35930243e+01 ...  3.13170171e+00\n",
      "    -1.35708165e+00  1.29997072e+01]]]], device=cuda())\n",
      "B          = NDArray([[[[ 1.55579513e-02]\n",
      "   [-3.04658723e+00]\n",
      "   [ 4.09725952e+00]\n",
      "   [ 1.29906273e+00]\n",
      "   [-7.50546598e+00]\n",
      "   [-... 3.02805209e+00]\n",
      "   [-4.20992136e+00]\n",
      "   [ 4.11707735e+00]\n",
      "   [ 4.99041653e+00]\n",
      "   [ 5.11040497e+00]]]], device=cuda())\n",
      "self       = <needle.ops.ops_mathematic.Conv object at 0x7fd94f5991d0>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:504: NotImplementedError\n",
      "\u001b[31m\u001b[1m_ test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape15-W_shape15-1-0] _\u001b[0m\n",
      "\n",
      "Z_shape = (3, 17, 17, 16), W_shape = (1, 1, 16, 1), stride = 1, padding = 0\n",
      "backward = True, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mimport\u001b[39;49;00m \u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "W          = needle.Tensor([[[[ 0.01555795]\n",
      "   [-3.0465872 ]\n",
      "   [ 4.0972595 ]\n",
      "   [ 1.2990627 ]\n",
      "   [-7.505466  ]\n",
      "   [-1.7335238 ]\n",
      "  ...[ 5.6893935 ]\n",
      "   [ 2.267663  ]\n",
      "   [-1.0709513 ]\n",
      "   [-5.0566583 ]\n",
      "   [-0.23777105]\n",
      "   [-9.931785  ]\n",
      "   [-0.44905776]]]])\n",
      "W_shape    = (1, 1, 16, 1)\n",
      "Z          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ...  6.08375072e-01\n",
      "     2.21931624e+00  1.66837168e...3e+00]\n",
      "   [-1.25365603e+00 -1.71452928e+00  1.35930243e+01 ...  3.13170171e+00\n",
      "    -1.35708165e+00  1.29997072e+01]]]])\n",
      "Z_shape    = (3, 17, 17, 16)\n",
      "_W         = array([[[[ 0.01555795],\n",
      "         [-3.0465872 ],\n",
      "         [ 4.0972595 ],\n",
      "         [ 1.2990627 ],\n",
      "         [-7.505466  ]...13 ],\n",
      "         [-5.0566583 ],\n",
      "         [-0.23777105],\n",
      "         [-9.931785  ],\n",
      "         [-0.44905776]]]], dtype=float32)\n",
      "_Z         = array([[[[ 8.82026196e+00,  2.00078607e+00,  4.89369011e+00, ...,\n",
      "           6.08375072e-01,  2.21931624e+00,  1.66837...452928e+00,  1.35930243e+01, ...,\n",
      "           3.13170171e+00, -1.35708165e+00,  1.29997072e+01]]]],\n",
      "      dtype=float32)\n",
      "backward   = True\n",
      "device     = cuda()\n",
      "padding    = 0\n",
      "stride     = 1\n",
      "torch      = <module 'torch' from '/opt/conda/envs/train/lib/python3.11/site-packages/torch/__init__.py'>\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:429: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:514: in conv\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Conv(stride, padding)(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ...  6.08375072e-01\n",
      "     2.21931624e+00  1.66837168e...3e+00]\n",
      "   [-1.25365603e+00 -1.71452928e+00  1.35930243e+01 ...  3.13170171e+00\n",
      "    -1.35708165e+00  1.29997072e+01]]]])\n",
      "        b          = needle.Tensor([[[[ 0.01555795]\n",
      "   [-3.0465872 ]\n",
      "   [ 4.0972595 ]\n",
      "   [ 1.2990627 ]\n",
      "   [-7.505466  ]\n",
      "   [-1.7335238 ]\n",
      "  ...[ 5.6893935 ]\n",
      "   [ 2.267663  ]\n",
      "   [-1.0709513 ]\n",
      "   [-5.0566583 ]\n",
      "   [-0.23777105]\n",
      "   [-9.931785  ]\n",
      "   [-0.44905776]]]])\n",
      "        padding    = 0\n",
      "        stride     = 1\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ...  6.08375072e-01\n",
      "     2.21931624e+00  1.66837168... 5.6893935 ]\n",
      "   [ 2.267663  ]\n",
      "   [-1.0709513 ]\n",
      "   [-5.0566583 ]\n",
      "   [-0.23777105]\n",
      "   [-9.931785  ]\n",
      "   [-0.44905776]]]]))\n",
      "        self       = <needle.ops.ops_mathematic.Conv object at 0x7fd94f578cd0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ...  6.08375072e-01\n",
      "     2.21931624e+00  1.66837168... 5.6893935 ]\n",
      "   [ 2.267663  ]\n",
      "   [-1.0709513 ]\n",
      "   [-5.0566583 ]\n",
      "   [-0.23777105]\n",
      "   [-9.931785  ]\n",
      "   [-0.44905776]]]]))\n",
      "        op         = <needle.ops.ops_mathematic.Conv object at 0x7fd94f578cd0>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fd94f57a190>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fd94f57a190>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Conv object at 0x7fd94f578cd0>\n",
      "A = NDArray([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ...  6.08375072e-01\n",
      "     2.21931624e+00  1.66837168e+00]\n",
      " ...5365603e+00 -1.71452928e+00  1.35930243e+01 ...  3.13170171e+00\n",
      "    -1.35708165e+00  1.29997072e+01]]]], device=cuda())\n",
      "B = NDArray([[[[ 0.01555795]\n",
      "   [-3.0465872 ]\n",
      "   [ 4.0972595 ]\n",
      "   [ 1.2990627 ]\n",
      "   [-7.505466  ]\n",
      "   [-1.7335238 ]\n",
      "   [-2.5...  [ 2.267663  ]\n",
      "   [-1.0709513 ]\n",
      "   [-5.0566583 ]\n",
      "   [-0.23777105]\n",
      "   [-9.931785  ]\n",
      "   [-0.44905776]]]], device=cuda())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, A, B):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "A          = NDArray([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ...  6.08375072e-01\n",
      "     2.21931624e+00  1.66837168e+00]\n",
      " ...5365603e+00 -1.71452928e+00  1.35930243e+01 ...  3.13170171e+00\n",
      "    -1.35708165e+00  1.29997072e+01]]]], device=cuda())\n",
      "B          = NDArray([[[[ 0.01555795]\n",
      "   [-3.0465872 ]\n",
      "   [ 4.0972595 ]\n",
      "   [ 1.2990627 ]\n",
      "   [-7.505466  ]\n",
      "   [-1.7335238 ]\n",
      "   [-2.5...  [ 2.267663  ]\n",
      "   [-1.0709513 ]\n",
      "   [-5.0566583 ]\n",
      "   [-0.23777105]\n",
      "   [-9.931785  ]\n",
      "   [-0.44905776]]]], device=cuda())\n",
      "self       = <needle.ops.ops_mathematic.Conv object at 0x7fd94f578cd0>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:504: NotImplementedError\n",
      "\u001b[31m\u001b[1m_ test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape16-W_shape16-1-0] _\u001b[0m\n",
      "\n",
      "Z_shape = (1, 14, 14, 2), W_shape = (3, 3, 2, 2), stride = 1, padding = 0\n",
      "backward = True, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mimport\u001b[39;49;00m \u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "W          = needle.Tensor([[[[ -1.7671587   -8.082371  ]\n",
      "   [ -1.4591868   -3.807461  ]]\n",
      "\n",
      "  [[  4.2896194    5.705509  ]\n",
      "   [  7.3...7526     2.911123  ]\n",
      "   [-10.473016     0.61860955]]\n",
      "\n",
      "  [[ -0.65053475   0.46976614]\n",
      "   [  4.7152305  -13.698386  ]]]])\n",
      "W_shape    = (3, 3, 2, 2)\n",
      "Z          = needle.Tensor([[[[  8.820262     2.000786  ]\n",
      "   [  4.89369     11.204466  ]\n",
      "   [  9.33779     -4.8863893 ]\n",
      "   [  4.750...19315276  -8.283575  ]\n",
      "   [ -4.9275537   -7.359175  ]\n",
      "   [  8.240675     0.8211388 ]\n",
      "   [  2.8364513   -1.1133755 ]]]])\n",
      "Z_shape    = (1, 14, 14, 2)\n",
      "_W         = array([[[[ -1.7671587 ,  -8.082371  ],\n",
      "         [ -1.4591868 ,  -3.807461  ]],\n",
      "\n",
      "        [[  4.2896194 ,   5.705509  ],...016  ,   0.61860955]],\n",
      "\n",
      "        [[ -0.65053475,   0.46976614],\n",
      "         [  4.7152305 , -13.698386  ]]]], dtype=float32)\n",
      "_Z         = array([[[[  8.820262  ,   2.000786  ],\n",
      "         [  4.89369   ,  11.204466  ],\n",
      "         [  9.33779   ,  -4.8863893 ],\n",
      " ...275537 ,  -7.359175  ],\n",
      "         [  8.240675  ,   0.8211388 ],\n",
      "         [  2.8364513 ,  -1.1133755 ]]]], dtype=float32)\n",
      "backward   = True\n",
      "device     = cuda()\n",
      "padding    = 0\n",
      "stride     = 1\n",
      "torch      = <module 'torch' from '/opt/conda/envs/train/lib/python3.11/site-packages/torch/__init__.py'>\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:429: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:514: in conv\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Conv(stride, padding)(a, b)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[[  8.820262     2.000786  ]\n",
      "   [  4.89369     11.204466  ]\n",
      "   [  9.33779     -4.8863893 ]\n",
      "   [  4.750...19315276  -8.283575  ]\n",
      "   [ -4.9275537   -7.359175  ]\n",
      "   [  8.240675     0.8211388 ]\n",
      "   [  2.8364513   -1.1133755 ]]]])\n",
      "        b          = needle.Tensor([[[[ -1.7671587   -8.082371  ]\n",
      "   [ -1.4591868   -3.807461  ]]\n",
      "\n",
      "  [[  4.2896194    5.705509  ]\n",
      "   [  7.3...7526     2.911123  ]\n",
      "   [-10.473016     0.61860955]]\n",
      "\n",
      "  [[ -0.65053475   0.46976614]\n",
      "   [  4.7152305  -13.698386  ]]]])\n",
      "        padding    = 0\n",
      "        stride     = 1\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:83: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[  8.820262     2.000786  ]\n",
      "   [  4.89369     11.204466  ]\n",
      "   [  9.33779     -4.8863893 ]\n",
      "   [  4.75...526     2.911123  ]\n",
      "   [-10.473016     0.61860955]]\n",
      "\n",
      "  [[ -0.65053475   0.46976614]\n",
      "   [  4.7152305  -13.698386  ]]]]))\n",
      "        self       = <needle.ops.ops_mathematic.Conv object at 0x7fd94f524a50>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:245: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[[  8.820262     2.000786  ]\n",
      "   [  4.89369     11.204466  ]\n",
      "   [  9.33779     -4.8863893 ]\n",
      "   [  4.75...526     2.911123  ]\n",
      "   [-10.473016     0.61860955]]\n",
      "\n",
      "  [[ -0.65053475   0.46976614]\n",
      "   [  4.7152305  -13.698386  ]]]]))\n",
      "        op         = <needle.ops.ops_mathematic.Conv object at 0x7fd94f524a50>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7fd94f524ad0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:110: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7fd94f524ad0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Conv object at 0x7fd94f524a50>\n",
      "A = NDArray([[[[  8.820262     2.000786  ]\n",
      "   [  4.89369     11.204466  ]\n",
      "   [  9.33779     -4.8863893 ]\n",
      "   [  4.750442   ...3575  ]\n",
      "   [ -4.9275537   -7.359175  ]\n",
      "   [  8.240675     0.8211388 ]\n",
      "   [  2.8364513   -1.1133755 ]]]], device=cuda())\n",
      "B = NDArray([[[[ -1.7671587   -8.082371  ]\n",
      "   [ -1.4591868   -3.807461  ]]\n",
      "\n",
      "  [[  4.2896194    5.705509  ]\n",
      "   [  7.3328934...23  ]\n",
      "   [-10.473016     0.61860955]]\n",
      "\n",
      "  [[ -0.65053475   0.46976614]\n",
      "   [  4.7152305  -13.698386  ]]]], device=cuda())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, A, B):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "A          = NDArray([[[[  8.820262     2.000786  ]\n",
      "   [  4.89369     11.204466  ]\n",
      "   [  9.33779     -4.8863893 ]\n",
      "   [  4.750442   ...3575  ]\n",
      "   [ -4.9275537   -7.359175  ]\n",
      "   [  8.240675     0.8211388 ]\n",
      "   [  2.8364513   -1.1133755 ]]]], device=cuda())\n",
      "B          = NDArray([[[[ -1.7671587   -8.082371  ]\n",
      "   [ -1.4591868   -3.807461  ]]\n",
      "\n",
      "  [[  4.2896194    5.705509  ]\n",
      "   [  7.3328934...23  ]\n",
      "   [-10.473016     0.61860955]]\n",
      "\n",
      "  [[ -0.65053475   0.46976614]\n",
      "   [  4.7152305  -13.698386  ]]]], device=cuda())\n",
      "self       = <needle.ops.ops_mathematic.Conv object at 0x7fd94f524a50>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:504: NotImplementedError\n",
      "\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape0-W_shape0-1-0]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape1-W_shape1-1-1]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape2-W_shape2-1-2]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape3-W_shape3-1-0]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape4-W_shape4-1-0]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape5-W_shape5-2-0]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape6-W_shape6-2-1]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape7-W_shape7-2-2]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape8-W_shape8-2-0]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape9-W_shape9-2-0]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape10-W_shape10-1-0]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape11-W_shape11-1-0]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape12-W_shape12-1-0]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape13-W_shape13-1-0]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape14-W_shape14-1-0]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape15-W_shape15-1-0]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape16-W_shape16-1-0]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape0-W_shape0-1-0]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape1-W_shape1-1-1]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape2-W_shape2-1-2]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape3-W_shape3-1-0]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape4-W_shape4-1-0]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape5-W_shape5-2-0]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape6-W_shape6-2-1]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape7-W_shape7-2-2]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape8-W_shape8-2-0]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape9-W_shape9-2-0]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape10-W_shape10-1-0]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape11-W_shape11-1-0]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape12-W_shape12-1-0]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape13-W_shape13-1-0]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape14-W_shape14-1-0]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape15-W_shape15-1-0]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape16-W_shape16-1-0]\u001b[0m - NotImplementedError\n",
      "\u001b[31m===================== \u001b[31m\u001b[1m34 failed\u001b[0m, \u001b[33m1769 deselected\u001b[0m\u001b[31m in 5.00s\u001b[0m\u001b[31m ======================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m pytest -l -v -k \"op_conv and backward\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.Conv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Fixing init._calculate_fans for convolution\n",
    "Previously, we have implemented Kaiming uniform/normal initializations, where we essentially assigned `fan_in = input_size` and `fan_out = output_size`.\n",
    "For convolution, this becomes somewhat more detailed, in that you should multiply both of these by the \"receptive field size\", which is in this case just the product of the kernel sizes -- which in our case are always going to be the same, i.e., $k\\times k$ kernels.\n",
    "\n",
    "**You will need to edit your `kaiming_uniform` in `python/needle/init/init_initializers.py`, etc. init functions to support multidimensional arrays.** In particular, it should support a new `shape` argument which is then passed to, e.g., the underlying `rand` function. Specifically, if the argument `shape` is not `None`, then ignore `fan_in` and `fan_out`, and use the value of `shape` for initializations instead.\n",
    "\n",
    "You can test this below; though it is not _directly_ graded, it must match ours to pass the nn.Conv mugrade tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.11.10, pytest-8.3.4, pluggy-1.5.0 -- /opt/conda/envs/train/bin/python\n",
      "cachedir: .pytest_cache\n",
      "hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase(PosixPath('/mnt/zhumingzhu/work/00test/hw4/.hypothesis/examples'))\n",
      "rootdir: /mnt/zhumingzhu/work/00test/hw4\n",
      "plugins: hypothesis-6.115.5\n",
      "collected 1803 items / 1801 deselected / 2 selected                            \u001b[0m\u001b[1m\n",
      "\n",
      "tests/hw4/test_conv.py::test_init_kaiming_uniform[needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [ 50%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_init_kaiming_uniform[needle.backend_ndarray.ndarray_backend_cuda] \u001b[32mPASSED\u001b[0m\u001b[32m [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m====================== \u001b[32m\u001b[1m2 passed\u001b[0m, \u001b[33m1801 deselected\u001b[0m\u001b[32m in 2.55s\u001b[0m\u001b[32m ======================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m pytest -l -v -k \"kaiming_uniform\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementing nn.Conv\n",
    "\n",
    "Essentially, nn.Conv is just a wrapper of the convolution operator we previously implemented\n",
    "which adds a bias term, initializes the weight and bias, and ensures that the padding is set so that the input and output dimensions are the same (in the `stride=1` case, anyways). \n",
    "\n",
    "Importantly, nn.Conv should support NCHW format instead of NHWC format. In particular, we think this makes more sense given our current BatchNorm implementation. You can implement this by applying `transpose` twice to both the input and output.  \n",
    "\n",
    "- Ensure nn.Conv works for `(N, C, H, W)` tensors even though we implemented the conv op for `(N, H, W, C)` tensors\n",
    "- Initialize the `(k, k, i, o)` weight tensor using Kaiming uniform initialization with default settings\n",
    "- Initialize the `(o,)` bias tensor using uniform initialization on the interval $\\displaystyle\\pm\\frac{1}{\\sqrt{\\verb|in_channels| \\times \\verb|kernel_size|^2}}$\n",
    "- Calculate the appropriate padding to ensure input and output dimensions are the same\n",
    "- Calculate the convolution, then add the properly-broadcasted bias term if present\n",
    "\n",
    "You can now test your nn.Conv against PyTorch's nn.Conv2d with the two PyTest calls below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.11.10, pytest-8.3.4, pluggy-1.5.0 -- /opt/conda/envs/train/bin/python\n",
      "cachedir: .pytest_cache\n",
      "hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase(PosixPath('/mnt/zhumingzhu/work/00test/hw4/.hypothesis/examples'))\n",
      "rootdir: /mnt/zhumingzhu/work/00test/hw4\n",
      "plugins: hypothesis-6.115.5\n",
      "collected 1803 items / 1793 deselected / 10 selected                           \u001b[0m\u001b[1m\n",
      "\n",
      "tests/hw4/test_conv.py::test_nn_conv_forward[needle.backend_ndarray.ndarray_backend_cpu-4-8-16-3-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 10%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_nn_conv_forward[needle.backend_ndarray.ndarray_backend_cpu-32-8-16-3-2] \u001b[31mFAILED\u001b[0m\u001b[31m [ 20%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_nn_conv_forward[needle.backend_ndarray.ndarray_backend_cpu-32-8-8-3-2] \u001b[31mFAILED\u001b[0m\u001b[31m [ 30%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_nn_conv_forward[needle.backend_ndarray.ndarray_backend_cpu-32-16-8-3-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 40%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_nn_conv_forward[needle.backend_ndarray.ndarray_backend_cpu-32-16-8-3-2] \u001b[31mFAILED\u001b[0m\u001b[31m [ 50%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_nn_conv_forward[needle.backend_ndarray.ndarray_backend_cuda-4-8-16-3-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 60%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_nn_conv_forward[needle.backend_ndarray.ndarray_backend_cuda-32-8-16-3-2] \u001b[31mFAILED\u001b[0m\u001b[31m [ 70%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_nn_conv_forward[needle.backend_ndarray.ndarray_backend_cuda-32-8-8-3-2] \u001b[31mFAILED\u001b[0m\u001b[31m [ 80%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_nn_conv_forward[needle.backend_ndarray.ndarray_backend_cuda-32-16-8-3-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 90%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_nn_conv_forward[needle.backend_ndarray.ndarray_backend_cuda-32-16-8-3-2] \u001b[31mFAILED\u001b[0m\u001b[31m [100%]\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m_ test_nn_conv_forward[needle.backend_ndarray.ndarray_backend_cpu-4-8-16-3-1] __\u001b[0m\n",
      "\n",
      "s = 4, cin = 8, cout = 16, k = 3, stride = 1, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33ms,cin,cout,k,stride\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, conv_forward_params)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_nn_conv_forward\u001b[39;49;00m(s, cin, cout, k, stride, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mimport\u001b[39;49;00m \u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       f = ndl.nn.Conv(cin, cout, k, stride=stride, device=device)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "cin        = 8\n",
      "cout       = 16\n",
      "device     = cpu()\n",
      "k          = 3\n",
      "s          = 4\n",
      "stride     = 1\n",
      "torch      = <module 'torch' from '/opt/conda/envs/train/lib/python3.11/site-packages/torch/__init__.py'>\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:349: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.nn.nn_conv.Conv object at 0x7f9624b3c890>, in_channels = 8\n",
      "out_channels = 16, kernel_size = 3, stride = 1, bias = True, device = cpu()\n",
      "dtype = 'float32'\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92m__init__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, in_channels, out_channels, kernel_size, stride=\u001b[94m1\u001b[39;49;00m, bias=\u001b[94mTrue\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, dtype=\u001b[33m\"\u001b[39;49;00m\u001b[33mfloat32\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96msuper\u001b[39;49;00m().\u001b[92m__init__\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(kernel_size, \u001b[96mtuple\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      "            kernel_size = kernel_size[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(stride, \u001b[96mtuple\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      "            stride = stride[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m.in_channels = in_channels\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m.out_channels = out_channels\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m.kernel_size = kernel_size\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m.stride = stride\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "__class__  = <class 'needle.nn.nn_conv.Conv'>\n",
      "bias       = True\n",
      "device     = cpu()\n",
      "dtype      = 'float32'\n",
      "in_channels = 8\n",
      "kernel_size = 3\n",
      "out_channels = 16\n",
      "self       = <needle.nn.nn_conv.Conv object at 0x7f9624b3c890>\n",
      "stride     = 1\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_conv.py\u001b[0m:31: NotImplementedError\n",
      "\u001b[31m\u001b[1m_ test_nn_conv_forward[needle.backend_ndarray.ndarray_backend_cpu-32-8-16-3-2] _\u001b[0m\n",
      "\n",
      "s = 32, cin = 8, cout = 16, k = 3, stride = 2, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33ms,cin,cout,k,stride\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, conv_forward_params)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_nn_conv_forward\u001b[39;49;00m(s, cin, cout, k, stride, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mimport\u001b[39;49;00m \u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       f = ndl.nn.Conv(cin, cout, k, stride=stride, device=device)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "cin        = 8\n",
      "cout       = 16\n",
      "device     = cpu()\n",
      "k          = 3\n",
      "s          = 32\n",
      "stride     = 2\n",
      "torch      = <module 'torch' from '/opt/conda/envs/train/lib/python3.11/site-packages/torch/__init__.py'>\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:349: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.nn.nn_conv.Conv object at 0x7f912490d7d0>, in_channels = 8\n",
      "out_channels = 16, kernel_size = 3, stride = 2, bias = True, device = cpu()\n",
      "dtype = 'float32'\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92m__init__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, in_channels, out_channels, kernel_size, stride=\u001b[94m1\u001b[39;49;00m, bias=\u001b[94mTrue\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, dtype=\u001b[33m\"\u001b[39;49;00m\u001b[33mfloat32\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96msuper\u001b[39;49;00m().\u001b[92m__init__\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(kernel_size, \u001b[96mtuple\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      "            kernel_size = kernel_size[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(stride, \u001b[96mtuple\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      "            stride = stride[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m.in_channels = in_channels\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m.out_channels = out_channels\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m.kernel_size = kernel_size\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m.stride = stride\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "__class__  = <class 'needle.nn.nn_conv.Conv'>\n",
      "bias       = True\n",
      "device     = cpu()\n",
      "dtype      = 'float32'\n",
      "in_channels = 8\n",
      "kernel_size = 3\n",
      "out_channels = 16\n",
      "self       = <needle.nn.nn_conv.Conv object at 0x7f912490d7d0>\n",
      "stride     = 2\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_conv.py\u001b[0m:31: NotImplementedError\n",
      "\u001b[31m\u001b[1m_ test_nn_conv_forward[needle.backend_ndarray.ndarray_backend_cpu-32-8-8-3-2] __\u001b[0m\n",
      "\n",
      "s = 32, cin = 8, cout = 8, k = 3, stride = 2, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33ms,cin,cout,k,stride\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, conv_forward_params)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_nn_conv_forward\u001b[39;49;00m(s, cin, cout, k, stride, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mimport\u001b[39;49;00m \u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       f = ndl.nn.Conv(cin, cout, k, stride=stride, device=device)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "cin        = 8\n",
      "cout       = 8\n",
      "device     = cpu()\n",
      "k          = 3\n",
      "s          = 32\n",
      "stride     = 2\n",
      "torch      = <module 'torch' from '/opt/conda/envs/train/lib/python3.11/site-packages/torch/__init__.py'>\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:349: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.nn.nn_conv.Conv object at 0x7f91246241d0>, in_channels = 8\n",
      "out_channels = 8, kernel_size = 3, stride = 2, bias = True, device = cpu()\n",
      "dtype = 'float32'\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92m__init__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, in_channels, out_channels, kernel_size, stride=\u001b[94m1\u001b[39;49;00m, bias=\u001b[94mTrue\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, dtype=\u001b[33m\"\u001b[39;49;00m\u001b[33mfloat32\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96msuper\u001b[39;49;00m().\u001b[92m__init__\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(kernel_size, \u001b[96mtuple\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      "            kernel_size = kernel_size[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(stride, \u001b[96mtuple\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      "            stride = stride[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m.in_channels = in_channels\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m.out_channels = out_channels\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m.kernel_size = kernel_size\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m.stride = stride\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "__class__  = <class 'needle.nn.nn_conv.Conv'>\n",
      "bias       = True\n",
      "device     = cpu()\n",
      "dtype      = 'float32'\n",
      "in_channels = 8\n",
      "kernel_size = 3\n",
      "out_channels = 8\n",
      "self       = <needle.nn.nn_conv.Conv object at 0x7f91246241d0>\n",
      "stride     = 2\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_conv.py\u001b[0m:31: NotImplementedError\n",
      "\u001b[31m\u001b[1m_ test_nn_conv_forward[needle.backend_ndarray.ndarray_backend_cpu-32-16-8-3-1] _\u001b[0m\n",
      "\n",
      "s = 32, cin = 16, cout = 8, k = 3, stride = 1, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33ms,cin,cout,k,stride\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, conv_forward_params)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_nn_conv_forward\u001b[39;49;00m(s, cin, cout, k, stride, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mimport\u001b[39;49;00m \u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       f = ndl.nn.Conv(cin, cout, k, stride=stride, device=device)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "cin        = 16\n",
      "cout       = 8\n",
      "device     = cpu()\n",
      "k          = 3\n",
      "s          = 32\n",
      "stride     = 1\n",
      "torch      = <module 'torch' from '/opt/conda/envs/train/lib/python3.11/site-packages/torch/__init__.py'>\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:349: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.nn.nn_conv.Conv object at 0x7f9124637810>, in_channels = 16\n",
      "out_channels = 8, kernel_size = 3, stride = 1, bias = True, device = cpu()\n",
      "dtype = 'float32'\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92m__init__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, in_channels, out_channels, kernel_size, stride=\u001b[94m1\u001b[39;49;00m, bias=\u001b[94mTrue\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, dtype=\u001b[33m\"\u001b[39;49;00m\u001b[33mfloat32\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96msuper\u001b[39;49;00m().\u001b[92m__init__\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(kernel_size, \u001b[96mtuple\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      "            kernel_size = kernel_size[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(stride, \u001b[96mtuple\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      "            stride = stride[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m.in_channels = in_channels\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m.out_channels = out_channels\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m.kernel_size = kernel_size\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m.stride = stride\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "__class__  = <class 'needle.nn.nn_conv.Conv'>\n",
      "bias       = True\n",
      "device     = cpu()\n",
      "dtype      = 'float32'\n",
      "in_channels = 16\n",
      "kernel_size = 3\n",
      "out_channels = 8\n",
      "self       = <needle.nn.nn_conv.Conv object at 0x7f9124637810>\n",
      "stride     = 1\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_conv.py\u001b[0m:31: NotImplementedError\n",
      "\u001b[31m\u001b[1m_ test_nn_conv_forward[needle.backend_ndarray.ndarray_backend_cpu-32-16-8-3-2] _\u001b[0m\n",
      "\n",
      "s = 32, cin = 16, cout = 8, k = 3, stride = 2, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33ms,cin,cout,k,stride\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, conv_forward_params)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_nn_conv_forward\u001b[39;49;00m(s, cin, cout, k, stride, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mimport\u001b[39;49;00m \u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       f = ndl.nn.Conv(cin, cout, k, stride=stride, device=device)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "cin        = 16\n",
      "cout       = 8\n",
      "device     = cpu()\n",
      "k          = 3\n",
      "s          = 32\n",
      "stride     = 2\n",
      "torch      = <module 'torch' from '/opt/conda/envs/train/lib/python3.11/site-packages/torch/__init__.py'>\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:349: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.nn.nn_conv.Conv object at 0x7f9124620550>, in_channels = 16\n",
      "out_channels = 8, kernel_size = 3, stride = 2, bias = True, device = cpu()\n",
      "dtype = 'float32'\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92m__init__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, in_channels, out_channels, kernel_size, stride=\u001b[94m1\u001b[39;49;00m, bias=\u001b[94mTrue\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, dtype=\u001b[33m\"\u001b[39;49;00m\u001b[33mfloat32\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96msuper\u001b[39;49;00m().\u001b[92m__init__\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(kernel_size, \u001b[96mtuple\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      "            kernel_size = kernel_size[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(stride, \u001b[96mtuple\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      "            stride = stride[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m.in_channels = in_channels\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m.out_channels = out_channels\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m.kernel_size = kernel_size\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m.stride = stride\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "__class__  = <class 'needle.nn.nn_conv.Conv'>\n",
      "bias       = True\n",
      "device     = cpu()\n",
      "dtype      = 'float32'\n",
      "in_channels = 16\n",
      "kernel_size = 3\n",
      "out_channels = 8\n",
      "self       = <needle.nn.nn_conv.Conv object at 0x7f9124620550>\n",
      "stride     = 2\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_conv.py\u001b[0m:31: NotImplementedError\n",
      "\u001b[31m\u001b[1m_ test_nn_conv_forward[needle.backend_ndarray.ndarray_backend_cuda-4-8-16-3-1] _\u001b[0m\n",
      "\n",
      "s = 4, cin = 8, cout = 16, k = 3, stride = 1, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33ms,cin,cout,k,stride\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, conv_forward_params)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_nn_conv_forward\u001b[39;49;00m(s, cin, cout, k, stride, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mimport\u001b[39;49;00m \u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       f = ndl.nn.Conv(cin, cout, k, stride=stride, device=device)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "cin        = 8\n",
      "cout       = 16\n",
      "device     = cuda()\n",
      "k          = 3\n",
      "s          = 4\n",
      "stride     = 1\n",
      "torch      = <module 'torch' from '/opt/conda/envs/train/lib/python3.11/site-packages/torch/__init__.py'>\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:349: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.nn.nn_conv.Conv object at 0x7f9124641c90>, in_channels = 8\n",
      "out_channels = 16, kernel_size = 3, stride = 1, bias = True, device = cuda()\n",
      "dtype = 'float32'\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92m__init__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, in_channels, out_channels, kernel_size, stride=\u001b[94m1\u001b[39;49;00m, bias=\u001b[94mTrue\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, dtype=\u001b[33m\"\u001b[39;49;00m\u001b[33mfloat32\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96msuper\u001b[39;49;00m().\u001b[92m__init__\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(kernel_size, \u001b[96mtuple\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      "            kernel_size = kernel_size[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(stride, \u001b[96mtuple\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      "            stride = stride[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m.in_channels = in_channels\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m.out_channels = out_channels\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m.kernel_size = kernel_size\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m.stride = stride\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "__class__  = <class 'needle.nn.nn_conv.Conv'>\n",
      "bias       = True\n",
      "device     = cuda()\n",
      "dtype      = 'float32'\n",
      "in_channels = 8\n",
      "kernel_size = 3\n",
      "out_channels = 16\n",
      "self       = <needle.nn.nn_conv.Conv object at 0x7f9124641c90>\n",
      "stride     = 1\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_conv.py\u001b[0m:31: NotImplementedError\n",
      "\u001b[31m\u001b[1m_ test_nn_conv_forward[needle.backend_ndarray.ndarray_backend_cuda-32-8-16-3-2] _\u001b[0m\n",
      "\n",
      "s = 32, cin = 8, cout = 16, k = 3, stride = 2, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33ms,cin,cout,k,stride\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, conv_forward_params)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_nn_conv_forward\u001b[39;49;00m(s, cin, cout, k, stride, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mimport\u001b[39;49;00m \u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       f = ndl.nn.Conv(cin, cout, k, stride=stride, device=device)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "cin        = 8\n",
      "cout       = 16\n",
      "device     = cuda()\n",
      "k          = 3\n",
      "s          = 32\n",
      "stride     = 2\n",
      "torch      = <module 'torch' from '/opt/conda/envs/train/lib/python3.11/site-packages/torch/__init__.py'>\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:349: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.nn.nn_conv.Conv object at 0x7f91246e71d0>, in_channels = 8\n",
      "out_channels = 16, kernel_size = 3, stride = 2, bias = True, device = cuda()\n",
      "dtype = 'float32'\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92m__init__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, in_channels, out_channels, kernel_size, stride=\u001b[94m1\u001b[39;49;00m, bias=\u001b[94mTrue\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, dtype=\u001b[33m\"\u001b[39;49;00m\u001b[33mfloat32\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96msuper\u001b[39;49;00m().\u001b[92m__init__\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(kernel_size, \u001b[96mtuple\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      "            kernel_size = kernel_size[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(stride, \u001b[96mtuple\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      "            stride = stride[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m.in_channels = in_channels\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m.out_channels = out_channels\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m.kernel_size = kernel_size\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m.stride = stride\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "__class__  = <class 'needle.nn.nn_conv.Conv'>\n",
      "bias       = True\n",
      "device     = cuda()\n",
      "dtype      = 'float32'\n",
      "in_channels = 8\n",
      "kernel_size = 3\n",
      "out_channels = 16\n",
      "self       = <needle.nn.nn_conv.Conv object at 0x7f91246e71d0>\n",
      "stride     = 2\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_conv.py\u001b[0m:31: NotImplementedError\n",
      "\u001b[31m\u001b[1m_ test_nn_conv_forward[needle.backend_ndarray.ndarray_backend_cuda-32-8-8-3-2] _\u001b[0m\n",
      "\n",
      "s = 32, cin = 8, cout = 8, k = 3, stride = 2, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33ms,cin,cout,k,stride\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, conv_forward_params)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_nn_conv_forward\u001b[39;49;00m(s, cin, cout, k, stride, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mimport\u001b[39;49;00m \u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       f = ndl.nn.Conv(cin, cout, k, stride=stride, device=device)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "cin        = 8\n",
      "cout       = 8\n",
      "device     = cuda()\n",
      "k          = 3\n",
      "s          = 32\n",
      "stride     = 2\n",
      "torch      = <module 'torch' from '/opt/conda/envs/train/lib/python3.11/site-packages/torch/__init__.py'>\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:349: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.nn.nn_conv.Conv object at 0x7f912463f710>, in_channels = 8\n",
      "out_channels = 8, kernel_size = 3, stride = 2, bias = True, device = cuda()\n",
      "dtype = 'float32'\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92m__init__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, in_channels, out_channels, kernel_size, stride=\u001b[94m1\u001b[39;49;00m, bias=\u001b[94mTrue\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, dtype=\u001b[33m\"\u001b[39;49;00m\u001b[33mfloat32\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96msuper\u001b[39;49;00m().\u001b[92m__init__\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(kernel_size, \u001b[96mtuple\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      "            kernel_size = kernel_size[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(stride, \u001b[96mtuple\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      "            stride = stride[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m.in_channels = in_channels\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m.out_channels = out_channels\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m.kernel_size = kernel_size\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m.stride = stride\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "__class__  = <class 'needle.nn.nn_conv.Conv'>\n",
      "bias       = True\n",
      "device     = cuda()\n",
      "dtype      = 'float32'\n",
      "in_channels = 8\n",
      "kernel_size = 3\n",
      "out_channels = 8\n",
      "self       = <needle.nn.nn_conv.Conv object at 0x7f912463f710>\n",
      "stride     = 2\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_conv.py\u001b[0m:31: NotImplementedError\n",
      "\u001b[31m\u001b[1m_ test_nn_conv_forward[needle.backend_ndarray.ndarray_backend_cuda-32-16-8-3-1] _\u001b[0m\n",
      "\n",
      "s = 32, cin = 16, cout = 8, k = 3, stride = 1, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33ms,cin,cout,k,stride\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, conv_forward_params)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_nn_conv_forward\u001b[39;49;00m(s, cin, cout, k, stride, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mimport\u001b[39;49;00m \u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       f = ndl.nn.Conv(cin, cout, k, stride=stride, device=device)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "cin        = 16\n",
      "cout       = 8\n",
      "device     = cuda()\n",
      "k          = 3\n",
      "s          = 32\n",
      "stride     = 1\n",
      "torch      = <module 'torch' from '/opt/conda/envs/train/lib/python3.11/site-packages/torch/__init__.py'>\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:349: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.nn.nn_conv.Conv object at 0x7f9126927790>, in_channels = 16\n",
      "out_channels = 8, kernel_size = 3, stride = 1, bias = True, device = cuda()\n",
      "dtype = 'float32'\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92m__init__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, in_channels, out_channels, kernel_size, stride=\u001b[94m1\u001b[39;49;00m, bias=\u001b[94mTrue\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, dtype=\u001b[33m\"\u001b[39;49;00m\u001b[33mfloat32\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96msuper\u001b[39;49;00m().\u001b[92m__init__\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(kernel_size, \u001b[96mtuple\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      "            kernel_size = kernel_size[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(stride, \u001b[96mtuple\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      "            stride = stride[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m.in_channels = in_channels\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m.out_channels = out_channels\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m.kernel_size = kernel_size\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m.stride = stride\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "__class__  = <class 'needle.nn.nn_conv.Conv'>\n",
      "bias       = True\n",
      "device     = cuda()\n",
      "dtype      = 'float32'\n",
      "in_channels = 16\n",
      "kernel_size = 3\n",
      "out_channels = 8\n",
      "self       = <needle.nn.nn_conv.Conv object at 0x7f9126927790>\n",
      "stride     = 1\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_conv.py\u001b[0m:31: NotImplementedError\n",
      "\u001b[31m\u001b[1m_ test_nn_conv_forward[needle.backend_ndarray.ndarray_backend_cuda-32-16-8-3-2] _\u001b[0m\n",
      "\n",
      "s = 32, cin = 16, cout = 8, k = 3, stride = 2, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33ms,cin,cout,k,stride\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, conv_forward_params)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_nn_conv_forward\u001b[39;49;00m(s, cin, cout, k, stride, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mimport\u001b[39;49;00m \u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       f = ndl.nn.Conv(cin, cout, k, stride=stride, device=device)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "cin        = 16\n",
      "cout       = 8\n",
      "device     = cuda()\n",
      "k          = 3\n",
      "s          = 32\n",
      "stride     = 2\n",
      "torch      = <module 'torch' from '/opt/conda/envs/train/lib/python3.11/site-packages/torch/__init__.py'>\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:349: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.nn.nn_conv.Conv object at 0x7f9124c42510>, in_channels = 16\n",
      "out_channels = 8, kernel_size = 3, stride = 2, bias = True, device = cuda()\n",
      "dtype = 'float32'\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92m__init__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, in_channels, out_channels, kernel_size, stride=\u001b[94m1\u001b[39;49;00m, bias=\u001b[94mTrue\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, dtype=\u001b[33m\"\u001b[39;49;00m\u001b[33mfloat32\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96msuper\u001b[39;49;00m().\u001b[92m__init__\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(kernel_size, \u001b[96mtuple\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      "            kernel_size = kernel_size[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(stride, \u001b[96mtuple\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      "            stride = stride[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m.in_channels = in_channels\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m.out_channels = out_channels\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m.kernel_size = kernel_size\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m.stride = stride\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "__class__  = <class 'needle.nn.nn_conv.Conv'>\n",
      "bias       = True\n",
      "device     = cuda()\n",
      "dtype      = 'float32'\n",
      "in_channels = 16\n",
      "kernel_size = 3\n",
      "out_channels = 8\n",
      "self       = <needle.nn.nn_conv.Conv object at 0x7f9124c42510>\n",
      "stride     = 2\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_conv.py\u001b[0m:31: NotImplementedError\n",
      "\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_nn_conv_forward[needle.backend_ndarray.ndarray_backend_cpu-4-8-16-3-1]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_nn_conv_forward[needle.backend_ndarray.ndarray_backend_cpu-32-8-16-3-2]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_nn_conv_forward[needle.backend_ndarray.ndarray_backend_cpu-32-8-8-3-2]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_nn_conv_forward[needle.backend_ndarray.ndarray_backend_cpu-32-16-8-3-1]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_nn_conv_forward[needle.backend_ndarray.ndarray_backend_cpu-32-16-8-3-2]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_nn_conv_forward[needle.backend_ndarray.ndarray_backend_cuda-4-8-16-3-1]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_nn_conv_forward[needle.backend_ndarray.ndarray_backend_cuda-32-8-16-3-2]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_nn_conv_forward[needle.backend_ndarray.ndarray_backend_cuda-32-8-8-3-2]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_nn_conv_forward[needle.backend_ndarray.ndarray_backend_cuda-32-16-8-3-1]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_nn_conv_forward[needle.backend_ndarray.ndarray_backend_cuda-32-16-8-3-2]\u001b[0m - NotImplementedError\n",
      "\u001b[31m===================== \u001b[31m\u001b[1m10 failed\u001b[0m, \u001b[33m1793 deselected\u001b[0m\u001b[31m in 2.14s\u001b[0m\u001b[31m ======================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m pytest -l -v -k \"nn_conv_forward\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.11.10, pytest-8.3.4, pluggy-1.5.0 -- /opt/conda/envs/train/bin/python\n",
      "cachedir: .pytest_cache\n",
      "hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase(PosixPath('/mnt/zhumingzhu/work/00test/hw4/.hypothesis/examples'))\n",
      "rootdir: /mnt/zhumingzhu/work/00test/hw4\n",
      "plugins: hypothesis-6.115.5\n",
      "collected 1803 items / 1789 deselected / 14 selected                           \u001b[0m\u001b[1m\n",
      "\n",
      "tests/hw4/test_conv.py::test_nn_conv_backward[needle.backend_ndarray.ndarray_backend_cpu-4-1-1-3-1] \u001b[31mFAILED\u001b[0m\u001b[31m [  7%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_nn_conv_backward[needle.backend_ndarray.ndarray_backend_cpu-14-8-16-3-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 14%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_nn_conv_backward[needle.backend_ndarray.ndarray_backend_cpu-14-8-16-3-2] \u001b[31mFAILED\u001b[0m\u001b[31m [ 21%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_nn_conv_backward[needle.backend_ndarray.ndarray_backend_cpu-14-8-8-3-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 28%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_nn_conv_backward[needle.backend_ndarray.ndarray_backend_cpu-14-8-8-3-2] \u001b[31mFAILED\u001b[0m\u001b[31m [ 35%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_nn_conv_backward[needle.backend_ndarray.ndarray_backend_cpu-14-16-8-3-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 42%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_nn_conv_backward[needle.backend_ndarray.ndarray_backend_cpu-14-16-8-3-2] \u001b[31mFAILED\u001b[0m\u001b[31m [ 50%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_nn_conv_backward[needle.backend_ndarray.ndarray_backend_cuda-4-1-1-3-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 57%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_nn_conv_backward[needle.backend_ndarray.ndarray_backend_cuda-14-8-16-3-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 64%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_nn_conv_backward[needle.backend_ndarray.ndarray_backend_cuda-14-8-16-3-2] \u001b[31mFAILED\u001b[0m\u001b[31m [ 71%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_nn_conv_backward[needle.backend_ndarray.ndarray_backend_cuda-14-8-8-3-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 78%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_nn_conv_backward[needle.backend_ndarray.ndarray_backend_cuda-14-8-8-3-2] \u001b[31mFAILED\u001b[0m\u001b[31m [ 85%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_nn_conv_backward[needle.backend_ndarray.ndarray_backend_cuda-14-16-8-3-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 92%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_nn_conv_backward[needle.backend_ndarray.ndarray_backend_cuda-14-16-8-3-2] \u001b[31mFAILED\u001b[0m\u001b[31m [100%]\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m_ test_nn_conv_backward[needle.backend_ndarray.ndarray_backend_cpu-4-1-1-3-1] __\u001b[0m\n",
      "\n",
      "s = 4, cin = 1, cout = 1, k = 3, stride = 1, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33ms,cin,cout,k,stride\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, conv_back_params)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_nn_conv_backward\u001b[39;49;00m(s, cin, cout, k, stride, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mimport\u001b[39;49;00m \u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       f = ndl.nn.Conv(cin, cout, k, stride=stride, device=device)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "cin        = 1\n",
      "cout       = 1\n",
      "device     = cpu()\n",
      "k          = 3\n",
      "s          = 4\n",
      "stride     = 1\n",
      "torch      = <module 'torch' from '/opt/conda/envs/train/lib/python3.11/site-packages/torch/__init__.py'>\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:374: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.nn.nn_conv.Conv object at 0x7fc793c96890>, in_channels = 1\n",
      "out_channels = 1, kernel_size = 3, stride = 1, bias = True, device = cpu()\n",
      "dtype = 'float32'\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92m__init__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, in_channels, out_channels, kernel_size, stride=\u001b[94m1\u001b[39;49;00m, bias=\u001b[94mTrue\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, dtype=\u001b[33m\"\u001b[39;49;00m\u001b[33mfloat32\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96msuper\u001b[39;49;00m().\u001b[92m__init__\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(kernel_size, \u001b[96mtuple\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      "            kernel_size = kernel_size[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(stride, \u001b[96mtuple\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      "            stride = stride[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m.in_channels = in_channels\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m.out_channels = out_channels\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m.kernel_size = kernel_size\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m.stride = stride\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "__class__  = <class 'needle.nn.nn_conv.Conv'>\n",
      "bias       = True\n",
      "device     = cpu()\n",
      "dtype      = 'float32'\n",
      "in_channels = 1\n",
      "kernel_size = 3\n",
      "out_channels = 1\n",
      "self       = <needle.nn.nn_conv.Conv object at 0x7fc793c96890>\n",
      "stride     = 1\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_conv.py\u001b[0m:31: NotImplementedError\n",
      "\u001b[31m\u001b[1m_ test_nn_conv_backward[needle.backend_ndarray.ndarray_backend_cpu-14-8-16-3-1] _\u001b[0m\n",
      "\n",
      "s = 14, cin = 8, cout = 16, k = 3, stride = 1, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33ms,cin,cout,k,stride\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, conv_back_params)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_nn_conv_backward\u001b[39;49;00m(s, cin, cout, k, stride, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mimport\u001b[39;49;00m \u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       f = ndl.nn.Conv(cin, cout, k, stride=stride, device=device)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "cin        = 8\n",
      "cout       = 16\n",
      "device     = cpu()\n",
      "k          = 3\n",
      "s          = 14\n",
      "stride     = 1\n",
      "torch      = <module 'torch' from '/opt/conda/envs/train/lib/python3.11/site-packages/torch/__init__.py'>\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:374: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.nn.nn_conv.Conv object at 0x7fc29459b0d0>, in_channels = 8\n",
      "out_channels = 16, kernel_size = 3, stride = 1, bias = True, device = cpu()\n",
      "dtype = 'float32'\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92m__init__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, in_channels, out_channels, kernel_size, stride=\u001b[94m1\u001b[39;49;00m, bias=\u001b[94mTrue\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, dtype=\u001b[33m\"\u001b[39;49;00m\u001b[33mfloat32\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96msuper\u001b[39;49;00m().\u001b[92m__init__\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(kernel_size, \u001b[96mtuple\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      "            kernel_size = kernel_size[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(stride, \u001b[96mtuple\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      "            stride = stride[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m.in_channels = in_channels\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m.out_channels = out_channels\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m.kernel_size = kernel_size\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m.stride = stride\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "__class__  = <class 'needle.nn.nn_conv.Conv'>\n",
      "bias       = True\n",
      "device     = cpu()\n",
      "dtype      = 'float32'\n",
      "in_channels = 8\n",
      "kernel_size = 3\n",
      "out_channels = 16\n",
      "self       = <needle.nn.nn_conv.Conv object at 0x7fc29459b0d0>\n",
      "stride     = 1\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_conv.py\u001b[0m:31: NotImplementedError\n",
      "\u001b[31m\u001b[1m_ test_nn_conv_backward[needle.backend_ndarray.ndarray_backend_cpu-14-8-16-3-2] _\u001b[0m\n",
      "\n",
      "s = 14, cin = 8, cout = 16, k = 3, stride = 2, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33ms,cin,cout,k,stride\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, conv_back_params)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_nn_conv_backward\u001b[39;49;00m(s, cin, cout, k, stride, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mimport\u001b[39;49;00m \u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       f = ndl.nn.Conv(cin, cout, k, stride=stride, device=device)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "cin        = 8\n",
      "cout       = 16\n",
      "device     = cpu()\n",
      "k          = 3\n",
      "s          = 14\n",
      "stride     = 2\n",
      "torch      = <module 'torch' from '/opt/conda/envs/train/lib/python3.11/site-packages/torch/__init__.py'>\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:374: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.nn.nn_conv.Conv object at 0x7fc293824050>, in_channels = 8\n",
      "out_channels = 16, kernel_size = 3, stride = 2, bias = True, device = cpu()\n",
      "dtype = 'float32'\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92m__init__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, in_channels, out_channels, kernel_size, stride=\u001b[94m1\u001b[39;49;00m, bias=\u001b[94mTrue\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, dtype=\u001b[33m\"\u001b[39;49;00m\u001b[33mfloat32\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96msuper\u001b[39;49;00m().\u001b[92m__init__\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(kernel_size, \u001b[96mtuple\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      "            kernel_size = kernel_size[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(stride, \u001b[96mtuple\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      "            stride = stride[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m.in_channels = in_channels\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m.out_channels = out_channels\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m.kernel_size = kernel_size\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m.stride = stride\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "__class__  = <class 'needle.nn.nn_conv.Conv'>\n",
      "bias       = True\n",
      "device     = cpu()\n",
      "dtype      = 'float32'\n",
      "in_channels = 8\n",
      "kernel_size = 3\n",
      "out_channels = 16\n",
      "self       = <needle.nn.nn_conv.Conv object at 0x7fc293824050>\n",
      "stride     = 2\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_conv.py\u001b[0m:31: NotImplementedError\n",
      "\u001b[31m\u001b[1m_ test_nn_conv_backward[needle.backend_ndarray.ndarray_backend_cpu-14-8-8-3-1] _\u001b[0m\n",
      "\n",
      "s = 14, cin = 8, cout = 8, k = 3, stride = 1, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33ms,cin,cout,k,stride\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, conv_back_params)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_nn_conv_backward\u001b[39;49;00m(s, cin, cout, k, stride, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mimport\u001b[39;49;00m \u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       f = ndl.nn.Conv(cin, cout, k, stride=stride, device=device)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "cin        = 8\n",
      "cout       = 8\n",
      "device     = cpu()\n",
      "k          = 3\n",
      "s          = 14\n",
      "stride     = 1\n",
      "torch      = <module 'torch' from '/opt/conda/envs/train/lib/python3.11/site-packages/torch/__init__.py'>\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:374: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.nn.nn_conv.Conv object at 0x7fc2938351d0>, in_channels = 8\n",
      "out_channels = 8, kernel_size = 3, stride = 1, bias = True, device = cpu()\n",
      "dtype = 'float32'\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92m__init__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, in_channels, out_channels, kernel_size, stride=\u001b[94m1\u001b[39;49;00m, bias=\u001b[94mTrue\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, dtype=\u001b[33m\"\u001b[39;49;00m\u001b[33mfloat32\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96msuper\u001b[39;49;00m().\u001b[92m__init__\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(kernel_size, \u001b[96mtuple\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      "            kernel_size = kernel_size[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(stride, \u001b[96mtuple\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      "            stride = stride[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m.in_channels = in_channels\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m.out_channels = out_channels\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m.kernel_size = kernel_size\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m.stride = stride\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "__class__  = <class 'needle.nn.nn_conv.Conv'>\n",
      "bias       = True\n",
      "device     = cpu()\n",
      "dtype      = 'float32'\n",
      "in_channels = 8\n",
      "kernel_size = 3\n",
      "out_channels = 8\n",
      "self       = <needle.nn.nn_conv.Conv object at 0x7fc2938351d0>\n",
      "stride     = 1\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_conv.py\u001b[0m:31: NotImplementedError\n",
      "\u001b[31m\u001b[1m_ test_nn_conv_backward[needle.backend_ndarray.ndarray_backend_cpu-14-8-8-3-2] _\u001b[0m\n",
      "\n",
      "s = 14, cin = 8, cout = 8, k = 3, stride = 2, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33ms,cin,cout,k,stride\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, conv_back_params)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_nn_conv_backward\u001b[39;49;00m(s, cin, cout, k, stride, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mimport\u001b[39;49;00m \u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       f = ndl.nn.Conv(cin, cout, k, stride=stride, device=device)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "cin        = 8\n",
      "cout       = 8\n",
      "device     = cpu()\n",
      "k          = 3\n",
      "s          = 14\n",
      "stride     = 2\n",
      "torch      = <module 'torch' from '/opt/conda/envs/train/lib/python3.11/site-packages/torch/__init__.py'>\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:374: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.nn.nn_conv.Conv object at 0x7fc293820810>, in_channels = 8\n",
      "out_channels = 8, kernel_size = 3, stride = 2, bias = True, device = cpu()\n",
      "dtype = 'float32'\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92m__init__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, in_channels, out_channels, kernel_size, stride=\u001b[94m1\u001b[39;49;00m, bias=\u001b[94mTrue\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, dtype=\u001b[33m\"\u001b[39;49;00m\u001b[33mfloat32\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96msuper\u001b[39;49;00m().\u001b[92m__init__\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(kernel_size, \u001b[96mtuple\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      "            kernel_size = kernel_size[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(stride, \u001b[96mtuple\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      "            stride = stride[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m.in_channels = in_channels\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m.out_channels = out_channels\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m.kernel_size = kernel_size\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m.stride = stride\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "__class__  = <class 'needle.nn.nn_conv.Conv'>\n",
      "bias       = True\n",
      "device     = cpu()\n",
      "dtype      = 'float32'\n",
      "in_channels = 8\n",
      "kernel_size = 3\n",
      "out_channels = 8\n",
      "self       = <needle.nn.nn_conv.Conv object at 0x7fc293820810>\n",
      "stride     = 2\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_conv.py\u001b[0m:31: NotImplementedError\n",
      "\u001b[31m\u001b[1m_ test_nn_conv_backward[needle.backend_ndarray.ndarray_backend_cpu-14-16-8-3-1] _\u001b[0m\n",
      "\n",
      "s = 14, cin = 16, cout = 8, k = 3, stride = 1, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33ms,cin,cout,k,stride\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, conv_back_params)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_nn_conv_backward\u001b[39;49;00m(s, cin, cout, k, stride, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mimport\u001b[39;49;00m \u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       f = ndl.nn.Conv(cin, cout, k, stride=stride, device=device)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "cin        = 16\n",
      "cout       = 8\n",
      "device     = cpu()\n",
      "k          = 3\n",
      "s          = 14\n",
      "stride     = 1\n",
      "torch      = <module 'torch' from '/opt/conda/envs/train/lib/python3.11/site-packages/torch/__init__.py'>\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:374: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.nn.nn_conv.Conv object at 0x7fc293a42710>, in_channels = 16\n",
      "out_channels = 8, kernel_size = 3, stride = 1, bias = True, device = cpu()\n",
      "dtype = 'float32'\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92m__init__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, in_channels, out_channels, kernel_size, stride=\u001b[94m1\u001b[39;49;00m, bias=\u001b[94mTrue\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, dtype=\u001b[33m\"\u001b[39;49;00m\u001b[33mfloat32\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96msuper\u001b[39;49;00m().\u001b[92m__init__\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(kernel_size, \u001b[96mtuple\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      "            kernel_size = kernel_size[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(stride, \u001b[96mtuple\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      "            stride = stride[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m.in_channels = in_channels\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m.out_channels = out_channels\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m.kernel_size = kernel_size\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m.stride = stride\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "__class__  = <class 'needle.nn.nn_conv.Conv'>\n",
      "bias       = True\n",
      "device     = cpu()\n",
      "dtype      = 'float32'\n",
      "in_channels = 16\n",
      "kernel_size = 3\n",
      "out_channels = 8\n",
      "self       = <needle.nn.nn_conv.Conv object at 0x7fc293a42710>\n",
      "stride     = 1\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_conv.py\u001b[0m:31: NotImplementedError\n",
      "\u001b[31m\u001b[1m_ test_nn_conv_backward[needle.backend_ndarray.ndarray_backend_cpu-14-16-8-3-2] _\u001b[0m\n",
      "\n",
      "s = 14, cin = 16, cout = 8, k = 3, stride = 2, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33ms,cin,cout,k,stride\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, conv_back_params)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_nn_conv_backward\u001b[39;49;00m(s, cin, cout, k, stride, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mimport\u001b[39;49;00m \u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       f = ndl.nn.Conv(cin, cout, k, stride=stride, device=device)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "cin        = 16\n",
      "cout       = 8\n",
      "device     = cpu()\n",
      "k          = 3\n",
      "s          = 14\n",
      "stride     = 2\n",
      "torch      = <module 'torch' from '/opt/conda/envs/train/lib/python3.11/site-packages/torch/__init__.py'>\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:374: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.nn.nn_conv.Conv object at 0x7fc2938d9350>, in_channels = 16\n",
      "out_channels = 8, kernel_size = 3, stride = 2, bias = True, device = cpu()\n",
      "dtype = 'float32'\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92m__init__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, in_channels, out_channels, kernel_size, stride=\u001b[94m1\u001b[39;49;00m, bias=\u001b[94mTrue\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, dtype=\u001b[33m\"\u001b[39;49;00m\u001b[33mfloat32\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96msuper\u001b[39;49;00m().\u001b[92m__init__\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(kernel_size, \u001b[96mtuple\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      "            kernel_size = kernel_size[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(stride, \u001b[96mtuple\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      "            stride = stride[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m.in_channels = in_channels\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m.out_channels = out_channels\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m.kernel_size = kernel_size\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m.stride = stride\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "__class__  = <class 'needle.nn.nn_conv.Conv'>\n",
      "bias       = True\n",
      "device     = cpu()\n",
      "dtype      = 'float32'\n",
      "in_channels = 16\n",
      "kernel_size = 3\n",
      "out_channels = 8\n",
      "self       = <needle.nn.nn_conv.Conv object at 0x7fc2938d9350>\n",
      "stride     = 2\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_conv.py\u001b[0m:31: NotImplementedError\n",
      "\u001b[31m\u001b[1m_ test_nn_conv_backward[needle.backend_ndarray.ndarray_backend_cuda-4-1-1-3-1] _\u001b[0m\n",
      "\n",
      "s = 4, cin = 1, cout = 1, k = 3, stride = 1, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33ms,cin,cout,k,stride\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, conv_back_params)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_nn_conv_backward\u001b[39;49;00m(s, cin, cout, k, stride, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mimport\u001b[39;49;00m \u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       f = ndl.nn.Conv(cin, cout, k, stride=stride, device=device)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "cin        = 1\n",
      "cout       = 1\n",
      "device     = cuda()\n",
      "k          = 3\n",
      "s          = 4\n",
      "stride     = 1\n",
      "torch      = <module 'torch' from '/opt/conda/envs/train/lib/python3.11/site-packages/torch/__init__.py'>\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:374: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.nn.nn_conv.Conv object at 0x7fc293943950>, in_channels = 1\n",
      "out_channels = 1, kernel_size = 3, stride = 1, bias = True, device = cuda()\n",
      "dtype = 'float32'\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92m__init__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, in_channels, out_channels, kernel_size, stride=\u001b[94m1\u001b[39;49;00m, bias=\u001b[94mTrue\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, dtype=\u001b[33m\"\u001b[39;49;00m\u001b[33mfloat32\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96msuper\u001b[39;49;00m().\u001b[92m__init__\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(kernel_size, \u001b[96mtuple\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      "            kernel_size = kernel_size[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(stride, \u001b[96mtuple\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      "            stride = stride[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m.in_channels = in_channels\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m.out_channels = out_channels\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m.kernel_size = kernel_size\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m.stride = stride\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "__class__  = <class 'needle.nn.nn_conv.Conv'>\n",
      "bias       = True\n",
      "device     = cuda()\n",
      "dtype      = 'float32'\n",
      "in_channels = 1\n",
      "kernel_size = 3\n",
      "out_channels = 1\n",
      "self       = <needle.nn.nn_conv.Conv object at 0x7fc293943950>\n",
      "stride     = 1\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_conv.py\u001b[0m:31: NotImplementedError\n",
      "\u001b[31m\u001b[1m_ test_nn_conv_backward[needle.backend_ndarray.ndarray_backend_cuda-14-8-16-3-1] _\u001b[0m\n",
      "\n",
      "s = 14, cin = 8, cout = 16, k = 3, stride = 1, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33ms,cin,cout,k,stride\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, conv_back_params)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_nn_conv_backward\u001b[39;49;00m(s, cin, cout, k, stride, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mimport\u001b[39;49;00m \u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       f = ndl.nn.Conv(cin, cout, k, stride=stride, device=device)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "cin        = 8\n",
      "cout       = 16\n",
      "device     = cuda()\n",
      "k          = 3\n",
      "s          = 14\n",
      "stride     = 1\n",
      "torch      = <module 'torch' from '/opt/conda/envs/train/lib/python3.11/site-packages/torch/__init__.py'>\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:374: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.nn.nn_conv.Conv object at 0x7fc29383ff50>, in_channels = 8\n",
      "out_channels = 16, kernel_size = 3, stride = 1, bias = True, device = cuda()\n",
      "dtype = 'float32'\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92m__init__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, in_channels, out_channels, kernel_size, stride=\u001b[94m1\u001b[39;49;00m, bias=\u001b[94mTrue\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, dtype=\u001b[33m\"\u001b[39;49;00m\u001b[33mfloat32\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96msuper\u001b[39;49;00m().\u001b[92m__init__\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(kernel_size, \u001b[96mtuple\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      "            kernel_size = kernel_size[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(stride, \u001b[96mtuple\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      "            stride = stride[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m.in_channels = in_channels\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m.out_channels = out_channels\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m.kernel_size = kernel_size\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m.stride = stride\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "__class__  = <class 'needle.nn.nn_conv.Conv'>\n",
      "bias       = True\n",
      "device     = cuda()\n",
      "dtype      = 'float32'\n",
      "in_channels = 8\n",
      "kernel_size = 3\n",
      "out_channels = 16\n",
      "self       = <needle.nn.nn_conv.Conv object at 0x7fc29383ff50>\n",
      "stride     = 1\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_conv.py\u001b[0m:31: NotImplementedError\n",
      "\u001b[31m\u001b[1m_ test_nn_conv_backward[needle.backend_ndarray.ndarray_backend_cuda-14-8-16-3-2] _\u001b[0m\n",
      "\n",
      "s = 14, cin = 8, cout = 16, k = 3, stride = 2, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33ms,cin,cout,k,stride\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, conv_back_params)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_nn_conv_backward\u001b[39;49;00m(s, cin, cout, k, stride, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mimport\u001b[39;49;00m \u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       f = ndl.nn.Conv(cin, cout, k, stride=stride, device=device)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "cin        = 8\n",
      "cout       = 16\n",
      "device     = cuda()\n",
      "k          = 3\n",
      "s          = 14\n",
      "stride     = 2\n",
      "torch      = <module 'torch' from '/opt/conda/envs/train/lib/python3.11/site-packages/torch/__init__.py'>\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:374: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.nn.nn_conv.Conv object at 0x7fc350d25dd0>, in_channels = 8\n",
      "out_channels = 16, kernel_size = 3, stride = 2, bias = True, device = cuda()\n",
      "dtype = 'float32'\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92m__init__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, in_channels, out_channels, kernel_size, stride=\u001b[94m1\u001b[39;49;00m, bias=\u001b[94mTrue\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, dtype=\u001b[33m\"\u001b[39;49;00m\u001b[33mfloat32\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96msuper\u001b[39;49;00m().\u001b[92m__init__\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(kernel_size, \u001b[96mtuple\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      "            kernel_size = kernel_size[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(stride, \u001b[96mtuple\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      "            stride = stride[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m.in_channels = in_channels\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m.out_channels = out_channels\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m.kernel_size = kernel_size\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m.stride = stride\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "__class__  = <class 'needle.nn.nn_conv.Conv'>\n",
      "bias       = True\n",
      "device     = cuda()\n",
      "dtype      = 'float32'\n",
      "in_channels = 8\n",
      "kernel_size = 3\n",
      "out_channels = 16\n",
      "self       = <needle.nn.nn_conv.Conv object at 0x7fc350d25dd0>\n",
      "stride     = 2\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_conv.py\u001b[0m:31: NotImplementedError\n",
      "\u001b[31m\u001b[1m_ test_nn_conv_backward[needle.backend_ndarray.ndarray_backend_cuda-14-8-8-3-1] _\u001b[0m\n",
      "\n",
      "s = 14, cin = 8, cout = 8, k = 3, stride = 1, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33ms,cin,cout,k,stride\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, conv_back_params)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_nn_conv_backward\u001b[39;49;00m(s, cin, cout, k, stride, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mimport\u001b[39;49;00m \u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       f = ndl.nn.Conv(cin, cout, k, stride=stride, device=device)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "cin        = 8\n",
      "cout       = 8\n",
      "device     = cuda()\n",
      "k          = 3\n",
      "s          = 14\n",
      "stride     = 1\n",
      "torch      = <module 'torch' from '/opt/conda/envs/train/lib/python3.11/site-packages/torch/__init__.py'>\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:374: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.nn.nn_conv.Conv object at 0x7fc293e42290>, in_channels = 8\n",
      "out_channels = 8, kernel_size = 3, stride = 1, bias = True, device = cuda()\n",
      "dtype = 'float32'\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92m__init__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, in_channels, out_channels, kernel_size, stride=\u001b[94m1\u001b[39;49;00m, bias=\u001b[94mTrue\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, dtype=\u001b[33m\"\u001b[39;49;00m\u001b[33mfloat32\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96msuper\u001b[39;49;00m().\u001b[92m__init__\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(kernel_size, \u001b[96mtuple\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      "            kernel_size = kernel_size[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(stride, \u001b[96mtuple\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      "            stride = stride[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m.in_channels = in_channels\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m.out_channels = out_channels\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m.kernel_size = kernel_size\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m.stride = stride\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "__class__  = <class 'needle.nn.nn_conv.Conv'>\n",
      "bias       = True\n",
      "device     = cuda()\n",
      "dtype      = 'float32'\n",
      "in_channels = 8\n",
      "kernel_size = 3\n",
      "out_channels = 8\n",
      "self       = <needle.nn.nn_conv.Conv object at 0x7fc293e42290>\n",
      "stride     = 1\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_conv.py\u001b[0m:31: NotImplementedError\n",
      "\u001b[31m\u001b[1m_ test_nn_conv_backward[needle.backend_ndarray.ndarray_backend_cuda-14-8-8-3-2] _\u001b[0m\n",
      "\n",
      "s = 14, cin = 8, cout = 8, k = 3, stride = 2, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33ms,cin,cout,k,stride\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, conv_back_params)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_nn_conv_backward\u001b[39;49;00m(s, cin, cout, k, stride, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mimport\u001b[39;49;00m \u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       f = ndl.nn.Conv(cin, cout, k, stride=stride, device=device)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "cin        = 8\n",
      "cout       = 8\n",
      "device     = cuda()\n",
      "k          = 3\n",
      "s          = 14\n",
      "stride     = 2\n",
      "torch      = <module 'torch' from '/opt/conda/envs/train/lib/python3.11/site-packages/torch/__init__.py'>\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:374: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.nn.nn_conv.Conv object at 0x7fc2937b4a10>, in_channels = 8\n",
      "out_channels = 8, kernel_size = 3, stride = 2, bias = True, device = cuda()\n",
      "dtype = 'float32'\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92m__init__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, in_channels, out_channels, kernel_size, stride=\u001b[94m1\u001b[39;49;00m, bias=\u001b[94mTrue\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, dtype=\u001b[33m\"\u001b[39;49;00m\u001b[33mfloat32\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96msuper\u001b[39;49;00m().\u001b[92m__init__\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(kernel_size, \u001b[96mtuple\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      "            kernel_size = kernel_size[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(stride, \u001b[96mtuple\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      "            stride = stride[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m.in_channels = in_channels\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m.out_channels = out_channels\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m.kernel_size = kernel_size\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m.stride = stride\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "__class__  = <class 'needle.nn.nn_conv.Conv'>\n",
      "bias       = True\n",
      "device     = cuda()\n",
      "dtype      = 'float32'\n",
      "in_channels = 8\n",
      "kernel_size = 3\n",
      "out_channels = 8\n",
      "self       = <needle.nn.nn_conv.Conv object at 0x7fc2937b4a10>\n",
      "stride     = 2\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_conv.py\u001b[0m:31: NotImplementedError\n",
      "\u001b[31m\u001b[1m_ test_nn_conv_backward[needle.backend_ndarray.ndarray_backend_cuda-14-16-8-3-1] _\u001b[0m\n",
      "\n",
      "s = 14, cin = 16, cout = 8, k = 3, stride = 1, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33ms,cin,cout,k,stride\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, conv_back_params)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_nn_conv_backward\u001b[39;49;00m(s, cin, cout, k, stride, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mimport\u001b[39;49;00m \u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       f = ndl.nn.Conv(cin, cout, k, stride=stride, device=device)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "cin        = 16\n",
      "cout       = 8\n",
      "device     = cuda()\n",
      "k          = 3\n",
      "s          = 14\n",
      "stride     = 1\n",
      "torch      = <module 'torch' from '/opt/conda/envs/train/lib/python3.11/site-packages/torch/__init__.py'>\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:374: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.nn.nn_conv.Conv object at 0x7fc2938235d0>, in_channels = 16\n",
      "out_channels = 8, kernel_size = 3, stride = 1, bias = True, device = cuda()\n",
      "dtype = 'float32'\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92m__init__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, in_channels, out_channels, kernel_size, stride=\u001b[94m1\u001b[39;49;00m, bias=\u001b[94mTrue\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, dtype=\u001b[33m\"\u001b[39;49;00m\u001b[33mfloat32\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96msuper\u001b[39;49;00m().\u001b[92m__init__\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(kernel_size, \u001b[96mtuple\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      "            kernel_size = kernel_size[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(stride, \u001b[96mtuple\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      "            stride = stride[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m.in_channels = in_channels\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m.out_channels = out_channels\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m.kernel_size = kernel_size\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m.stride = stride\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "__class__  = <class 'needle.nn.nn_conv.Conv'>\n",
      "bias       = True\n",
      "device     = cuda()\n",
      "dtype      = 'float32'\n",
      "in_channels = 16\n",
      "kernel_size = 3\n",
      "out_channels = 8\n",
      "self       = <needle.nn.nn_conv.Conv object at 0x7fc2938235d0>\n",
      "stride     = 1\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_conv.py\u001b[0m:31: NotImplementedError\n",
      "\u001b[31m\u001b[1m_ test_nn_conv_backward[needle.backend_ndarray.ndarray_backend_cuda-14-16-8-3-2] _\u001b[0m\n",
      "\n",
      "s = 14, cin = 16, cout = 8, k = 3, stride = 2, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33ms,cin,cout,k,stride\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, conv_back_params)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_nn_conv_backward\u001b[39;49;00m(s, cin, cout, k, stride, device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mimport\u001b[39;49;00m \u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       f = ndl.nn.Conv(cin, cout, k, stride=stride, device=device)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "cin        = 16\n",
      "cout       = 8\n",
      "device     = cuda()\n",
      "k          = 3\n",
      "s          = 14\n",
      "stride     = 2\n",
      "torch      = <module 'torch' from '/opt/conda/envs/train/lib/python3.11/site-packages/torch/__init__.py'>\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:374: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.nn.nn_conv.Conv object at 0x7fc29386a510>, in_channels = 16\n",
      "out_channels = 8, kernel_size = 3, stride = 2, bias = True, device = cuda()\n",
      "dtype = 'float32'\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92m__init__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, in_channels, out_channels, kernel_size, stride=\u001b[94m1\u001b[39;49;00m, bias=\u001b[94mTrue\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, dtype=\u001b[33m\"\u001b[39;49;00m\u001b[33mfloat32\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96msuper\u001b[39;49;00m().\u001b[92m__init__\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(kernel_size, \u001b[96mtuple\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      "            kernel_size = kernel_size[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(stride, \u001b[96mtuple\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      "            stride = stride[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m.in_channels = in_channels\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m.out_channels = out_channels\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m.kernel_size = kernel_size\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m.stride = stride\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "__class__  = <class 'needle.nn.nn_conv.Conv'>\n",
      "bias       = True\n",
      "device     = cuda()\n",
      "dtype      = 'float32'\n",
      "in_channels = 16\n",
      "kernel_size = 3\n",
      "out_channels = 8\n",
      "self       = <needle.nn.nn_conv.Conv object at 0x7fc29386a510>\n",
      "stride     = 2\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_conv.py\u001b[0m:31: NotImplementedError\n",
      "\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_nn_conv_backward[needle.backend_ndarray.ndarray_backend_cpu-4-1-1-3-1]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_nn_conv_backward[needle.backend_ndarray.ndarray_backend_cpu-14-8-16-3-1]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_nn_conv_backward[needle.backend_ndarray.ndarray_backend_cpu-14-8-16-3-2]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_nn_conv_backward[needle.backend_ndarray.ndarray_backend_cpu-14-8-8-3-1]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_nn_conv_backward[needle.backend_ndarray.ndarray_backend_cpu-14-8-8-3-2]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_nn_conv_backward[needle.backend_ndarray.ndarray_backend_cpu-14-16-8-3-1]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_nn_conv_backward[needle.backend_ndarray.ndarray_backend_cpu-14-16-8-3-2]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_nn_conv_backward[needle.backend_ndarray.ndarray_backend_cuda-4-1-1-3-1]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_nn_conv_backward[needle.backend_ndarray.ndarray_backend_cuda-14-8-16-3-1]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_nn_conv_backward[needle.backend_ndarray.ndarray_backend_cuda-14-8-16-3-2]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_nn_conv_backward[needle.backend_ndarray.ndarray_backend_cuda-14-8-8-3-1]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_nn_conv_backward[needle.backend_ndarray.ndarray_backend_cuda-14-8-8-3-2]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_nn_conv_backward[needle.backend_ndarray.ndarray_backend_cuda-14-16-8-3-1]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_nn_conv_backward[needle.backend_ndarray.ndarray_backend_cuda-14-16-8-3-2]\u001b[0m - NotImplementedError\n",
      "\u001b[31m===================== \u001b[31m\u001b[1m14 failed\u001b[0m, \u001b[33m1789 deselected\u001b[0m\u001b[31m in 2.18s\u001b[0m\u001b[31m ======================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m pytest -l -v -k \"nn_conv_backward\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Implementing \"ResNet9\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now use your convolutional layer to implement a model similar to _ResNet9_, which is known to be a reasonable model for getting good accuracy on CIFAR-10 quickly (see [here](https://github.com/davidcpage/cifar10-fast)). Our main change is that we used striding instead of pooling and divided all of the channels by 4 for the sake of performance (as our framework is not as well-optimized as industry-grade frameworks).\n",
    "\n",
    "In the figure below, before the first linear layer, you should \"flatten\" the tensor. You can use the module `Flatten` in `nn_basic.py`, or you can simply use `.reshape` in the `forward()` method of your ResNet9.\n",
    "\n",
    "Make sure that you pass the device to all modules in your model; otherwise, you will get errors about mismatched devices when trying to run with CUDA.\n",
    "\n",
    "<center><img src=\"https://github.com/dlsyscourse/hw4/blob/main/ResNet9.png?raw=true\" alt=\"ResNet9\" style=\"width: 400px;\" /></center>\n",
    "\n",
    "We have tried to make it easier to pass the tests here than for previous assignments where you have implemented models. In particular, we are just going to make sure it has the right number of parameters and similar accuracy and loss after 1 or 2 batches of CIFAR-10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.11.10, pytest-8.3.4, pluggy-1.5.0 -- /opt/conda/envs/train/bin/python\n",
      "cachedir: .pytest_cache\n",
      "hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase(PosixPath('/mnt/zhumingzhu/work/00test/hw4/.hypothesis/examples'))\n",
      "rootdir: /mnt/zhumingzhu/work/00test/hw4\n",
      "plugins: hypothesis-6.115.5\n",
      "collected 1803 items / 1801 deselected / 2 selected                            \u001b[0m\u001b[1m\n",
      "\n",
      "tests/hw4/test_conv.py::test_resnet9[needle.backend_ndarray.ndarray_backend_cpu] \u001b[31mFAILED\u001b[0m\u001b[31m [ 50%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_resnet9[needle.backend_ndarray.ndarray_backend_cuda] \u001b[31mFAILED\u001b[0m\u001b[31m [100%]\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m___________ test_resnet9[needle.backend_ndarray.ndarray_backend_cpu] ___________\u001b[0m\n",
      "\n",
      "device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_resnet9\u001b[39;49;00m(device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mdef\u001b[39;49;00m \u001b[92mnum_params\u001b[39;49;00m(model):\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m np.sum([np.prod(x.shape) \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m model.parameters()])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mfrom\u001b[39;49;00m \u001b[04m\u001b[96mapps\u001b[39;49;00m\u001b[04m\u001b[96m.\u001b[39;49;00m\u001b[04m\u001b[96mmodels\u001b[39;49;00m \u001b[94mimport\u001b[39;49;00m ResNet9\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      ">       model = ResNet9(device=device)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "ResNet9    = <class 'apps.models.ResNet9'>\n",
      "device     = cpu()\n",
      "num_params = <function test_resnet9.<locals>.num_params at 0x7f27a72299e0>\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:180: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <apps.models.ResNet9 object at 0x7f27a7d05cd0>, device = cpu()\n",
      "dtype = 'float32'\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92m__init__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, dtype=\u001b[33m\"\u001b[39;49;00m\u001b[33mfloat32\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96msuper\u001b[39;49;00m().\u001b[92m__init__\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION ###\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m() \u001b[90m###\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "__class__  = <class 'apps.models.ResNet9'>\n",
      "device     = cpu()\n",
      "dtype      = 'float32'\n",
      "self       = <apps.models.ResNet9 object at 0x7f27a7d05cd0>\n",
      "\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:14: NotImplementedError\n",
      "\u001b[31m\u001b[1m__________ test_resnet9[needle.backend_ndarray.ndarray_backend_cuda] ___________\u001b[0m\n",
      "\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_resnet9\u001b[39;49;00m(device):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mdef\u001b[39;49;00m \u001b[92mnum_params\u001b[39;49;00m(model):\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m np.sum([np.prod(x.shape) \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m model.parameters()])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mfrom\u001b[39;49;00m \u001b[04m\u001b[96mapps\u001b[39;49;00m\u001b[04m\u001b[96m.\u001b[39;49;00m\u001b[04m\u001b[96mmodels\u001b[39;49;00m \u001b[94mimport\u001b[39;49;00m ResNet9\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      ">       model = ResNet9(device=device)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "ResNet9    = <class 'apps.models.ResNet9'>\n",
      "device     = cuda()\n",
      "num_params = <function test_resnet9.<locals>.num_params at 0x7f27a7229c60>\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:180: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <apps.models.ResNet9 object at 0x7f27a7309890>, device = cuda()\n",
      "dtype = 'float32'\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92m__init__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, dtype=\u001b[33m\"\u001b[39;49;00m\u001b[33mfloat32\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96msuper\u001b[39;49;00m().\u001b[92m__init__\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION ###\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m() \u001b[90m###\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "__class__  = <class 'apps.models.ResNet9'>\n",
      "device     = cuda()\n",
      "dtype      = 'float32'\n",
      "self       = <apps.models.ResNet9 object at 0x7f27a7309890>\n",
      "\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:14: NotImplementedError\n",
      "\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_resnet9[needle.backend_ndarray.ndarray_backend_cpu]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_resnet9[needle.backend_ndarray.ndarray_backend_cuda]\u001b[0m - NotImplementedError\n",
      "\u001b[31m====================== \u001b[31m\u001b[1m2 failed\u001b[0m, \u001b[33m1801 deselected\u001b[0m\u001b[31m in 1.63s\u001b[0m\u001b[31m ======================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m pytest -l -v -k \"resnet9\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train a ResNet on CIFAR10: (remember to copy the solutions in `python/needle/optim.py` from previous homeworks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.11.10, pytest-8.3.4, pluggy-1.5.0 -- /opt/conda/envs/train/bin/python\n",
      "cachedir: .pytest_cache\n",
      "hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase(PosixPath('/mnt/zhumingzhu/work/00test/hw4/.hypothesis/examples'))\n",
      "rootdir: /mnt/zhumingzhu/work/00test/hw4\n",
      "plugins: hypothesis-6.115.5\n",
      "collected 1803 items / 1801 deselected / 2 selected                            \u001b[0m\u001b[1m\n",
      "\n",
      "tests/hw4/test_conv.py::test_train_cifar10[needle.backend_ndarray.ndarray_backend_cpu] \u001b[31mFAILED\u001b[0m\u001b[31m [ 50%]\u001b[0m\n",
      "tests/hw4/test_conv.py::test_train_cifar10[needle.backend_ndarray.ndarray_backend_cuda] \u001b[31mFAILED\u001b[0m\u001b[31m [100%]\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m________ test_train_cifar10[needle.backend_ndarray.ndarray_backend_cpu] ________\u001b[0m\n",
      "\n",
      "device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_train_cifar10\u001b[39;49;00m(device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      ">       dataset = ndl.data.CIFAR10Dataset(\u001b[33m\"\u001b[39;49;00m\u001b[33m./data/cifar-10-batches-py\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, train=\u001b[94mTrue\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "device     = cpu()\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:454: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.data.datasets.cifar10_dataset.CIFAR10Dataset object at 0x7f6ce2259cd0>\n",
      "base_folder = './data/cifar-10-batches-py', train = True, p = 0.5\n",
      "transforms = None\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92m__init__\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        base_folder: \u001b[96mstr\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        train: \u001b[96mbool\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        p: Optional[\u001b[96mint\u001b[39;49;00m] = \u001b[94m0.5\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        transforms: Optional[List] = \u001b[94mNone\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m\n",
      "    \u001b[33m    Parameters:\u001b[39;49;00m\n",
      "    \u001b[33m    base_folder - cifar-10-batches-py folder filepath\u001b[39;49;00m\n",
      "    \u001b[33m    train - bool, if True load training dataset, else load test dataset\u001b[39;49;00m\n",
      "    \u001b[33m    Divide pixel values by 255. so that images are in 0-1 range.\u001b[39;49;00m\n",
      "    \u001b[33m    Attributes:\u001b[39;49;00m\n",
      "    \u001b[33m    X - numpy array of images\u001b[39;49;00m\n",
      "    \u001b[33m    y - numpy array of labels\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "base_folder = './data/cifar-10-batches-py'\n",
      "p          = 0.5\n",
      "self       = <needle.data.datasets.cifar10_dataset.CIFAR10Dataset object at 0x7f6ce2259cd0>\n",
      "train      = True\n",
      "transforms = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/data/datasets/cifar10_dataset.py\u001b[0m:25: NotImplementedError\n",
      "\u001b[31m\u001b[1m_______ test_train_cifar10[needle.backend_ndarray.ndarray_backend_cuda] ________\u001b[0m\n",
      "\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_train_cifar10\u001b[39;49;00m(device):\u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      ">       dataset = ndl.data.CIFAR10Dataset(\u001b[33m\"\u001b[39;49;00m\u001b[33m./data/cifar-10-batches-py\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, train=\u001b[94mTrue\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "device     = cuda()\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:454: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.data.datasets.cifar10_dataset.CIFAR10Dataset object at 0x7f6ce23be410>\n",
      "base_folder = './data/cifar-10-batches-py', train = True, p = 0.5\n",
      "transforms = None\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92m__init__\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        base_folder: \u001b[96mstr\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        train: \u001b[96mbool\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        p: Optional[\u001b[96mint\u001b[39;49;00m] = \u001b[94m0.5\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        transforms: Optional[List] = \u001b[94mNone\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m\n",
      "    \u001b[33m    Parameters:\u001b[39;49;00m\n",
      "    \u001b[33m    base_folder - cifar-10-batches-py folder filepath\u001b[39;49;00m\n",
      "    \u001b[33m    train - bool, if True load training dataset, else load test dataset\u001b[39;49;00m\n",
      "    \u001b[33m    Divide pixel values by 255. so that images are in 0-1 range.\u001b[39;49;00m\n",
      "    \u001b[33m    Attributes:\u001b[39;49;00m\n",
      "    \u001b[33m    X - numpy array of images\u001b[39;49;00m\n",
      "    \u001b[33m    y - numpy array of labels\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "base_folder = './data/cifar-10-batches-py'\n",
      "p          = 0.5\n",
      "self       = <needle.data.datasets.cifar10_dataset.CIFAR10Dataset object at 0x7f6ce23be410>\n",
      "train      = True\n",
      "transforms = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/data/datasets/cifar10_dataset.py\u001b[0m:25: NotImplementedError\n",
      "\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_train_cifar10[needle.backend_ndarray.ndarray_backend_cpu]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_train_cifar10[needle.backend_ndarray.ndarray_backend_cuda]\u001b[0m - NotImplementedError\n",
      "\u001b[31m====================== \u001b[31m\u001b[1m2 failed\u001b[0m, \u001b[33m1801 deselected\u001b[0m\u001b[31m in 1.46s\u001b[0m\u001b[31m ======================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m pytest -l -v -k \"train_cifar10\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit ResNet9 to mugrade [10 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can train your model on CIFAR-10 using the following code. Note that this is likely going to be quite slow, and also  not all that accurate due to the lack of data augmentation. You should expect it to take around 500s per epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using needle backend\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msimple_ml\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_cifar10, evaluate_cifar10\n\u001b[1;32m      8\u001b[0m device \u001b[38;5;241m=\u001b[39m ndl\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[0;32m----> 9\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mndl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCIFAR10Dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata/cifar-10-batches-py\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m ndl\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(\\\n\u001b[1;32m     11\u001b[0m          dataset\u001b[38;5;241m=\u001b[39mdataset,\n\u001b[1;32m     12\u001b[0m          batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m,\n\u001b[1;32m     13\u001b[0m          shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,)\n\u001b[1;32m     14\u001b[0m model \u001b[38;5;241m=\u001b[39m ResNet9(device\u001b[38;5;241m=\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/mnt/zhumingzhu/work/00test/hw4/./python/needle/data/datasets/cifar10_dataset.py:25\u001b[0m, in \u001b[0;36mCIFAR10Dataset.__init__\u001b[0;34m(self, base_folder, train, p, transforms)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124;03mParameters:\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124;03mbase_folder - cifar-10-batches-py folder filepath\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;124;03my - numpy array of labels\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m### BEGIN YOUR SOLUTION\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m()\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('./python')\n",
    "sys.path.append('./apps')\n",
    "import needle as ndl\n",
    "from models import ResNet9\n",
    "from simple_ml import train_cifar10, evaluate_cifar10\n",
    "\n",
    "device = ndl.cpu()\n",
    "dataset = ndl.data.CIFAR10Dataset(\"data/cifar-10-batches-py\", train=True)\n",
    "dataloader = ndl.data.DataLoader(\\\n",
    "         dataset=dataset,\n",
    "         batch_size=128,\n",
    "         shuffle=True,)\n",
    "model = ResNet9(device=device, dtype=\"float32\")\n",
    "train_cifar10(model, dataloader, n_epochs=10, optimizer=ndl.optim.Adam,\n",
    "      lr=0.001, weight_decay=0.001)\n",
    "evaluate_cifar10(model, dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Recurrent neural network [10 points]\n",
    "\n",
    "**Note:** In the following sections, you may find yourself wanting to index into tensors, i.e., to use getitem or setitem. However, we have not implemented these for tensors in our library; instead, you should use `stack` and `split` operations.\n",
    "\n",
    "In `python/needle/nn_sequence.py`, implement `RNNCell`.\n",
    "\n",
    "$h^\\prime = \\text{tanh}(xW_{ih} + b_{ih} + hW_{hh} + b_{hh})$. If nonlinearity is 'relu', then ReLU is used in place of tanh.\n",
    "\n",
    "All weights and biases should be uniformly initialized on the interval $\\displaystyle\\pm\\frac{1}{\\sqrt{\\verb|hidden_size|}}$.\n",
    "\n",
    "In `python/needle/nn_sequence.py`, implement `RNN`.\n",
    "\n",
    "For each element in the input sequence, each layer computes the following function:\n",
    "\n",
    "$h_t = \\text{tanh}(x_tW_{ih} + b_{ih} + h_{(t-1)}W_{hh} + b_{hh})$\n",
    "\n",
    "where $h_t$ is the hidden state at time $t$, $x_t$ is the input at time $t$, and $h_{(t-1)}$ is the hidden state of the previous layer at time $t-1$ or the initial hidden state at time $0$. If nonlinearity is 'relu', then ReLU is used in place of tanh.\n",
    "\n",
    "In a multi-layer RNN, the input $x_t^{(l)}$ of the $l$-th layer ($l \\ge 2$) is the hidden state $h_t^{(l-1)}$ of the previous layer. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pytest -l -v -k \"test_rnn\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Long short-term memory network [10 points]\n",
    "In `python/needle/nn/nn_sequence.py`, implement `Sigmoid`.\n",
    "\n",
    "$$\\sigma(x) = \\frac{1}{1 + \\text{exp}(-x)}$$\n",
    "\n",
    "In `python/needle/nn/nn_sequence.py`, implement `LSTMCell`.\n",
    "\n",
    "\\begin{align*}\n",
    "i &= \\sigma(xW_{ii} + b_{ii} + hW_{hi} + b_{hi}) \\\\\n",
    "f &= \\sigma(xW_{if} + b_{if} + hW_{hf} + b_{hf}) \\\\\n",
    "g &= \\text{tanh}(xW_{ig} + b_{ig} + hW_{hg} + b_{hg}) \\\\\n",
    "o &= \\sigma(xW_{io} + b_{io} + hW_{ho} + b_{ho}) \\\\\n",
    "c^\\prime &= f * c + i * g \\\\\n",
    "h^\\prime &= o * \\text{tanh}(c^\\prime)\n",
    "\\end{align*}\n",
    "\n",
    "where $\\sigma$ is the sigmoid function, and $i$, $f$, $g$, $o$ are the input, forget, cell, and output gates, respectively. \n",
    "\n",
    "All weights and biases should be uniformly initialized on the interval $\\displaystyle\\pm\\frac{1}{\\sqrt{\\verb|hidden_size|}}$.\n",
    "\n",
    "Now implement `LSTM` in `python/needle/nn/nn_sequence.py`, which applies a multi-layer LSTM RNN to an input sequence. For each element in the input sequence, each layer computes the following function:\n",
    "\n",
    "\\begin{align*}\n",
    "i_t &= \\sigma(x_tW_{ii} + b_{ii} + h_{(t-1)}W_{hi} + b_{hi}) \\\\\n",
    "f_t &= \\sigma(x_tW_{if} + b_{if} + h_{(t-1)}W_{hf} + b_{hf}) \\\\\n",
    "g_t &= \\text{tanh}(x_tW_{ig} + b_{ig} + h_{(t-1)}W_{hg} + b_{hg}) \\\\\n",
    "o_t &= \\sigma(x_tW_{io} + b_{io} + h_{(t-1)}W_{ho} + b_{ho}) \\\\\n",
    "c_t &= f * c_{(t-1)} + i * g \\\\\n",
    "h_t &= o * \\text{tanh}(c_t)\n",
    "\\end{align*}\n",
    "\n",
    "where $h_t$ is the hidden state at time $t$, $c_t$ is the cell state at time $t$, $x_t$ is the input at time $t$, $h_{(t-1)}$ is the hidden state of the layer at time $t-1$ or the initial hidden state at time $0$, and $i_t$, $f_t$, $g_t$, $o_t$ are the input, forget, cell, and output gates at time $t$ respectively. \n",
    "\n",
    "In a multi-layer LSTM, the input $x_t^{(l)}$ of the $l$-th layer ($l \\ge 2$) is the hidden state $h_t^{(l-1)}$ of the previous layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pytest -l -v -k \"test_lstm\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Penn Treebank dataset [10 points]\n",
    "\n",
    "In word-level language modeling tasks, the model predicts the probability of the next word in the sequence, based on the words already observed in the sequence. You will write support for the Penn Treebank dataset, which consists of stories from the Wall Street Journal, to train and evaluate a language model on word-level prediction.\n",
    "\n",
    "In `python/needle/data/datasets/ptb_dataset.py`, start by implementing the `Dictionary` class, which creates a dictionary from a list of words, mapping each word to a unique integer.\n",
    "\n",
    "Next, we will use this `Dictionary` class to create a corpus from the train and test txt files in the Penn Treebank dataset that you downloaded at the beginning of the notebook. Implement the `tokenize` function in the `Corpus` class to do this.\n",
    "\n",
    "In order to prepare the data for training and evaluation, you will next implement the `batchify` function. Starting from sequential data, batchify arranges the dataset into columns. For instance, with the alphabet as the sequence and batch size 4, we'd get\n",
    "\n",
    "```\n",
    " a g m s \n",
    " b h n t \n",
    " c i o u \n",
    " d j p v \n",
    " e k q w \n",
    " f l r x \n",
    "```\n",
    "\n",
    "These columns are treated as independent by the model, which means that the dependence of e. g. 'g' on 'f' cannot be learned, but allows more efficient batch processing.\n",
    "\n",
    "Next, implement the `get_batch` function. `get_batch` subdivides the source data into chunks of length `bptt`. If source is equal to the example output of the batchify function, with a bptt-limit of 2, we'd get the following two `Tensor`s for i = 0:\n",
    "```\n",
    " a g m s   b h n t \n",
    " b h n t   c i o u \n",
    "```\n",
    "Note that despite the name of the function, the subdivison of data is not done along the batch dimension (i.e. dimension 1), since that was handled by the batchify function. The chunks are along dimension 0, corresponding to the seq_len dimension in the LSTM or RNN. Also, as per the function docs, the second returned `Tensor` (the targets) should be reshaped to be 1-dimensional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pytest -l -v -k \"ptb\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Training a word-level language model [10 points]\n",
    "\n",
    "Finally, you will use the `RNN` and `LSTM` components you have written to construct a language model that we will train on the Penn Treebank dataset.\n",
    "\n",
    "First, in `python/needle/nn/nn_sequence.py` implement `Embedding`. Consider we have a dictionary with 1000 words. Then for a word which indexes into this dictionary, we can represent this word as a one-hot vector of size 1000, and then use a linear layer to project this to a vector of some embedding size.\n",
    "\n",
    "In `apps/models.py`, you can now implement `LanguageModel`. Your language model should consist of \n",
    "\n",
    "- An embedding layer (which maps word IDs to embeddings) \n",
    "- A sequence model (either RNN or LSTM)\n",
    "- A linear layer (which outputs probabilities of the next word)\n",
    "\n",
    "In `apps/simple_ml.py` implement `epoch_general_ptb`, `train_ptb`, and `evaluate_ptb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pytest -l -v -k \"language_model_implementation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pytest -l -v -k \"language_model_training\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can train your language model on the Penn Treebank dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import needle as ndl\n",
    "sys.path.append('./apps')\n",
    "from models import LanguageModel\n",
    "from simple_ml import train_ptb, evaluate_ptb\n",
    "\n",
    "device = ndl.cpu()\n",
    "corpus = ndl.data.Corpus(\"data/ptb\")\n",
    "train_data = ndl.data.batchify(corpus.train, batch_size=16, device=ndl.cpu(), dtype=\"float32\")\n",
    "model = LanguageModel(30, len(corpus.dictionary), hidden_size=10, num_layers=2, seq_model='rnn', device=ndl.cpu())\n",
    "train_ptb(model, train_data, seq_len=1, n_epochs=1, device=device)\n",
    "evaluate_ptb(model, train_data, seq_len=40, device=device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
